{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystark11/transformer-from-scratch/blob/main/src/translation_transformer_from_scratch_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ey--O2jcRnW"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgR32j24cTR9",
        "outputId": "def46b80-a383-418c-f537-49f811101d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for cupy-wheel (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U 'spacy[cuda-autodetect]' -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJoajxGHupW",
        "outputId": "cbcbcc2f-f0ad-41d5-9c1f-419839265eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ------------------ --------------------- 5.8/12.8 MB 27.0 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.6/12.8 MB 31.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 28.6 MB/s  0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "     ------------------ --------------------- 6.0/12.9 MB 33.5 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 10.5/12.9 MB 32.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.9/12.9 MB 21.2 MB/s  0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96u-2MGiXs-4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "giaE9jhGXmY-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ydNngtERXu4H",
        "outputId": "ca1d569d-1cf6-4ce2-e665-99a966cb8d7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SieEu-aCLUUn"
      },
      "outputs": [],
      "source": [
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJawqQ9GX8rF"
      },
      "source": [
        "# MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CnGEAqsBXzkU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7yfVhkMYEhx"
      },
      "source": [
        "# Position wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nhLEdRqBYAyN"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv9loWVOYKQt"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_SqsjrSaYIlt"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model, device=device)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.pow(10_000, (-torch.arange(0, d_model, 2, device=device).float() / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUrDvlYYRWk"
      },
      "source": [
        "# Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i-Suvxv0YOqh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ySRXYzYYH9"
      },
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HRTalpKJYU6P"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YubbLshNYekk"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x6RA6zpgYbHF"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fu_f8PgYnXw"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi6nauuSYhQR",
        "outputId": "1002321f-5af3-43fc-af7f-f4d350b923f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.manythings.org/anki/spa-eng.zip\"\n",
        "filename = \"spa-eng.zip\"\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\\\n",
        " Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "response = requests.get(url, headers=headers, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(1024):\n",
        "            f.write(chunk)\n",
        "    print(\"Download completed\")\n",
        "else:\n",
        "    print(f\"Failed to download. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h14iqrxHZOXI",
        "outputId": "c7cb0ca8-04cd-49f1-a90f-777679df96f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = 'spa-eng.zip'\n",
        "\n",
        "# Path to extract the contents\n",
        "extract_to = './unzip_folder'\n",
        "\n",
        "# Open the zip file in read mode\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Extraction completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uptfO7IsZ2TO"
      },
      "outputs": [],
      "source": [
        "with open(r'C:\\Users\\rahul.g\\Documents\\transformer-from-scratch\\src\\unzip_folder\\spa.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HECjSS1lZ-x3",
        "outputId": "8c9fb2ee-0799-4ec8-b595-cdc3a3b797a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "142928"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bsswMXSfaAy6",
        "outputId": "416cfe6c-01c4-4e06-8421-ff933f5603fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'He made her cry.\\tLe hizo llorar.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1657716 (Spamster) & #2351614 (Shishir)\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OUCSX2QzaEJM"
      },
      "outputs": [],
      "source": [
        "# Remove everything after the 2nd tab character.\n",
        "# As we can see above, we only need the first two columns of the data\n",
        "lines = [line.split('\\t') for line in lines]\n",
        "lines = ['\\t'.join(line[:2]) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYLNfLd9aMYw",
        "outputId": "d5737dd7-089c-4f4b-8185-f79b948cd945"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'He made her cry.\\tLe hizo llorar.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0jrrLdoMKRs-"
      },
      "outputs": [],
      "source": [
        "# Create train, val, test split\n",
        "train_lines, val_test_lines = train_test_split(lines, test_size=0.2, random_state=random_seed, shuffle=True)\n",
        "val_lines, test_lines = train_test_split(val_test_lines, test_size=0.5, random_state=random_seed, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXX2WcrTLlPO",
        "outputId": "0a35df38-3cb0-48fd-8563-f58b7fe42b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "114342\n",
            "14293\n",
            "14293\n"
          ]
        }
      ],
      "source": [
        "print(len(train_lines))\n",
        "print(len(val_lines))\n",
        "print(len(test_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NJwO1OCELxwM",
        "outputId": "d1ec50ee-e52a-4970-a7bb-66e1ec24bf9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"There's a sandwich here.\\tHay un sándwich aquí.\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wuOmjimML1GB",
        "outputId": "9078578e-726d-4915-b87a-8a465ab2a582"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tom cannot drive.\\tTom no sabe conducir.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5CIt4obBL3fK",
        "outputId": "b482b4d6-57f4-48ae-be0c-524548b50880"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'If you really want to help, please come by 2:30.\\tSi realmente quieres ayudar, por favor ven a las 2:30.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_lines[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5cl9qOIukd"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XWO6oUP9aQrE"
      },
      "outputs": [],
      "source": [
        "SRC_LANGUAGE = \"en\"\n",
        "TGT_LANGUAGE = \"es\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "K57FvxR9HjHS"
      },
      "outputs": [],
      "source": [
        "tokenizer = {}\n",
        "tokenizer[SRC_LANGUAGE] = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
        "tokenizer[TGT_LANGUAGE] = get_tokenizer(\"spacy\", \"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av5y5eatMm3P"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9pQb9TXQIQHU"
      },
      "outputs": [],
      "source": [
        "class SentencePairDataset(Dataset):\n",
        "    def __init__(self, lines, src_tokenizer, tgt_tokenizer):\n",
        "        super(SentencePairDataset, self).__init__()\n",
        "\n",
        "        self.lines = lines\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line = self.lines[idx]\n",
        "\n",
        "        src, tgt = line.split('\\t')\n",
        "        src_tokens = self.src_tokenizer(src)\n",
        "        tgt_tokens = self.tgt_tokenizer(tgt)\n",
        "\n",
        "        return src_tokens, tgt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_VhZbqdSKIn5"
      },
      "outputs": [],
      "source": [
        "train_ds = SentencePairDataset(train_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "val_ds = SentencePairDataset(val_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "test_ds = SentencePairDataset(test_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBOyY4vqwfi",
        "outputId": "019210aa-3cfb-4ad4-f8fa-fe3709d73263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "81\n",
            "40\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "# Length of longest src sequence\n",
        "print(max(len(x[0]) for x in train_ds))\n",
        "print(max(len(x[0]) for x in val_ds))\n",
        "print(max(len(x[0]) for x in test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFxXfbeSrB4h",
        "outputId": "1fb399f5-266b-434b-c207-1725d5c56502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78\n",
            "44\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "# Length of longest tgt sequence\n",
        "print(max(len(x[1]) for x in train_ds))\n",
        "print(max(len(x[1]) for x in val_ds))\n",
        "print(max(len(x[1]) for x in test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsIgzRtAMNQ-",
        "outputId": "6452a00e-aaa1-4fc4-f5b3-d545c309fadb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['There', \"'s\", 'a', 'sandwich', 'here', '.'],\n",
              " ['Hay', 'un', 'sándwich', 'aquí', '.'])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqo3q9RkMkJC"
      },
      "source": [
        "## Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rvyraKQ4r9gG"
      },
      "outputs": [],
      "source": [
        "vocab = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "f4FMeg4yMP8x"
      },
      "outputs": [],
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "max_seq_len = 100\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "BOS_IDX = 2\n",
        "EOS_IDX = 3\n",
        "\n",
        "special_symbols = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ujR7SLRvM1h7"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(dataset, lang_idx=0):\n",
        "    n = len(dataset)\n",
        "    i = 0\n",
        "\n",
        "    while i < n:\n",
        "        yield dataset[i][lang_idx]\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "knnnCTpMNGMA"
      },
      "outputs": [],
      "source": [
        "src_iterator = yield_tokens(train_ds, lang_idx=0)\n",
        "tgt_iterator = yield_tokens(train_ds, lang_idx=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "igmo37DaNTox"
      },
      "outputs": [],
      "source": [
        "vocab[SRC_LANGUAGE] = build_vocab_from_iterator(\n",
        "    src_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=src_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YZ0aiHA-OAEf"
      },
      "outputs": [],
      "source": [
        "vocab[TGT_LANGUAGE] = build_vocab_from_iterator(\n",
        "    tgt_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=tgt_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "J5I7rHfFOMTP"
      },
      "outputs": [],
      "source": [
        "vocab[SRC_LANGUAGE].set_default_index(UNK_IDX)\n",
        "vocab[TGT_LANGUAGE].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6dKNNCzOfoy",
        "outputId": "ea75d92f-e2c4-477c-ed1b-47e7c2bcef1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2148"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[SRC_LANGUAGE]['hello']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJCNg2MfORYQ",
        "outputId": "a3506664-f70b-4a62-da85-b4c2e06b240f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2626"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[TGT_LANGUAGE]['Hola']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hfK2UQZFmxQD"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, vocab):\n",
        "    batch_size = len(batch)\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        src_vectors[i] = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](srcs[i]) + [EOS_IDX] + [0] * (max_seq_len - len(srcs[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "        tgt_vectors[i] = torch.tensor(([BOS_IDX] + vocab[TGT_LANGUAGE](tgts[i]) + [EOS_IDX] + [0] * (max_seq_len - len(tgts[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "    return src_vectors, tgt_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Uqxlzw-Hu_YJ"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxUsDY5JzNj7",
        "outputId": "8193140e-cbc0-4d38-ea8d-e3f9986e5726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "------------------------------\n",
            "Epoch: 1, Training Loss: 9.309993743896484\n",
            "Epoch: 1, Training Loss: 9.143234252929688\n",
            "Epoch: 1, Training Loss: 8.865801811218262\n",
            "Epoch: 1, Training Loss: 8.617685317993164\n",
            "Epoch: 1, Training Loss: 8.465410232543945\n",
            "Epoch: 1, Training Loss: 8.318875312805176\n",
            "Epoch: 1, Training Loss: 8.179346084594727\n",
            "Epoch: 1, Training Loss: 8.004155158996582\n",
            "Epoch: 1, Training Loss: 7.983952045440674\n",
            "Epoch: 1, Training Loss: 7.95011043548584\n",
            "Epoch: 1, Training Loss: 7.701634883880615\n",
            "Epoch: 1, Training Loss: 7.790953636169434\n",
            "Epoch: 1, Training Loss: 7.780338287353516\n",
            "Epoch: 1, Training Loss: 7.67399787902832\n",
            "Epoch: 1, Training Loss: 7.690968036651611\n",
            "Epoch: 1, Training Loss: 7.561918258666992\n",
            "Epoch: 1, Training Loss: 7.549985885620117\n",
            "Epoch: 1, Training Loss: 7.631964683532715\n",
            "Epoch: 1, Training Loss: 7.414304733276367\n",
            "Epoch: 1, Training Loss: 7.434693813323975\n",
            "Epoch: 1, Training Loss: 7.432102203369141\n",
            "Epoch: 1, Training Loss: 7.441831588745117\n",
            "Epoch: 1, Training Loss: 7.300600051879883\n",
            "Epoch: 1, Training Loss: 7.286588191986084\n",
            "Epoch: 1, Training Loss: 7.299835205078125\n",
            "Epoch: 1, Training Loss: 7.177742958068848\n",
            "Epoch: 1, Training Loss: 7.248348712921143\n",
            "Epoch: 1, Training Loss: 7.2080559730529785\n",
            "Epoch: 1, Training Loss: 7.0789031982421875\n",
            "Epoch: 1, Training Loss: 7.110100269317627\n",
            "Epoch: 1, Training Loss: 7.026004791259766\n",
            "Epoch: 1, Training Loss: 7.047985553741455\n",
            "Epoch: 1, Training Loss: 7.074185848236084\n",
            "Epoch: 1, Training Loss: 7.000983238220215\n",
            "Epoch: 1, Training Loss: 6.952051639556885\n",
            "Epoch: 1, Training Loss: 7.097329616546631\n",
            "Epoch: 1, Training Loss: 6.781764507293701\n",
            "Epoch: 1, Training Loss: 6.855526447296143\n",
            "Epoch: 1, Training Loss: 6.796132564544678\n",
            "Epoch: 1, Training Loss: 6.736219882965088\n",
            "Epoch: 1, Training Loss: 6.684711933135986\n",
            "Epoch: 1, Training Loss: 6.648652076721191\n",
            "Epoch: 1, Training Loss: 6.705461502075195\n",
            "Epoch: 1, Training Loss: 6.644474506378174\n",
            "Epoch: 1, Training Loss: 6.645124435424805\n",
            "Epoch: 1, Training Loss: 6.541707515716553\n",
            "Epoch: 1, Training Loss: 6.564250469207764\n",
            "Epoch: 1, Training Loss: 6.613136291503906\n",
            "Epoch: 1, Training Loss: 6.464811325073242\n",
            "Epoch: 1, Training Loss: 6.387989521026611\n",
            "Epoch: 1, Training Loss: 6.562385559082031\n",
            "Epoch: 1, Training Loss: 6.639536380767822\n",
            "Epoch: 1, Training Loss: 6.416617393493652\n",
            "Epoch: 1, Training Loss: 6.400162220001221\n",
            "Epoch: 1, Training Loss: 6.227995872497559\n",
            "Epoch: 1, Training Loss: 6.298835277557373\n",
            "Epoch: 1, Training Loss: 6.233336448669434\n",
            "Epoch: 1, Training Loss: 6.20196533203125\n",
            "Epoch: 1, Training Loss: 6.346276760101318\n",
            "Epoch: 1, Training Loss: 6.302163124084473\n",
            "Epoch: 1, Training Loss: 6.113179683685303\n",
            "Epoch: 1, Training Loss: 6.094748020172119\n",
            "Epoch: 1, Training Loss: 6.143090724945068\n",
            "Epoch: 1, Training Loss: 6.094417572021484\n",
            "Epoch: 1, Training Loss: 6.15900182723999\n",
            "Epoch: 1, Training Loss: 6.139334201812744\n",
            "Epoch: 1, Training Loss: 6.087578296661377\n",
            "Epoch: 1, Training Loss: 5.951548099517822\n",
            "Epoch: 1, Training Loss: 6.117733955383301\n",
            "Epoch: 1, Training Loss: 6.00855016708374\n",
            "Epoch: 1, Training Loss: 6.119764804840088\n",
            "Epoch: 1, Training Loss: 6.028697967529297\n",
            "Epoch: 1, Training Loss: 6.074151992797852\n",
            "Epoch: 1, Training Loss: 5.865989685058594\n",
            "Epoch: 1, Training Loss: 5.7638468742370605\n",
            "Epoch: 1, Training Loss: 5.791269302368164\n",
            "Epoch: 1, Training Loss: 6.050870418548584\n",
            "Epoch: 1, Training Loss: 5.75129508972168\n",
            "Epoch: 1, Training Loss: 5.8739848136901855\n",
            "Epoch: 1, Training Loss: 5.979610919952393\n",
            "Epoch: 1, Training Loss: 5.756208896636963\n",
            "Epoch: 1, Training Loss: 5.803490161895752\n",
            "Epoch: 1, Training Loss: 5.861530780792236\n",
            "Epoch: 1, Training Loss: 5.8377556800842285\n",
            "Epoch: 1, Training Loss: 5.796089172363281\n",
            "Epoch: 1, Training Loss: 5.694845676422119\n",
            "Epoch: 1, Training Loss: 5.8108720779418945\n",
            "Epoch: 1, Training Loss: 5.783857345581055\n",
            "Epoch: 1, Training Loss: 5.640870094299316\n",
            "Epoch: 1, Training Loss: 5.695380210876465\n",
            "Epoch: 1, Training Loss: 5.627395153045654\n",
            "Epoch: 1, Training Loss: 5.717816352844238\n",
            "Epoch: 1, Training Loss: 5.6395344734191895\n",
            "Epoch: 1, Training Loss: 5.621973514556885\n",
            "Epoch: 1, Training Loss: 5.744734287261963\n",
            "Epoch: 1, Training Loss: 5.5782599449157715\n",
            "Epoch: 1, Training Loss: 5.69777774810791\n",
            "Epoch: 1, Training Loss: 5.723029613494873\n",
            "Epoch: 1, Training Loss: 5.524999618530273\n",
            "Epoch: 1, Training Loss: 5.605471611022949\n",
            "Epoch: 1, Training Loss: 5.469097137451172\n",
            "Epoch: 1, Training Loss: 5.560014247894287\n",
            "Epoch: 1, Training Loss: 5.51223611831665\n",
            "Epoch: 1, Training Loss: 5.558585166931152\n",
            "Epoch: 1, Training Loss: 5.4659810066223145\n",
            "Epoch: 1, Training Loss: 5.387320518493652\n",
            "Epoch: 1, Training Loss: 5.441988945007324\n",
            "Epoch: 1, Training Loss: 5.447600364685059\n",
            "Epoch: 1, Training Loss: 5.643627166748047\n",
            "Epoch: 1, Training Loss: 5.427838325500488\n",
            "Epoch: 1, Training Loss: 5.429619312286377\n",
            "Epoch: 1, Training Loss: 5.482417106628418\n",
            "Epoch: 1, Training Loss: 5.407061576843262\n",
            "Epoch: 1, Training Loss: 5.350980758666992\n",
            "Epoch: 1, Training Loss: 5.283756256103516\n",
            "Epoch: 1, Training Loss: 5.210575103759766\n",
            "Epoch: 1, Training Loss: 5.4162516593933105\n",
            "Epoch: 1, Training Loss: 5.3315534591674805\n",
            "Epoch: 1, Training Loss: 5.285844802856445\n",
            "Epoch: 1, Training Loss: 5.386286735534668\n",
            "Epoch: 1, Training Loss: 5.212069034576416\n",
            "Epoch: 1, Training Loss: 5.523492336273193\n",
            "Epoch: 1, Training Loss: 5.332248210906982\n",
            "Epoch: 1, Training Loss: 5.247672080993652\n",
            "Epoch: 1, Training Loss: 5.283916473388672\n",
            "Epoch: 1, Training Loss: 5.260719299316406\n",
            "Epoch: 1, Training Loss: 5.2604241371154785\n",
            "Epoch: 1, Training Loss: 5.448766231536865\n",
            "Epoch: 1, Training Loss: 5.168501853942871\n",
            "Epoch: 1, Training Loss: 5.296581268310547\n",
            "Epoch: 1, Training Loss: 5.341166973114014\n",
            "Epoch: 1, Training Loss: 5.116868495941162\n",
            "Epoch: 1, Training Loss: 5.314951419830322\n",
            "Epoch: 1, Training Loss: 5.485835552215576\n",
            "Epoch: 1, Training Loss: 5.250004291534424\n",
            "Epoch: 1, Training Loss: 5.214219570159912\n",
            "Epoch: 1, Training Loss: 5.294381618499756\n",
            "Epoch: 1, Training Loss: 5.19492769241333\n",
            "Epoch: 1, Training Loss: 5.321325778961182\n",
            "Epoch: 1, Training Loss: 5.212672233581543\n",
            "Epoch: 1, Training Loss: 5.166995048522949\n",
            "Epoch: 1, Training Loss: 5.183736801147461\n",
            "Epoch: 1, Training Loss: 5.1490397453308105\n",
            "Epoch: 1, Training Loss: 5.068110466003418\n",
            "Epoch: 1, Training Loss: 5.064634799957275\n",
            "Epoch: 1, Training Loss: 5.199353218078613\n",
            "Epoch: 1, Training Loss: 5.122798442840576\n",
            "Epoch: 1, Training Loss: 5.197324752807617\n",
            "Epoch: 1, Training Loss: 5.173385143280029\n",
            "Epoch: 1, Training Loss: 5.043766021728516\n",
            "Epoch: 1, Training Loss: 5.105233192443848\n",
            "Epoch: 1, Training Loss: 5.225930213928223\n",
            "Epoch: 1, Training Loss: 5.233549118041992\n",
            "Epoch: 1, Training Loss: 5.029357433319092\n",
            "Epoch: 1, Training Loss: 4.913367748260498\n",
            "Epoch: 1, Training Loss: 5.02675199508667\n",
            "Epoch: 1, Training Loss: 5.056581974029541\n",
            "Epoch: 1, Training Loss: 5.078619956970215\n",
            "Epoch: 1, Training Loss: 4.979591369628906\n",
            "Epoch: 1, Training Loss: 5.187216281890869\n",
            "Epoch: 1, Training Loss: 5.02327823638916\n",
            "Epoch: 1, Training Loss: 5.020928382873535\n",
            "Epoch: 1, Training Loss: 5.12261962890625\n",
            "Epoch: 1, Training Loss: 5.174752235412598\n",
            "Epoch: 1, Training Loss: 5.076435089111328\n",
            "Epoch: 1, Training Loss: 5.005903720855713\n",
            "Epoch: 1, Training Loss: 5.001223087310791\n",
            "Epoch: 1, Training Loss: 5.046663284301758\n",
            "Epoch: 1, Training Loss: 5.164262294769287\n",
            "Epoch: 1, Training Loss: 5.071535110473633\n",
            "Epoch: 1, Training Loss: 5.369804382324219\n",
            "Epoch: 1, Training Loss: 4.95628023147583\n",
            "Epoch: 1, Training Loss: 5.161340713500977\n",
            "Epoch: 1, Training Loss: 5.135703086853027\n",
            "Epoch: 1, Training Loss: 5.136730670928955\n",
            "Epoch: 1, Training Loss: 4.925997257232666\n",
            "Epoch: 1, Training Loss: 4.8858866691589355\n",
            "Epoch: 1, Training Loss: 4.96631383895874\n",
            "Epoch: 1, Training Loss: 5.126109600067139\n",
            "Epoch: 1, Training Loss: 4.9904255867004395\n",
            "Epoch: 1, Training Loss: 5.031979084014893\n",
            "Epoch: 1, Training Loss: 5.178310871124268\n",
            "Epoch: 1, Training Loss: 5.099203109741211\n",
            "Epoch: 1, Training Loss: 4.821572303771973\n",
            "Epoch: 1, Training Loss: 4.8707380294799805\n",
            "Epoch: 1, Training Loss: 5.090236663818359\n",
            "Epoch: 1, Training Loss: 4.827845096588135\n",
            "Epoch: 1, Training Loss: 4.878617286682129\n",
            "Epoch: 1, Training Loss: 4.994654655456543\n",
            "Epoch: 1, Training Loss: 5.081111431121826\n",
            "Epoch: 1, Training Loss: 4.934135913848877\n",
            "Epoch: 1, Training Loss: 4.985705375671387\n",
            "Epoch: 1, Training Loss: 4.971428871154785\n",
            "Epoch: 1, Training Loss: 4.955575466156006\n",
            "Epoch: 1, Training Loss: 5.0262651443481445\n",
            "Epoch: 1, Training Loss: 4.928199291229248\n",
            "Epoch: 1, Training Loss: 4.8904032707214355\n",
            "Epoch: 1, Training Loss: 4.922813415527344\n",
            "Epoch: 1, Training Loss: 5.063413619995117\n",
            "Epoch: 1, Training Loss: 4.784198760986328\n",
            "Epoch: 1, Training Loss: 4.800551891326904\n",
            "Epoch: 1, Training Loss: 4.970179557800293\n",
            "Epoch: 1, Training Loss: 4.911307334899902\n",
            "Epoch: 1, Training Loss: 4.978357315063477\n",
            "Epoch: 1, Training Loss: 4.967973232269287\n",
            "Epoch: 1, Training Loss: 4.904322624206543\n",
            "Epoch: 1, Training Loss: 4.997712135314941\n",
            "Epoch: 1, Training Loss: 4.975851535797119\n",
            "Epoch: 1, Training Loss: 4.924929141998291\n",
            "Epoch: 1, Training Loss: 4.756866455078125\n",
            "Epoch: 1, Training Loss: 4.82118558883667\n",
            "Epoch: 1, Training Loss: 4.6645073890686035\n",
            "Epoch: 1, Training Loss: 4.869192123413086\n",
            "Epoch: 1, Training Loss: 4.760694980621338\n",
            "Epoch: 1, Training Loss: 4.744829177856445\n",
            "Epoch: 1, Training Loss: 5.066864490509033\n",
            "Epoch: 1, Training Loss: 4.859106540679932\n",
            "Epoch: 1, Training Loss: 4.776403903961182\n",
            "Epoch: 1, Training Loss: 4.9673848152160645\n",
            "Epoch: 1, Training Loss: 4.795843124389648\n",
            "Epoch: 1, Training Loss: 4.836122989654541\n",
            "Epoch: 1, Training Loss: 5.006308078765869\n",
            "Epoch: 1, Training Loss: 4.861119747161865\n",
            "Epoch: 1, Training Loss: 4.852853775024414\n",
            "Epoch: 1, Training Loss: 5.106157302856445\n",
            "Epoch: 1, Training Loss: 4.752585411071777\n",
            "Epoch: 1, Training Loss: 4.912466049194336\n",
            "Epoch: 1, Training Loss: 4.89614725112915\n",
            "Epoch: 1, Training Loss: 4.857871055603027\n",
            "Epoch: 1, Training Loss: 4.7307000160217285\n",
            "Epoch: 1, Training Loss: 5.176998138427734\n",
            "Epoch: 1, Training Loss: 4.804028511047363\n",
            "Epoch: 1, Training Loss: 4.913069725036621\n",
            "Epoch: 1, Training Loss: 4.8710246086120605\n",
            "Epoch: 1, Training Loss: 4.541362285614014\n",
            "Epoch: 1, Training Loss: 4.913558483123779\n",
            "Epoch: 1, Training Loss: 4.764648914337158\n",
            "Epoch: 1, Training Loss: 4.891202926635742\n",
            "Epoch: 1, Training Loss: 4.839318752288818\n",
            "Epoch: 1, Training Loss: 4.8523173332214355\n",
            "Epoch: 1, Training Loss: 4.609517574310303\n",
            "Epoch: 1, Training Loss: 4.380338191986084\n",
            "Epoch: 1, Training Loss: 4.782203674316406\n",
            "Epoch: 1, Training Loss: 4.809544086456299\n",
            "Epoch: 1, Training Loss: 4.796485424041748\n",
            "Epoch: 1, Training Loss: 5.037647247314453\n",
            "Epoch: 1, Training Loss: 4.7800397872924805\n",
            "Epoch: 1, Training Loss: 4.840560436248779\n",
            "Epoch: 1, Training Loss: 4.687616348266602\n",
            "Epoch: 1, Training Loss: 4.85635232925415\n",
            "Epoch: 1, Training Loss: 4.588719844818115\n",
            "Epoch: 1, Training Loss: 4.593011856079102\n",
            "Epoch: 1, Training Loss: 4.699978351593018\n",
            "Epoch: 1, Training Loss: 4.854485511779785\n",
            "Epoch: 1, Training Loss: 4.583610534667969\n",
            "Epoch: 1, Training Loss: 4.9412031173706055\n",
            "Epoch: 1, Training Loss: 4.83073091506958\n",
            "Epoch: 1, Training Loss: 4.670584678649902\n",
            "Epoch: 1, Training Loss: 4.663613319396973\n",
            "Epoch: 1, Training Loss: 4.708348751068115\n",
            "Epoch: 1, Training Loss: 4.5187883377075195\n",
            "Epoch: 1, Training Loss: 4.7087249755859375\n",
            "Epoch: 1, Training Loss: 4.93649959564209\n",
            "Epoch: 1, Training Loss: 4.962706089019775\n",
            "Epoch: 1, Training Loss: 4.818032741546631\n",
            "Epoch: 1, Training Loss: 4.773948669433594\n",
            "Epoch: 1, Training Loss: 4.7112040519714355\n",
            "Epoch: 1, Training Loss: 4.8056511878967285\n",
            "Epoch: 1, Training Loss: 4.696655750274658\n",
            "Epoch: 1, Training Loss: 4.765683650970459\n",
            "Epoch: 1, Training Loss: 4.705020427703857\n",
            "Epoch: 1, Training Loss: 4.729584217071533\n",
            "Epoch: 1, Training Loss: 4.729331016540527\n",
            "Epoch: 1, Training Loss: 4.804299354553223\n",
            "Epoch: 1, Training Loss: 4.72163724899292\n",
            "Epoch: 1, Training Loss: 4.833462715148926\n",
            "Epoch: 1, Training Loss: 4.889105319976807\n",
            "Epoch: 1, Training Loss: 4.564162254333496\n",
            "Epoch: 1, Training Loss: 4.790606498718262\n",
            "Epoch: 1, Training Loss: 4.817575931549072\n",
            "Epoch: 1, Training Loss: 4.63909912109375\n",
            "Epoch: 1, Training Loss: 4.6410675048828125\n",
            "Epoch: 1, Training Loss: 4.830713272094727\n",
            "Epoch: 1, Training Loss: 4.605563163757324\n",
            "Epoch: 1, Training Loss: 4.573169231414795\n",
            "Epoch: 1, Training Loss: 4.693850040435791\n",
            "Epoch: 1, Training Loss: 4.7376508712768555\n",
            "Epoch: 1, Training Loss: 4.533447265625\n",
            "Epoch: 1, Training Loss: 4.6756978034973145\n",
            "Epoch: 1, Training Loss: 4.774113655090332\n",
            "Epoch: 1, Training Loss: 4.741672515869141\n",
            "Epoch: 1, Training Loss: 4.362431526184082\n",
            "Epoch: 1, Training Loss: 4.595221042633057\n",
            "Epoch: 1, Training Loss: 4.583676338195801\n",
            "Epoch: 1, Training Loss: 4.531696796417236\n",
            "Epoch: 1, Training Loss: 4.783841609954834\n",
            "Epoch: 1, Training Loss: 4.52010440826416\n",
            "Epoch: 1, Training Loss: 4.690004348754883\n",
            "Epoch: 1, Training Loss: 4.85061502456665\n",
            "Epoch: 1, Training Loss: 4.627206802368164\n",
            "Epoch: 1, Training Loss: 4.682365894317627\n",
            "Epoch: 1, Training Loss: 4.732585906982422\n",
            "Epoch: 1, Training Loss: 4.67262601852417\n",
            "Epoch: 1, Training Loss: 4.6650848388671875\n",
            "Epoch: 1, Training Loss: 4.709200382232666\n",
            "Epoch: 1, Training Loss: 4.8231730461120605\n",
            "Epoch: 1, Training Loss: 4.363124847412109\n",
            "Epoch: 1, Training Loss: 4.495428562164307\n",
            "Epoch: 1, Training Loss: 4.631615161895752\n",
            "Epoch: 1, Training Loss: 4.657436847686768\n",
            "Epoch: 1, Training Loss: 4.748547077178955\n",
            "Epoch: 1, Training Loss: 4.659865379333496\n",
            "Epoch: 1, Training Loss: 4.611645698547363\n",
            "Epoch: 1, Training Loss: 4.593606472015381\n",
            "Epoch: 1, Training Loss: 4.342941761016846\n",
            "Epoch: 1, Training Loss: 4.732611179351807\n",
            "Epoch: 1, Training Loss: 4.702311038970947\n",
            "Epoch: 1, Training Loss: 4.697306156158447\n",
            "Epoch: 1, Training Loss: 4.679283142089844\n",
            "Epoch: 1, Training Loss: 4.544919967651367\n",
            "Epoch: 1, Training Loss: 4.553475379943848\n",
            "Epoch: 1, Training Loss: 4.54245138168335\n",
            "Epoch: 1, Training Loss: 4.352606296539307\n",
            "Epoch: 1, Training Loss: 4.6682515144348145\n",
            "Epoch: 1, Training Loss: 4.580405235290527\n",
            "Epoch: 1, Training Loss: 4.656153678894043\n",
            "Epoch: 1, Training Loss: 4.826428413391113\n",
            "Epoch: 1, Training Loss: 4.760082244873047\n",
            "Epoch: 1, Training Loss: 4.600167274475098\n",
            "Epoch: 1, Training Loss: 4.652204990386963\n",
            "Epoch: 1, Training Loss: 4.809182643890381\n",
            "Epoch: 1, Training Loss: 4.6078877449035645\n",
            "Epoch: 1, Training Loss: 4.436666488647461\n",
            "Epoch: 1, Training Loss: 4.5603413581848145\n",
            "Epoch: 1, Training Loss: 4.4120588302612305\n",
            "Epoch: 1, Training Loss: 4.504655838012695\n",
            "Epoch: 1, Training Loss: 4.5500617027282715\n",
            "Epoch: 1, Training Loss: 4.731905937194824\n",
            "Epoch: 1, Training Loss: 4.756551265716553\n",
            "Epoch: 1, Training Loss: 4.707787990570068\n",
            "Epoch: 1, Training Loss: 4.812934875488281\n",
            "Epoch: 1, Training Loss: 4.556403160095215\n",
            "Epoch: 1, Training Loss: 4.581886291503906\n",
            "Epoch: 1, Training Loss: 4.431252479553223\n",
            "Epoch: 1, Training Loss: 4.45136833190918\n",
            "Epoch: 1, Training Loss: 4.75849723815918\n",
            "Epoch: 1, Training Loss: 4.4905805587768555\n",
            "Epoch: 1, Training Loss: 4.437008380889893\n",
            "Epoch: 1, Training Loss: 4.62501335144043\n",
            "Epoch: 1, Training Loss: 4.534499168395996\n",
            "Epoch: 1, Training Loss: 4.69223165512085\n",
            "Epoch: 1, Training Loss: 4.660398006439209\n",
            "Epoch: 1, Training Loss: 4.615475654602051\n",
            "Epoch: 1, Training Loss: 4.552619457244873\n",
            "Epoch: 1, Training Loss: 4.5424885749816895\n",
            "Epoch: 1, Training Loss: 4.547762393951416\n",
            "Epoch: 1, Training Loss: 4.369552135467529\n",
            "Epoch: 1, Training Loss: 4.511250019073486\n",
            "Epoch: 1, Training Loss: 4.511629581451416\n",
            "Epoch: 1, Training Loss: 4.603819847106934\n",
            "Epoch: 1, Training Loss: 4.518033027648926\n",
            "Epoch: 1, Training Loss: 4.615187168121338\n",
            "Epoch: 1, Training Loss: 4.586618900299072\n",
            "Epoch: 1, Training Loss: 4.260767459869385\n",
            "Epoch: 1, Training Loss: 4.6427226066589355\n",
            "Epoch: 1, Training Loss: 4.541597366333008\n",
            "Epoch: 1, Training Loss: 4.485396862030029\n",
            "Epoch: 1, Training Loss: 4.413992404937744\n",
            "Epoch: 1, Training Loss: 4.5293049812316895\n",
            "Epoch: 1, Training Loss: 4.2695393562316895\n",
            "Epoch: 1, Training Loss: 4.381897926330566\n",
            "Epoch: 1, Training Loss: 4.408658981323242\n",
            "Epoch: 1, Training Loss: 4.451863765716553\n",
            "Epoch: 1, Training Loss: 4.569583892822266\n",
            "Epoch: 1, Training Loss: 4.615645408630371\n",
            "Epoch: 1, Training Loss: 4.4879302978515625\n",
            "Epoch: 1, Training Loss: 4.6153154373168945\n",
            "Epoch: 1, Training Loss: 4.813294887542725\n",
            "Epoch: 1, Training Loss: 4.5241780281066895\n",
            "Epoch: 1, Training Loss: 4.292499542236328\n",
            "Epoch: 1, Training Loss: 4.481149196624756\n",
            "Epoch: 1, Training Loss: 4.578007221221924\n",
            "Epoch: 1, Training Loss: 4.363916397094727\n",
            "Epoch: 1, Training Loss: 4.453173637390137\n",
            "Epoch: 1, Training Loss: 4.3892364501953125\n",
            "Epoch: 1, Training Loss: 4.446527481079102\n",
            "Epoch: 1, Training Loss: 4.538160800933838\n",
            "Epoch: 1, Training Loss: 4.466574192047119\n",
            "Epoch: 1, Training Loss: 4.294445991516113\n",
            "Epoch: 1, Training Loss: 4.411627769470215\n",
            "Epoch: 1, Training Loss: 4.547915458679199\n",
            "Epoch: 1, Training Loss: 4.517807483673096\n",
            "Epoch: 1, Training Loss: 4.411139011383057\n",
            "Epoch: 1, Training Loss: 4.43712854385376\n",
            "Epoch: 1, Training Loss: 4.575136661529541\n",
            "Epoch: 1, Training Loss: 4.45430850982666\n",
            "Epoch: 1, Training Loss: 4.422459125518799\n",
            "Epoch: 1, Training Loss: 4.48026180267334\n",
            "Epoch: 1, Training Loss: 4.525580883026123\n",
            "Epoch: 1, Training Loss: 4.341275691986084\n",
            "Epoch: 1, Training Loss: 4.506666660308838\n",
            "Epoch: 1, Training Loss: 4.482389450073242\n",
            "Epoch: 1, Training Loss: 4.476180553436279\n",
            "Epoch: 1, Training Loss: 4.35574197769165\n",
            "Epoch: 1, Training Loss: 4.496216773986816\n",
            "Epoch: 1, Training Loss: 4.472001552581787\n",
            "Epoch: 1, Training Loss: 4.4497904777526855\n",
            "Epoch: 1, Training Loss: 4.421351432800293\n",
            "Epoch: 1, Training Loss: 4.26292085647583\n",
            "Epoch: 1, Training Loss: 4.471097469329834\n",
            "Epoch: 1, Training Loss: 4.387352466583252\n",
            "Epoch: 1, Training Loss: 4.4600605964660645\n",
            "Epoch: 1, Training Loss: 4.333985805511475\n",
            "Epoch: 1, Training Loss: 4.1894917488098145\n",
            "Epoch: 1, Training Loss: 4.501485347747803\n",
            "Epoch: 1, Training Loss: 4.533106327056885\n",
            "Epoch: 1, Training Loss: 4.36760139465332\n",
            "Epoch: 1, Training Loss: 4.279766082763672\n",
            "Epoch: 1, Training Loss: 4.4335618019104\n",
            "Epoch: 1, Training Loss: 4.220203399658203\n",
            "Epoch: 1, Training Loss: 4.433669090270996\n",
            "Epoch: 1, Training Loss: 4.295933723449707\n",
            "Epoch: 1, Training Loss: 4.424713611602783\n",
            "Epoch: 1, Training Loss: 4.380530834197998\n",
            "Epoch: 1, Training Loss: 4.440041542053223\n",
            "Epoch: 1, Training Loss: 4.565845966339111\n",
            "Epoch: 1, Training Loss: 4.494661808013916\n",
            "Epoch: 1, Training Loss: 4.226029396057129\n",
            "Epoch: 1, Training Loss: 4.557461738586426\n",
            "Epoch: 1, Training Loss: 4.584173679351807\n",
            "Epoch: 1, Training Loss: 4.372558116912842\n",
            "Epoch: 1, Training Loss: 4.537632465362549\n",
            "Epoch: 1, Training Loss: 4.223979473114014\n",
            "Epoch: 1, Training Loss: 4.179973602294922\n",
            "Epoch: 1, Training Loss: 4.248750686645508\n",
            "Epoch: 1, Training Loss: 4.466982841491699\n",
            "Epoch: 1, Training Loss: 4.418116092681885\n",
            "Epoch: 1, Training Loss: 4.444195747375488\n",
            "Epoch: 1, Training Loss: 4.053461074829102\n",
            "Epoch: 1, Training Loss: 4.4673662185668945\n",
            "Epoch: 1, Training Loss: 4.467416286468506\n",
            "Epoch: 1, Training Loss: 4.274526596069336\n",
            "Epoch: 1, Training Loss: 4.514726638793945\n",
            "Epoch: 1, Training Loss: 4.478189468383789\n",
            "Epoch: 1, Training Loss: 4.3394598960876465\n",
            "Epoch: 1, Training Loss: 4.321965217590332\n",
            "Epoch: 1, Training Loss: 4.302982330322266\n",
            "Epoch: 1, Training Loss: 4.217896938323975\n",
            "Epoch: 1, Training Loss: 4.336886882781982\n",
            "Epoch: 1, Training Loss: 4.337098121643066\n",
            "Epoch: 1, Training Loss: 4.394825458526611\n",
            "Epoch: 1, Training Loss: 4.383901596069336\n",
            "Epoch: 1, Training Loss: 4.368535995483398\n",
            "Epoch: 1, Training Loss: 4.2488603591918945\n",
            "Epoch: 1, Training Loss: 4.323720455169678\n",
            "Epoch: 1, Training Loss: 4.355571269989014\n",
            "Epoch: 1, Training Loss: 4.276297092437744\n",
            "Epoch: 1, Training Loss: 4.481264114379883\n",
            "Epoch: 1, Training Loss: 4.271503925323486\n",
            "Epoch: 1, Training Loss: 4.1221184730529785\n",
            "Epoch: 1, Training Loss: 4.457661151885986\n",
            "Epoch: 1, Training Loss: 4.267328262329102\n",
            "Epoch: 1, Training Loss: 4.280683994293213\n",
            "Epoch: 1, Training Loss: 4.36331844329834\n",
            "Epoch: 1, Training Loss: 4.293248176574707\n",
            "Epoch: 1, Training Loss: 4.257144451141357\n",
            "Epoch: 1, Training Loss: 4.345723628997803\n",
            "Epoch: 1, Training Loss: 4.270276069641113\n",
            "Epoch: 1, Training Loss: 4.282246112823486\n",
            "Epoch: 1, Training Loss: 4.395328044891357\n",
            "Epoch: 1, Training Loss: 4.564913272857666\n",
            "Epoch: 1, Training Loss: 4.381428241729736\n",
            "Epoch: 1, Training Loss: 4.318599224090576\n",
            "Epoch: 1, Training Loss: 4.473092555999756\n",
            "Epoch: 1, Training Loss: 4.238581657409668\n",
            "Epoch: 1, Training Loss: 4.344358921051025\n",
            "Epoch: 1, Training Loss: 4.176053524017334\n",
            "Epoch: 1, Training Loss: 4.37808895111084\n",
            "Epoch: 1, Training Loss: 4.435304164886475\n",
            "Epoch: 1, Training Loss: 4.397952079772949\n",
            "Epoch: 1, Training Loss: 4.2625250816345215\n",
            "Epoch: 1, Training Loss: 4.346588134765625\n",
            "Epoch: 1, Training Loss: 4.372325897216797\n",
            "Epoch: 1, Training Loss: 4.319710731506348\n",
            "Epoch: 1, Training Loss: 4.229940414428711\n",
            "Epoch: 1, Training Loss: 4.220396995544434\n",
            "Epoch: 1, Training Loss: 4.3426971435546875\n",
            "Epoch: 1, Training Loss: 4.205305099487305\n",
            "Epoch: 1, Training Loss: 4.158506870269775\n",
            "Epoch: 1, Training Loss: 4.176510810852051\n",
            "Epoch: 1, Training Loss: 4.2351484298706055\n",
            "Epoch: 1, Training Loss: 4.216525554656982\n",
            "Epoch: 1, Training Loss: 4.097005844116211\n",
            "Epoch: 1, Training Loss: 4.369685649871826\n",
            "Epoch: 1, Training Loss: 4.209691047668457\n",
            "Epoch: 1, Training Loss: 4.271849155426025\n",
            "Epoch: 1, Training Loss: 4.150644302368164\n",
            "Epoch: 1, Training Loss: 4.232063293457031\n",
            "Epoch: 1, Training Loss: 4.297271251678467\n",
            "Epoch: 1, Training Loss: 4.250443935394287\n",
            "Epoch: 1, Training Loss: 4.257265090942383\n",
            "Epoch: 1, Training Loss: 4.152803897857666\n",
            "Epoch: 1, Training Loss: 4.291639804840088\n",
            "Epoch: 1, Training Loss: 4.27133846282959\n",
            "Epoch: 1, Training Loss: 4.20579719543457\n",
            "Epoch: 1, Training Loss: 4.243483543395996\n",
            "Epoch: 1, Training Loss: 3.9466552734375\n",
            "Epoch: 1, Training Loss: 4.462355613708496\n",
            "Epoch: 1, Training Loss: 4.375617980957031\n",
            "Epoch: 1, Training Loss: 4.397436141967773\n",
            "Epoch: 1, Training Loss: 4.269702434539795\n",
            "Epoch: 1, Training Loss: 4.328545093536377\n",
            "Epoch: 1, Training Loss: 4.196361541748047\n",
            "Epoch: 1, Training Loss: 4.294602870941162\n",
            "Epoch: 1, Training Loss: 4.35724401473999\n",
            "Epoch: 1, Training Loss: 4.2007927894592285\n",
            "Epoch: 1, Training Loss: 4.25696325302124\n",
            "Epoch: 1, Training Loss: 4.272009372711182\n",
            "Epoch: 1, Training Loss: 4.1753621101379395\n",
            "Epoch: 1, Training Loss: 4.24400520324707\n",
            "Epoch: 1, Training Loss: 4.235301494598389\n",
            "Epoch: 1, Training Loss: 4.3411478996276855\n",
            "Epoch: 1, Training Loss: 4.491523265838623\n",
            "Epoch: 1, Training Loss: 4.202838897705078\n",
            "Epoch: 1, Training Loss: 4.203616619110107\n",
            "Epoch: 1, Training Loss: 4.225188732147217\n",
            "Epoch: 1, Training Loss: 4.371121406555176\n",
            "Epoch: 1, Training Loss: 4.211281776428223\n",
            "Epoch: 1, Training Loss: 4.245739459991455\n",
            "Epoch: 1, Training Loss: 4.116694927215576\n",
            "Epoch: 1, Training Loss: 4.045357704162598\n",
            "Epoch: 1, Training Loss: 4.043394565582275\n",
            "Epoch: 1, Training Loss: 4.0881757736206055\n",
            "Epoch: 1, Training Loss: 4.2087860107421875\n",
            "Epoch: 1, Training Loss: 4.048006057739258\n",
            "Epoch: 1, Training Loss: 4.354147434234619\n",
            "Epoch: 1, Training Loss: 4.025674343109131\n",
            "Epoch: 1, Training Loss: 4.224637985229492\n",
            "Epoch: 1, Training Loss: 4.209446907043457\n",
            "Epoch: 1, Training Loss: 4.179113388061523\n",
            "Epoch: 1, Training Loss: 4.024500370025635\n",
            "Epoch: 1, Training Loss: 4.234396457672119\n",
            "Epoch: 1, Training Loss: 4.04526948928833\n",
            "Epoch: 1, Training Loss: 4.241279125213623\n",
            "Epoch: 1, Training Loss: 4.217357635498047\n",
            "Epoch: 1, Training Loss: 4.124783992767334\n",
            "Epoch: 1, Training Loss: 4.250957012176514\n",
            "Epoch: 1, Training Loss: 4.189436435699463\n",
            "Epoch: 1, Training Loss: 4.00601863861084\n",
            "Epoch: 1, Training Loss: 4.136717319488525\n",
            "Epoch: 1, Training Loss: 4.213335990905762\n",
            "Epoch: 1, Training Loss: 4.147591590881348\n",
            "Epoch: 1, Training Loss: 4.183080196380615\n",
            "Epoch: 1, Training Loss: 4.339419364929199\n",
            "Epoch: 1, Training Loss: 4.2141289710998535\n",
            "Epoch: 1, Training Loss: 4.144296646118164\n",
            "Epoch: 1, Training Loss: 4.080239772796631\n",
            "Epoch: 1, Training Loss: 4.11517333984375\n",
            "Epoch: 1, Training Loss: 4.221954822540283\n",
            "Epoch: 1, Training Loss: 4.120275497436523\n",
            "Epoch: 1, Training Loss: 4.183207035064697\n",
            "Epoch: 1, Training Loss: 4.211966514587402\n",
            "Epoch: 1, Training Loss: 4.0156474113464355\n",
            "Epoch: 1, Training Loss: 4.21688175201416\n",
            "Epoch: 1, Training Loss: 4.105267524719238\n",
            "Epoch: 1, Training Loss: 3.9229612350463867\n",
            "Epoch: 1, Training Loss: 4.258476734161377\n",
            "Epoch: 1, Training Loss: 4.060963153839111\n",
            "Epoch: 1, Training Loss: 4.184575080871582\n",
            "Epoch: 1, Training Loss: 4.028811931610107\n",
            "Epoch: 1, Training Loss: 4.287204265594482\n",
            "Epoch: 1, Training Loss: 4.116959095001221\n",
            "Epoch: 1, Training Loss: 4.199586391448975\n",
            "Epoch: 1, Training Loss: 4.175846099853516\n",
            "Epoch: 1, Training Loss: 3.9288454055786133\n",
            "Epoch: 1, Training Loss: 4.0015997886657715\n",
            "Epoch: 1, Training Loss: 4.267982006072998\n",
            "Epoch: 1, Training Loss: 4.0169572830200195\n",
            "Epoch: 1, Training Loss: 4.097992420196533\n",
            "Epoch: 1, Training Loss: 4.092081546783447\n",
            "Epoch: 1, Training Loss: 4.090921401977539\n",
            "Epoch: 1, Training Loss: 4.17219877243042\n",
            "Epoch: 1, Training Loss: 4.090641021728516\n",
            "Epoch: 1, Training Loss: 4.055793762207031\n",
            "Epoch: 1, Training Loss: 4.184847831726074\n",
            "Epoch: 1, Training Loss: 4.121902942657471\n",
            "Epoch: 1, Training Loss: 4.165899753570557\n",
            "Epoch: 1, Training Loss: 4.137866020202637\n",
            "Epoch: 1, Training Loss: 4.624712944030762\n",
            "Epoch: 1, Training Loss: 4.117627143859863\n",
            "Epoch: 1, Training Loss: 3.8708276748657227\n",
            "Epoch: 1, Training Loss: 4.309114456176758\n",
            "Epoch: 1, Training Loss: 4.4067063331604\n",
            "Epoch: 1, Training Loss: 4.07167911529541\n",
            "Epoch: 1, Training Loss: 4.067272663116455\n",
            "Epoch: 1, Training Loss: 4.160412788391113\n",
            "Epoch: 1, Training Loss: 4.09744119644165\n",
            "Epoch: 1, Training Loss: 4.1450958251953125\n",
            "Epoch: 1, Training Loss: 4.037508010864258\n",
            "Epoch: 1, Training Loss: 4.26236629486084\n",
            "Epoch: 1, Training Loss: 3.9771299362182617\n",
            "Epoch: 1, Training Loss: 4.120570182800293\n",
            "Epoch: 1, Training Loss: 4.178360939025879\n",
            "Epoch: 1, Training Loss: 4.224700450897217\n",
            "Epoch: 1, Training Loss: 4.025611877441406\n",
            "Epoch: 1, Training Loss: 4.176695823669434\n",
            "Epoch: 1, Training Loss: 4.047952175140381\n",
            "Epoch: 1, Training Loss: 4.292575359344482\n",
            "Epoch: 1, Training Loss: 4.3626484870910645\n",
            "Epoch: 1, Training Loss: 4.124743938446045\n",
            "Epoch: 1, Training Loss: 4.317734718322754\n",
            "Epoch: 1, Training Loss: 4.10076379776001\n",
            "Epoch: 1, Training Loss: 4.138592720031738\n",
            "Epoch: 1, Training Loss: 3.806212902069092\n",
            "Epoch: 1, Training Loss: 4.021503448486328\n",
            "Epoch: 1, Training Loss: 4.198153018951416\n",
            "Epoch: 1, Training Loss: 4.061580657958984\n",
            "Epoch: 1, Training Loss: 4.152502059936523\n",
            "Epoch: 1, Training Loss: 4.223767280578613\n",
            "Epoch: 1, Training Loss: 4.0984625816345215\n",
            "Epoch: 1, Training Loss: 4.0830607414245605\n",
            "Epoch: 1, Training Loss: 4.026679515838623\n",
            "Epoch: 1, Training Loss: 4.266228675842285\n",
            "Epoch: 1, Training Loss: 4.08716344833374\n",
            "Epoch: 1, Training Loss: 4.0706400871276855\n",
            "Epoch: 1, Training Loss: 3.950441360473633\n",
            "Epoch: 1, Training Loss: 3.9528205394744873\n",
            "Epoch: 1, Training Loss: 4.1417388916015625\n",
            "Epoch: 1, Training Loss: 4.126903057098389\n",
            "Epoch: 1, Training Loss: 4.087475776672363\n",
            "Epoch: 1, Training Loss: 3.9269509315490723\n",
            "Epoch: 1, Training Loss: 4.357389450073242\n",
            "Epoch: 1, Training Loss: 4.096155643463135\n",
            "Epoch: 1, Training Loss: 4.246997833251953\n",
            "Epoch: 1, Training Loss: 4.190067291259766\n",
            "Epoch: 1, Training Loss: 4.154543876647949\n",
            "Epoch: 1, Training Loss: 4.4243903160095215\n",
            "Epoch: 1, Training Loss: 3.949688673019409\n",
            "Epoch: 1, Training Loss: 4.138939380645752\n",
            "Epoch: 1, Training Loss: 3.973587989807129\n",
            "Epoch: 1, Training Loss: 3.8436570167541504\n",
            "Epoch: 1, Training Loss: 4.105509281158447\n",
            "Epoch: 1, Training Loss: 4.046505928039551\n",
            "Epoch: 1, Training Loss: 4.187071800231934\n",
            "Epoch: 1, Training Loss: 4.115741729736328\n",
            "Epoch: 1, Training Loss: 3.9345922470092773\n",
            "Epoch: 1, Training Loss: 4.228670120239258\n",
            "Epoch: 1, Training Loss: 4.0746235847473145\n",
            "Epoch: 1, Training Loss: 4.152756690979004\n",
            "Epoch: 1, Training Loss: 3.96466326713562\n",
            "Epoch: 1, Training Loss: 3.9920284748077393\n",
            "Epoch: 1, Training Loss: 3.9601399898529053\n",
            "Epoch: 1, Training Loss: 4.125543117523193\n",
            "Epoch: 1, Training Loss: 4.027566432952881\n",
            "Epoch: 1, Training Loss: 4.085712432861328\n",
            "Epoch: 1, Training Loss: 4.034315586090088\n",
            "Epoch: 1, Training Loss: 4.0855278968811035\n",
            "Epoch: 1, Training Loss: 4.046417236328125\n",
            "Epoch: 1, Training Loss: 3.7505545616149902\n",
            "Epoch: 1, Training Loss: 3.9538326263427734\n",
            "Epoch: 1, Training Loss: 4.115187644958496\n",
            "Epoch: 1, Training Loss: 4.1181769371032715\n",
            "Epoch: 1, Training Loss: 4.042886257171631\n",
            "Epoch: 1, Training Loss: 4.212967395782471\n",
            "Epoch: 1, Training Loss: 4.254480361938477\n",
            "Epoch: 1, Training Loss: 3.9014923572540283\n",
            "Epoch: 1, Training Loss: 4.276883602142334\n",
            "Epoch: 1, Training Loss: 4.035145282745361\n",
            "Epoch: 1, Training Loss: 3.8980162143707275\n",
            "Epoch: 1, Training Loss: 4.098355293273926\n",
            "Epoch: 1, Training Loss: 4.040951728820801\n",
            "Epoch: 1, Training Loss: 3.9309613704681396\n",
            "Epoch: 1, Training Loss: 3.9178566932678223\n",
            "Epoch: 1, Training Loss: 4.169703960418701\n",
            "Epoch: 1, Training Loss: 4.051961898803711\n",
            "Epoch: 1, Training Loss: 4.109438419342041\n",
            "Epoch: 1, Training Loss: 4.09607458114624\n",
            "Epoch: 1, Training Loss: 4.075295925140381\n",
            "Epoch: 1, Training Loss: 3.907135248184204\n",
            "Epoch: 1, Training Loss: 3.8258285522460938\n",
            "Epoch: 1, Training Loss: 4.011016845703125\n",
            "Epoch: 1, Training Loss: 4.071579456329346\n",
            "Epoch: 1, Training Loss: 4.038060188293457\n",
            "Epoch: 1, Training Loss: 3.9124767780303955\n",
            "Epoch: 1, Training Loss: 4.145778656005859\n",
            "Epoch: 1, Training Loss: 3.9434356689453125\n",
            "Epoch: 1, Training Loss: 4.291611194610596\n",
            "Epoch: 1, Training Loss: 3.982944965362549\n",
            "Epoch: 1, Training Loss: 4.082346439361572\n",
            "Epoch: 1, Training Loss: 3.992232084274292\n",
            "Epoch: 1, Training Loss: 4.196603298187256\n",
            "Epoch: 1, Training Loss: 3.9956400394439697\n",
            "Epoch: 1, Training Loss: 4.148230075836182\n",
            "Epoch: 1, Training Loss: 4.323087692260742\n",
            "Epoch: 1, Training Loss: 3.8819785118103027\n",
            "Epoch: 1, Training Loss: 4.138966083526611\n",
            "Epoch: 1, Training Loss: 3.86079478263855\n",
            "Epoch: 1, Training Loss: 3.7857837677001953\n",
            "Epoch: 1, Training Loss: 3.940236806869507\n",
            "Epoch: 1, Training Loss: 3.9332237243652344\n",
            "Epoch: 1, Training Loss: 4.053879261016846\n",
            "Epoch: 1, Training Loss: 4.0383100509643555\n",
            "Epoch: 1, Training Loss: 3.940145254135132\n",
            "Epoch: 1, Training Loss: 4.092012405395508\n",
            "Epoch: 1, Training Loss: 3.7478792667388916\n",
            "Epoch: 1, Training Loss: 3.920395612716675\n",
            "Epoch: 1, Training Loss: 3.938314199447632\n",
            "Epoch: 1, Training Loss: 4.284790515899658\n",
            "Epoch: 1, Training Loss: 3.9135282039642334\n",
            "Epoch: 1, Training Loss: 3.926180839538574\n",
            "Epoch: 1, Training Loss: 3.9940757751464844\n",
            "Epoch: 1, Training Loss: 4.034690856933594\n",
            "Epoch: 1, Training Loss: 4.087891101837158\n",
            "Epoch: 1, Training Loss: 3.9418578147888184\n",
            "Epoch: 1, Training Loss: 3.96565580368042\n",
            "Epoch: 1, Training Loss: 4.095135688781738\n",
            "Epoch: 1, Training Loss: 3.7892298698425293\n",
            "Epoch: 1, Training Loss: 4.02590799331665\n",
            "Epoch: 1, Training Loss: 3.9259421825408936\n",
            "Epoch: 1, Training Loss: 4.032742500305176\n",
            "Epoch: 1, Training Loss: 3.894014596939087\n",
            "Epoch: 1, Training Loss: 4.037672519683838\n",
            "Epoch: 1, Training Loss: 3.8922979831695557\n",
            "Epoch: 1, Training Loss: 4.065293788909912\n",
            "Epoch: 1, Training Loss: 4.016185760498047\n",
            "Epoch: 1, Training Loss: 4.11860990524292\n",
            "Epoch: 1, Training Loss: 4.015872955322266\n",
            "Epoch: 1, Training Loss: 4.031921863555908\n",
            "Epoch: 1, Training Loss: 3.8355891704559326\n",
            "Epoch: 1, Training Loss: 4.1119866371154785\n",
            "Epoch: 1, Training Loss: 3.8087332248687744\n",
            "Epoch: 1, Training Loss: 3.9958956241607666\n",
            "Epoch: 1, Training Loss: 3.810896635055542\n",
            "Epoch: 1, Training Loss: 3.9873902797698975\n",
            "Epoch: 1, Training Loss: 4.052025318145752\n",
            "Epoch: 1, Training Loss: 4.14590311050415\n",
            "Epoch: 1, Training Loss: 3.7655553817749023\n",
            "Epoch: 1, Training Loss: 3.9784317016601562\n",
            "Epoch: 1, Training Loss: 4.015403747558594\n",
            "Epoch: 1, Training Loss: 3.8195786476135254\n",
            "Epoch: 1, Training Loss: 4.107452392578125\n",
            "Epoch: 1, Training Loss: 4.103055477142334\n",
            "Epoch: 1, Training Loss: 4.184761047363281\n",
            "Epoch: 1, Training Loss: 3.8475606441497803\n",
            "Epoch: 1, Training Loss: 3.9099721908569336\n",
            "Epoch: 1, Training Loss: 3.972425699234009\n",
            "Epoch: 1, Training Loss: 3.8689520359039307\n",
            "Epoch: 1, Training Loss: 4.085397243499756\n",
            "Epoch: 1, Training Loss: 3.7873551845550537\n",
            "Epoch: 1, Training Loss: 3.92327880859375\n",
            "Epoch: 1, Training Loss: 3.854290246963501\n",
            "Epoch: 1, Training Loss: 3.9938082695007324\n",
            "Epoch: 1, Training Loss: 4.115880012512207\n",
            "Epoch: 1, Training Loss: 3.9676406383514404\n",
            "Epoch: 1, Training Loss: 3.7502658367156982\n",
            "Epoch: 1, Training Loss: 3.879774570465088\n",
            "Epoch: 1, Training Loss: 3.9731318950653076\n",
            "Epoch: 1, Training Loss: 3.946789503097534\n",
            "Epoch: 1, Training Loss: 3.633341073989868\n",
            "Epoch: 1, Training Loss: 3.8306527137756348\n",
            "Epoch: 1, Training Loss: 3.876783847808838\n",
            "Epoch: 1, Training Loss: 3.794934034347534\n",
            "Epoch: 1, Training Loss: 3.9746150970458984\n",
            "Epoch: 1, Training Loss: 3.6247754096984863\n",
            "Epoch: 1, Training Loss: 4.111331939697266\n",
            "Epoch: 1, Training Loss: 3.9149842262268066\n",
            "Epoch: 1, Training Loss: 3.9675986766815186\n",
            "Epoch: 1, Training Loss: 3.816042184829712\n",
            "Epoch: 1, Training Loss: 3.8819026947021484\n",
            "Epoch: 1, Training Loss: 4.026488780975342\n",
            "Epoch: 1, Training Loss: 3.8286168575286865\n",
            "Epoch: 1, Training Loss: 3.9781718254089355\n",
            "Epoch: 1, Training Loss: 3.8271939754486084\n",
            "Epoch: 1, Training Loss: 3.897307872772217\n",
            "Epoch: 1, Training Loss: 3.7319328784942627\n",
            "Epoch: 1, Training Loss: 3.8011209964752197\n",
            "Epoch: 1, Training Loss: 3.9989821910858154\n",
            "Epoch: 1, Training Loss: 3.7621638774871826\n",
            "Epoch: 1, Training Loss: 3.8892571926116943\n",
            "Epoch: 1, Training Loss: 3.961474895477295\n",
            "Epoch: 1, Training Loss: 3.541379451751709\n",
            "Epoch: 1, Training Loss: 3.785076856613159\n",
            "Epoch: 1, Training Loss: 3.949556827545166\n",
            "Epoch: 1, Training Loss: 3.8901865482330322\n",
            "Epoch: 1, Training Loss: 3.8417575359344482\n",
            "Epoch: 1, Training Loss: 3.8818581104278564\n",
            "Epoch: 1, Training Loss: 4.000624179840088\n",
            "Epoch: 1, Training Loss: 3.7851572036743164\n",
            "Epoch: 1, Training Loss: 3.979182243347168\n",
            "Epoch: 1, Training Loss: 3.8744091987609863\n",
            "Epoch: 1, Training Loss: 3.929421901702881\n",
            "Epoch: 1, Training Loss: 4.0035834312438965\n",
            "Epoch: 1, Training Loss: 3.941208839416504\n",
            "Epoch: 1, Training Loss: 3.8215506076812744\n",
            "Epoch: 1, Training Loss: 3.9482970237731934\n",
            "Epoch: 1, Training Loss: 4.156721591949463\n",
            "Epoch: 1, Training Loss: 3.84314227104187\n",
            "Epoch: 1, Training Loss: 4.05389928817749\n",
            "Epoch: 1, Training Loss: 4.015739917755127\n",
            "Epoch: 1, Training Loss: 3.8786046504974365\n",
            "Epoch: 1, Training Loss: 3.8544116020202637\n",
            "Epoch: 1, Training Loss: 3.893893003463745\n",
            "Epoch: 1, Training Loss: 3.695359468460083\n",
            "Epoch: 1, Training Loss: 3.909008741378784\n",
            "Epoch: 1, Training Loss: 3.8053183555603027\n",
            "Epoch: 1, Training Loss: 3.746520519256592\n",
            "Epoch: 1, Training Loss: 3.9272525310516357\n",
            "Epoch: 1, Training Loss: 3.6881680488586426\n",
            "Epoch: 1, Training Loss: 3.8623595237731934\n",
            "Epoch: 1, Training Loss: 3.6322906017303467\n",
            "Epoch: 1, Training Loss: 4.073732376098633\n",
            "Epoch: 1, Training Loss: 3.9280598163604736\n",
            "Epoch: 1, Training Loss: 3.831890344619751\n",
            "Epoch: 1, Training Loss: 3.81771183013916\n",
            "Epoch: 1, Training Loss: 4.007258415222168\n",
            "Epoch: 1, Training Loss: 4.020257949829102\n",
            "Epoch: 1, Training Loss: 3.6136012077331543\n",
            "Epoch: 1, Training Loss: 4.1073479652404785\n",
            "Epoch: 1, Training Loss: 3.7785167694091797\n",
            "Epoch: 1, Training Loss: 4.03507661819458\n",
            "Epoch: 1, Training Loss: 3.9599180221557617\n",
            "Epoch: 1, Training Loss: 3.8225934505462646\n",
            "Epoch: 1, Training Loss: 3.9914376735687256\n",
            "Epoch: 1, Training Loss: 3.9807591438293457\n",
            "Epoch: 1, Training Loss: 3.863403797149658\n",
            "Epoch: 1, Training Loss: 3.914099931716919\n",
            "Epoch: 1, Training Loss: 4.021841526031494\n",
            "Epoch: 1, Training Loss: 4.029323577880859\n",
            "Epoch: 1, Training Loss: 3.8278086185455322\n",
            "Epoch: 1, Training Loss: 3.8843462467193604\n",
            "Epoch: 1, Training Loss: 3.949337959289551\n",
            "Epoch: 1, Training Loss: 3.801711082458496\n",
            "Epoch: 1, Training Loss: 3.761631965637207\n",
            "Epoch: 1, Training Loss: 3.7839510440826416\n",
            "Epoch: 1, Training Loss: 3.8403451442718506\n",
            "Epoch: 1, Training Loss: 3.7364659309387207\n",
            "Epoch: 1, Training Loss: 3.828702211380005\n",
            "Epoch: 1, Training Loss: 3.7376575469970703\n",
            "Epoch: 1, Training Loss: 3.9016530513763428\n",
            "Epoch: 1, Training Loss: 4.107400894165039\n",
            "Epoch: 1, Training Loss: 3.861682176589966\n",
            "Epoch: 1, Training Loss: 3.932422637939453\n",
            "Epoch: 1, Training Loss: 3.87374210357666\n",
            "Epoch: 1, Training Loss: 3.8312747478485107\n",
            "Epoch: 1, Training Loss: 3.8970346450805664\n",
            "Epoch: 1, Training Loss: 3.8794631958007812\n",
            "Epoch: 1, Training Loss: 3.9188358783721924\n",
            "Epoch: 1, Training Loss: 3.796085834503174\n",
            "Epoch: 1, Training Loss: 3.7947518825531006\n",
            "Epoch: 1, Training Loss: 3.722245216369629\n",
            "Epoch: 1, Training Loss: 4.0116705894470215\n",
            "Epoch: 1, Training Loss: 3.8092455863952637\n",
            "Epoch: 1, Training Loss: 3.976043462753296\n",
            "Epoch: 1, Training Loss: 4.003701686859131\n",
            "Epoch: 1, Training Loss: 3.7718443870544434\n",
            "Epoch: 1, Training Loss: 3.759976863861084\n",
            "Epoch: 1, Training Loss: 4.0033063888549805\n",
            "Epoch: 1, Training Loss: 3.9996182918548584\n",
            "Epoch: 1, Training Loss: 3.6211535930633545\n",
            "Epoch: 1, Training Loss: 3.8649892807006836\n",
            "Epoch: 1, Training Loss: 3.940403461456299\n",
            "Epoch: 1, Training Loss: 3.8339896202087402\n",
            "Epoch: 1, Training Loss: 3.9710564613342285\n",
            "Epoch: 1, Training Loss: 4.069611072540283\n",
            "Epoch: 1, Training Loss: 3.900683641433716\n",
            "Epoch: 1, Training Loss: 4.115537643432617\n",
            "Epoch: 1, Training Loss: 3.957993268966675\n",
            "Epoch: 1, Training Loss: 3.9925458431243896\n",
            "Epoch: 1, Training Loss: 3.8282384872436523\n",
            "Epoch: 1, Training Loss: 3.8395490646362305\n",
            "Epoch: 1, Training Loss: 3.864187717437744\n",
            "Epoch: 1, Training Loss: 3.7181951999664307\n",
            "Epoch: 1, Training Loss: 3.804551839828491\n",
            "Epoch: 1, Training Loss: 3.61651349067688\n",
            "Epoch: 1, Training Loss: 3.7893781661987305\n",
            "Epoch: 1, Training Loss: 3.8271617889404297\n",
            "Epoch: 1, Training Loss: 3.746429204940796\n",
            "Epoch: 1, Training Loss: 3.780912160873413\n",
            "Epoch: 1, Training Loss: 3.8137717247009277\n",
            "Epoch: 1, Training Loss: 3.6142091751098633\n",
            "Epoch: 1, Training Loss: 3.989847421646118\n",
            "Epoch: 1, Training Loss: 4.192571640014648\n",
            "Epoch: 1, Training Loss: 4.072963714599609\n",
            "Epoch: 1, Training Loss: 3.8743207454681396\n",
            "Epoch: 1, Training Loss: 3.7537152767181396\n",
            "Epoch: 1, Training Loss: 4.085309028625488\n",
            "Epoch: 1, Training Loss: 3.8179123401641846\n",
            "Epoch: 1, Training Loss: 3.6074814796447754\n",
            "Epoch: 1, Training Loss: 3.575979709625244\n",
            "Epoch: 1, Training Loss: 3.789654493331909\n",
            "Epoch: 1, Training Loss: 3.8130645751953125\n",
            "Epoch: 1, Training Loss: 3.6921064853668213\n",
            "Epoch: 1, Training Loss: 3.803473949432373\n",
            "Epoch: 1, Training Loss: 3.7347378730773926\n",
            "Epoch: 1, Training Loss: 3.767119884490967\n",
            "Epoch: 1, Training Loss: 3.7905426025390625\n",
            "Epoch: 1, Training Loss: 3.83504319190979\n",
            "Epoch: 1, Training Loss: 3.9762184619903564\n",
            "Epoch: 1, Training Loss: 3.628823757171631\n",
            "Epoch: 1, Training Loss: 3.8998477458953857\n",
            "Epoch: 1, Training Loss: 3.6970019340515137\n",
            "Epoch: 1, Training Loss: 3.761784791946411\n",
            "Epoch: 1, Training Loss: 3.783893585205078\n",
            "Epoch: 1, Training Loss: 3.547757148742676\n",
            "Epoch: 1, Training Loss: 3.9855711460113525\n",
            "Epoch: 1, Training Loss: 3.7231085300445557\n",
            "Epoch: 1, Training Loss: 3.759599447250366\n",
            "Epoch: 1, Training Loss: 3.860915422439575\n",
            "Epoch: 1, Training Loss: 3.8208038806915283\n",
            "Epoch: 1, Training Loss: 3.654629707336426\n",
            "Epoch: 1, Training Loss: 3.7839925289154053\n",
            "Epoch: 1, Training Loss: 3.77119517326355\n",
            "Epoch: 1, Training Loss: 3.861154317855835\n",
            "Epoch: 1, Training Loss: 3.784654378890991\n",
            "Epoch: 1, Training Loss: 3.6745505332946777\n",
            "Epoch: 1, Training Loss: 3.9212992191314697\n",
            "Epoch: 1, Training Loss: 3.88391375541687\n",
            "Epoch: 1, Training Loss: 3.8242318630218506\n",
            "Epoch: 1, Training Loss: 3.9074594974517822\n",
            "Epoch: 1, Training Loss: 3.7829082012176514\n",
            "Epoch: 1, Training Loss: 3.97147798538208\n",
            "Epoch: 1, Training Loss: 3.8699405193328857\n",
            "Epoch: 1, Training Loss: 3.997498035430908\n",
            "Epoch: 1, Training Loss: 3.5326106548309326\n",
            "Epoch: 1, Training Loss: 3.732516050338745\n",
            "Epoch: 1, Training Loss: 3.7411060333251953\n",
            "Epoch: 1, Training Loss: 3.8782787322998047\n",
            "Epoch: 1, Training Loss: 3.9419100284576416\n",
            "Epoch: 1, Training Loss: 3.7123162746429443\n",
            "Epoch: 1, Training Loss: 3.7410690784454346\n",
            "Epoch: 1, Training Loss: 3.6568174362182617\n",
            "Epoch: 1, Training Loss: 3.8037731647491455\n",
            "Epoch: 1, Training Loss: 3.733766555786133\n",
            "Epoch: 1, Training Loss: 3.562922477722168\n",
            "Epoch: 1, Training Loss: 3.818448066711426\n",
            "Epoch: 1, Training Loss: 3.83069109916687\n",
            "Epoch: 1, Training Loss: 3.8361001014709473\n",
            "Epoch: 1, Training Loss: 3.8479511737823486\n",
            "Epoch: 1, Training Loss: 3.869291067123413\n",
            "Epoch: 1, Training Loss: 3.8453938961029053\n",
            "Epoch: 1, Training Loss: 3.9512076377868652\n",
            "Epoch: 1, Training Loss: 3.9249229431152344\n",
            "Epoch: 1, Training Loss: 3.9133293628692627\n",
            "Epoch: 1, Training Loss: 4.050882339477539\n",
            "Epoch: 1, Training Loss: 3.7586252689361572\n",
            "Epoch: 1, Training Loss: 3.7368147373199463\n",
            "Epoch: 1, Training Loss: 3.686873197555542\n",
            "Epoch: 1, Training Loss: 3.9306600093841553\n",
            "Epoch: 1, Training Loss: 3.471095323562622\n",
            "Epoch: 1, Training Loss: 3.8598976135253906\n",
            "Epoch: 1, Training Loss: 3.6683766841888428\n",
            "Epoch: 1, Training Loss: 3.93654203414917\n",
            "Epoch: 1, Training Loss: 3.7720272541046143\n",
            "Epoch: 1, Training Loss: 3.835946559906006\n",
            "Epoch: 1, Training Loss: 3.6751794815063477\n",
            "Epoch: 1, Training Loss: 3.678842782974243\n",
            "Epoch: 1, Training Loss: 3.735330820083618\n",
            "Epoch: 1, Training Loss: 3.940342903137207\n",
            "Epoch: 1, Training Loss: 3.9300272464752197\n",
            "Epoch: 1, Training Loss: 3.7192208766937256\n",
            "Epoch: 1, Training Loss: 3.7565574645996094\n",
            "Epoch: 1, Training Loss: 3.7856218814849854\n",
            "Epoch: 1, Training Loss: 3.6189332008361816\n",
            "Epoch: 1, Training Loss: 3.917921304702759\n",
            "Epoch: 1, Training Loss: 3.8749845027923584\n",
            "Epoch: 1, Training Loss: 3.6975579261779785\n",
            "Epoch: 1, Training Loss: 3.6886098384857178\n",
            "Epoch: 1, Training Loss: 3.8080995082855225\n",
            "Epoch: 1, Training Loss: 3.724600076675415\n",
            "Epoch: 1, Training Loss: 3.667726993560791\n",
            "Epoch: 1, Training Loss: 3.684626817703247\n",
            "Epoch: 1, Training Loss: 3.8340513706207275\n",
            "Epoch: 1, Training Loss: 3.6864869594573975\n",
            "Epoch: 1, Training Loss: 3.6417675018310547\n",
            "Epoch: 1, Training Loss: 3.528500556945801\n",
            "Epoch: 1, Training Loss: 4.023368835449219\n",
            "Epoch: 1, Training Loss: 3.5603904724121094\n",
            "Epoch: 1, Training Loss: 3.7863948345184326\n",
            "Epoch: 1, Training Loss: 3.789346218109131\n",
            "Epoch: 1, Training Loss: 3.8792061805725098\n",
            "Epoch: 1, Training Loss: 3.737260580062866\n",
            "Epoch: 1, Training Loss: 3.9851393699645996\n",
            "Epoch: 1, Training Loss: 3.5894784927368164\n",
            "Epoch: 1, Training Loss: 3.4149343967437744\n",
            "Epoch: 1, Training Loss: 3.590637683868408\n",
            "Epoch: 1, Training Loss: 3.8578577041625977\n",
            "Epoch: 1, Training Loss: 3.786746025085449\n",
            "Epoch: 1, Training Loss: 3.8214633464813232\n",
            "Epoch: 1, Training Loss: 3.9273507595062256\n",
            "Epoch: 1, Training Loss: 3.7414464950561523\n",
            "Epoch: 1, Training Loss: 3.7514796257019043\n",
            "Epoch: 1, Training Loss: 3.7504525184631348\n",
            "Epoch: 1, Training Loss: 3.7393431663513184\n",
            "Epoch: 1, Training Loss: 3.7106990814208984\n",
            "Epoch: 1, Training Loss: 3.7533576488494873\n",
            "Epoch: 1, Training Loss: 3.8348793983459473\n",
            "Epoch: 1, Training Loss: 3.892164707183838\n",
            "Epoch: 1, Training Loss: 3.7044601440429688\n",
            "Epoch: 1, Training Loss: 3.343869209289551\n",
            "Epoch: 1, Training Loss: 3.6862869262695312\n",
            "Epoch: 1, Training Loss: 3.586904287338257\n",
            "Epoch: 1, Training Loss: 3.6378586292266846\n",
            "Epoch: 1, Training Loss: 3.6291327476501465\n",
            "Epoch: 1, Training Loss: 3.525367021560669\n",
            "Epoch: 1, Training Loss: 3.705059051513672\n",
            "Epoch: 1, Training Loss: 3.6251871585845947\n",
            "Epoch: 1, Training Loss: 3.383714199066162\n",
            "Epoch: 1, Training Loss: 3.506932258605957\n",
            "Epoch: 1, Training Loss: 3.6040332317352295\n",
            "Epoch: 1, Training Loss: 3.551710367202759\n",
            "Epoch: 1, Training Loss: 3.8176071643829346\n",
            "Epoch: 1, Training Loss: 3.633125066757202\n",
            "Epoch: 1, Training Loss: 3.5662925243377686\n",
            "Epoch: 1, Training Loss: 3.6988656520843506\n",
            "Epoch: 1, Training Loss: 3.6946828365325928\n",
            "Epoch: 1, Training Loss: 3.840339422225952\n",
            "Epoch: 1, Training Loss: 3.6341421604156494\n",
            "Epoch: 1, Training Loss: 3.629260778427124\n",
            "Epoch: 1, Training Loss: 3.58152437210083\n",
            "Epoch: 1, Training Loss: 3.6185243129730225\n",
            "Epoch: 1, Training Loss: 3.816185712814331\n",
            "Epoch: 1, Training Loss: 3.5194787979125977\n",
            "Epoch: 1, Training Loss: 3.8475279808044434\n",
            "Epoch: 1, Training Loss: 3.8048579692840576\n",
            "Epoch: 1, Training Loss: 3.6363251209259033\n",
            "Epoch: 1, Training Loss: 3.761591911315918\n",
            "Epoch: 1, Training Loss: 3.8825528621673584\n",
            "Epoch: 1, Training Loss: 3.9570374488830566\n",
            "Epoch: 1, Training Loss: 3.715578317642212\n",
            "Epoch: 1, Training Loss: 3.895580768585205\n",
            "Epoch: 1, Training Loss: 3.5758883953094482\n",
            "Epoch: 1, Training Loss: 3.762009859085083\n",
            "Epoch: 1, Training Loss: 3.8074748516082764\n",
            "Epoch: 1, Training Loss: 3.5517759323120117\n",
            "Epoch: 1, Training Loss: 3.483747959136963\n",
            "Epoch: 1, Training Loss: 3.5014328956604004\n",
            "Epoch: 1, Training Loss: 3.5762710571289062\n",
            "Epoch: 1, Training Loss: 3.5128254890441895\n",
            "Epoch: 1, Training Loss: 3.906158447265625\n",
            "Epoch: 1, Training Loss: 3.5799624919891357\n",
            "Epoch: 1, Training Loss: 3.6384763717651367\n",
            "Epoch: 1, Training Loss: 3.7401413917541504\n",
            "Epoch: 1, Training Loss: 3.708068609237671\n",
            "Epoch: 1, Training Loss: 3.7728707790374756\n",
            "Epoch: 1, Training Loss: 3.7767887115478516\n",
            "Epoch: 1, Training Loss: 3.57851505279541\n",
            "Epoch: 1, Training Loss: 3.7835030555725098\n",
            "Epoch: 1, Training Loss: 3.7630624771118164\n",
            "Epoch: 1, Training Loss: 3.4778213500976562\n",
            "Epoch: 1, Training Loss: 3.6774351596832275\n",
            "Epoch: 1, Training Loss: 3.737730026245117\n",
            "Epoch: 1, Training Loss: 3.4495482444763184\n",
            "Epoch: 1, Training Loss: 3.69686222076416\n",
            "Epoch: 1, Training Loss: 3.8511385917663574\n",
            "Epoch: 1, Training Loss: 3.711103677749634\n",
            "Epoch: 1, Training Loss: 3.854254722595215\n",
            "Epoch: 1, Training Loss: 3.8870177268981934\n",
            "Epoch: 1, Training Loss: 3.590540885925293\n",
            "Epoch: 1, Training Loss: 3.429983615875244\n",
            "Epoch: 1, Training Loss: 3.633685827255249\n",
            "Epoch: 1, Training Loss: 3.568931818008423\n",
            "Epoch: 1, Training Loss: 3.4869282245635986\n",
            "Epoch: 1, Training Loss: 3.4339120388031006\n",
            "Epoch: 1, Training Loss: 3.472665548324585\n",
            "Epoch: 1, Training Loss: 3.8021469116210938\n",
            "Epoch: 1, Training Loss: 3.6854896545410156\n",
            "Epoch: 1, Training Loss: 3.717649459838867\n",
            "Epoch: 1, Training Loss: 3.3632524013519287\n",
            "Epoch: 1, Training Loss: 3.663403034210205\n",
            "Epoch: 1, Training Loss: 3.7448890209198\n",
            "Epoch: 1, Training Loss: 3.504281997680664\n",
            "Epoch: 1, Training Loss: 3.440260648727417\n",
            "Epoch: 1, Training Loss: 3.8600687980651855\n",
            "Epoch: 1, Training Loss: 3.578380584716797\n",
            "Epoch: 1, Training Loss: 3.7989048957824707\n",
            "Epoch: 1, Training Loss: 3.4493114948272705\n",
            "Epoch: 1, Training Loss: 3.8638460636138916\n",
            "Epoch: 1, Training Loss: 3.59670090675354\n",
            "Epoch: 1, Training Loss: 3.7750961780548096\n",
            "Epoch: 1, Training Loss: 3.4927055835723877\n",
            "Epoch: 1, Training Loss: 3.4904329776763916\n",
            "Epoch: 1, Training Loss: 3.7430291175842285\n",
            "Epoch: 1, Training Loss: 3.664635181427002\n",
            "Epoch: 1, Training Loss: 3.7145230770111084\n",
            "Epoch: 1, Training Loss: 3.480114459991455\n",
            "Epoch: 1, Training Loss: 3.738473892211914\n",
            "Epoch: 1, Training Loss: 3.7504634857177734\n",
            "Epoch: 1, Training Loss: 3.620865821838379\n",
            "Epoch: 1, Training Loss: 3.496401071548462\n",
            "Epoch: 1, Training Loss: 3.70818829536438\n",
            "Epoch: 1, Training Loss: 3.585455894470215\n",
            "Epoch: 1, Training Loss: 3.578634738922119\n",
            "Epoch: 1, Training Loss: 3.6725096702575684\n",
            "Epoch: 1, Training Loss: 3.7941157817840576\n",
            "Epoch: 1, Training Loss: 3.4683890342712402\n",
            "Epoch: 1, Training Loss: 3.4771382808685303\n",
            "Epoch: 1, Training Loss: 3.639686346054077\n",
            "Epoch: 1, Training Loss: 3.531426429748535\n",
            "Epoch: 1, Training Loss: 3.530600070953369\n",
            "Epoch: 1, Training Loss: 3.6076505184173584\n",
            "Epoch: 1, Training Loss: 3.546358108520508\n",
            "Epoch: 1, Training Loss: 3.652196168899536\n",
            "Epoch: 1, Training Loss: 3.747436285018921\n",
            "Epoch: 1, Training Loss: 3.3517515659332275\n",
            "Epoch: 1, Training Loss: 3.665217161178589\n",
            "Epoch: 1, Training Loss: 3.549542188644409\n",
            "Epoch: 1, Training Loss: 3.6626245975494385\n",
            "Epoch: 1, Training Loss: 3.4200384616851807\n",
            "Epoch: 1, Training Loss: 3.6750142574310303\n",
            "Epoch: 1, Training Loss: 3.4926321506500244\n",
            "Epoch: 1, Training Loss: 3.9114365577697754\n",
            "Epoch: 1, Training Loss: 3.766383409500122\n",
            "Epoch: 1, Training Loss: 3.2886176109313965\n",
            "Epoch: 1, Training Loss: 3.31671142578125\n",
            "Epoch: 1, Training Loss: 4.0305280685424805\n",
            "Epoch: 1, Training Loss: 3.6794333457946777\n",
            "Epoch: 1, Training Loss: 3.623222589492798\n",
            "Epoch: 1, Training Loss: 4.047508239746094\n",
            "Epoch: 1, Training Loss: 3.6781859397888184\n",
            "Epoch: 1, Training Loss: 3.6560473442077637\n",
            "Epoch: 1, Training Loss: 3.699069023132324\n",
            "Epoch: 1, Training Loss: 3.568131446838379\n",
            "Epoch: 1, Training Loss: 3.612281322479248\n",
            "Epoch: 1, Training Loss: 3.2973809242248535\n",
            "Epoch: 1, Training Loss: 3.658203601837158\n",
            "Epoch: 1, Training Loss: 3.670344829559326\n",
            "Epoch: 1, Training Loss: 3.5702738761901855\n",
            "Epoch: 1, Training Loss: 3.5265493392944336\n",
            "Epoch: 1, Training Loss: 3.6227192878723145\n",
            "Epoch: 1, Training Loss: 3.7301340103149414\n",
            "Epoch: 1, Training Loss: 3.601928234100342\n",
            "Epoch: 1, Training Loss: 3.7446517944335938\n",
            "Epoch: 1, Training Loss: 3.6906867027282715\n",
            "Epoch: 1, Training Loss: 3.645341157913208\n",
            "Epoch: 1, Training Loss: 3.707413911819458\n",
            "Epoch: 1, Training Loss: 3.630890369415283\n",
            "Epoch: 1, Training Loss: 3.6661183834075928\n",
            "Epoch: 1, Training Loss: 3.503325939178467\n",
            "Epoch: 1, Training Loss: 3.584566354751587\n",
            "Epoch: 1, Training Loss: 3.4339165687561035\n",
            "Epoch: 1, Training Loss: 3.4337894916534424\n",
            "Epoch: 1, Training Loss: 3.743039608001709\n",
            "Epoch: 1, Training Loss: 3.8349647521972656\n",
            "Epoch: 1, Training Loss: 3.7271084785461426\n",
            "Epoch: 1, Training Loss: 3.6579742431640625\n",
            "Epoch: 1, Training Loss: 3.563115119934082\n",
            "Epoch: 1, Training Loss: 3.359583616256714\n",
            "Epoch: 1, Training Loss: 3.4422123432159424\n",
            "Epoch: 1, Training Loss: 3.422809600830078\n",
            "Epoch: 1, Training Loss: 3.6455163955688477\n",
            "Epoch: 1, Training Loss: 3.5335960388183594\n",
            "Epoch: 1, Training Loss: 3.705348014831543\n",
            "Epoch: 1, Training Loss: 3.6003644466400146\n",
            "Epoch: 1, Training Loss: 3.853032112121582\n",
            "Epoch: 1, Training Loss: 3.780303716659546\n",
            "Epoch: 1, Training Loss: 3.742222785949707\n",
            "Epoch: 1, Training Loss: 3.635744571685791\n",
            "Epoch: 1, Training Loss: 3.7486159801483154\n",
            "Epoch: 1, Training Loss: 3.6687116622924805\n",
            "Epoch: 1, Training Loss: 3.497800588607788\n",
            "Epoch: 1, Training Loss: 3.717031240463257\n",
            "Epoch: 1, Training Loss: 3.41934871673584\n",
            "Epoch: 1, Training Loss: 3.408158779144287\n",
            "Epoch: 1, Training Loss: 3.6195268630981445\n",
            "Epoch: 1, Training Loss: 3.420711040496826\n",
            "Epoch: 1, Training Loss: 3.6054794788360596\n",
            "Epoch: 1, Training Loss: 3.6797187328338623\n",
            "Epoch: 1, Training Loss: 3.6821000576019287\n",
            "Epoch: 1, Training Loss: 3.7159667015075684\n",
            "Epoch: 1, Training Loss: 3.360962390899658\n",
            "Epoch: 1, Training Loss: 3.5539188385009766\n",
            "Epoch: 1, Training Loss: 3.3107900619506836\n",
            "Epoch: 1, Training Loss: 3.6071343421936035\n",
            "Epoch: 1, Training Loss: 3.6896812915802\n",
            "Epoch: 1, Training Loss: 3.453087329864502\n",
            "Epoch: 1, Training Loss: 3.823925018310547\n",
            "Epoch: 1, Training Loss: 3.6478586196899414\n",
            "Epoch: 1, Training Loss: 3.7026731967926025\n",
            "Epoch: 1, Training Loss: 3.31889271736145\n",
            "Epoch: 1, Training Loss: 3.5954346656799316\n",
            "Epoch: 1, Training Loss: 3.543583869934082\n",
            "Epoch: 1, Training Loss: 3.449983596801758\n",
            "Epoch: 1, Training Loss: 3.9605090618133545\n",
            "Epoch: 1, Training Loss: 3.4002110958099365\n",
            "Epoch: 1, Training Loss: 3.455658197402954\n",
            "Epoch: 1, Training Loss: 3.4550342559814453\n",
            "Epoch: 1, Training Loss: 3.677868366241455\n",
            "Epoch: 1, Training Loss: 3.4531335830688477\n",
            "Epoch: 1, Training Loss: 3.796907424926758\n",
            "Epoch: 1, Training Loss: 3.133086681365967\n",
            "Epoch: 1, Training Loss: 3.7805817127227783\n",
            "Epoch: 1, Training Loss: 3.5038681030273438\n",
            "Epoch: 1, Training Loss: 3.605386734008789\n",
            "Epoch: 1, Training Loss: 3.719756841659546\n",
            "Epoch: 1, Training Loss: 3.619551181793213\n",
            "Epoch: 1, Training Loss: 3.4933698177337646\n",
            "Epoch: 1, Training Loss: 3.396186351776123\n",
            "Epoch: 1, Training Loss: 3.3276193141937256\n",
            "Epoch: 1, Training Loss: 3.5198216438293457\n",
            "Epoch: 1, Training Loss: 3.3984744548797607\n",
            "Epoch: 1, Training Loss: 3.3197693824768066\n",
            "Epoch: 1, Training Loss: 3.4590604305267334\n",
            "Epoch: 1, Training Loss: 3.8084819316864014\n",
            "Epoch: 1, Training Loss: 3.451107978820801\n",
            "Epoch: 1, Training Loss: 3.5311827659606934\n",
            "Epoch: 1, Training Loss: 3.6318647861480713\n",
            "Epoch: 1, Training Loss: 3.110229015350342\n",
            "Epoch: 1, Training Loss: 3.4391636848449707\n",
            "Epoch: 1, Training Loss: 3.499006748199463\n",
            "Epoch: 1, Training Loss: 3.5819575786590576\n",
            "Epoch: 1, Training Loss: 3.4036271572113037\n",
            "Epoch: 1, Training Loss: 3.580427408218384\n",
            "Epoch: 1, Training Loss: 3.600663661956787\n",
            "Epoch: 1, Training Loss: 3.5054566860198975\n",
            "Epoch: 1, Training Loss: 3.6986985206604004\n",
            "Epoch: 1, Training Loss: 3.332803726196289\n",
            "Epoch: 1, Training Loss: 3.513437271118164\n",
            "Epoch: 1, Training Loss: 3.6402587890625\n",
            "Epoch: 1, Training Loss: 3.566932439804077\n",
            "Epoch: 1, Training Loss: 3.3955581188201904\n",
            "Epoch: 1, Training Loss: 3.697662115097046\n",
            "Epoch: 1, Training Loss: 3.7131879329681396\n",
            "Epoch: 1, Training Loss: 3.4624578952789307\n",
            "Epoch: 1, Training Loss: 3.4988861083984375\n",
            "Epoch: 1, Training Loss: 3.5070438385009766\n",
            "Epoch: 1, Training Loss: 3.605178117752075\n",
            "Epoch: 1, Training Loss: 3.545377016067505\n",
            "Epoch: 1, Training Loss: 3.674874782562256\n",
            "Epoch: 1, Training Loss: 3.4475882053375244\n",
            "Epoch: 1, Training Loss: 3.14241099357605\n",
            "Epoch: 1, Training Loss: 3.5056865215301514\n",
            "Epoch: 1, Training Loss: 3.694495916366577\n",
            "Epoch: 1, Training Loss: 3.520963430404663\n",
            "Epoch: 1, Training Loss: 3.571105480194092\n",
            "Epoch: 1, Training Loss: 3.624551773071289\n",
            "Epoch: 1, Training Loss: 3.6066524982452393\n",
            "Epoch: 1, Training Loss: 3.652923107147217\n",
            "Epoch: 1, Training Loss: 3.63238787651062\n",
            "Epoch: 1, Training Loss: 3.4079296588897705\n",
            "Epoch: 1, Training Loss: 3.487295627593994\n",
            "Epoch: 1, Training Loss: 3.3753437995910645\n",
            "Epoch: 1, Training Loss: 3.7428300380706787\n",
            "Epoch: 1, Training Loss: 3.6685121059417725\n",
            "Epoch: 1, Training Loss: 3.4444336891174316\n",
            "Epoch: 1, Training Loss: 3.6450395584106445\n",
            "Epoch: 1, Training Loss: 3.3263938426971436\n",
            "Epoch: 1, Training Loss: 3.305359125137329\n",
            "Epoch: 1, Training Loss: 3.672682523727417\n",
            "Epoch: 1, Training Loss: 3.2983696460723877\n",
            "Epoch: 1, Training Loss: 3.63529896736145\n",
            "Epoch: 1, Training Loss: 3.4429373741149902\n",
            "Epoch: 1, Training Loss: 3.7946975231170654\n",
            "Epoch: 1, Training Loss: 3.787062406539917\n",
            "Epoch: 1, Training Loss: 3.685790538787842\n",
            "Epoch: 1, Training Loss: 3.6997230052948\n",
            "Epoch: 1, Training Loss: 3.4159462451934814\n",
            "Epoch: 1, Training Loss: 3.3883345127105713\n",
            "Epoch: 1, Training Loss: 3.653477668762207\n",
            "Epoch: 1, Training Loss: 3.5295920372009277\n",
            "Epoch: 1, Training Loss: 3.591109037399292\n",
            "Epoch: 1, Training Loss: 3.570735454559326\n",
            "Epoch: 1, Training Loss: 3.6500396728515625\n",
            "Epoch: 1, Training Loss: 3.5444183349609375\n",
            "Epoch: 1, Training Loss: 3.5498950481414795\n",
            "Epoch: 1, Training Loss: 3.5981833934783936\n",
            "Epoch: 1, Training Loss: 3.4978909492492676\n",
            "Epoch: 1, Training Loss: 3.676170587539673\n",
            "Epoch: 1, Training Loss: 3.565612554550171\n",
            "Epoch: 1, Training Loss: 3.3851125240325928\n",
            "Epoch: 1, Training Loss: 3.444281816482544\n",
            "Epoch: 1, Training Loss: 3.4893126487731934\n",
            "Epoch: 1, Training Loss: 3.4941155910491943\n",
            "Epoch: 1, Training Loss: 3.5166311264038086\n",
            "Epoch: 1, Training Loss: 3.603609085083008\n",
            "Epoch: 1, Training Loss: 3.455583095550537\n",
            "Epoch: 1, Training Loss: 3.3973584175109863\n",
            "Epoch: 1, Training Loss: 3.4900550842285156\n",
            "Epoch: 1, Training Loss: 3.7936580181121826\n",
            "Epoch: 1, Training Loss: 3.4796175956726074\n",
            "Epoch: 1, Training Loss: 3.5707015991210938\n",
            "Epoch: 1, Training Loss: 3.7016208171844482\n",
            "Epoch: 1, Training Loss: 3.44041109085083\n",
            "Epoch: 1, Training Loss: 3.8482906818389893\n",
            "Epoch: 1, Training Loss: 3.496938705444336\n",
            "Epoch: 1, Training Loss: 3.5603606700897217\n",
            "Epoch: 1, Training Loss: 3.4350292682647705\n",
            "Epoch: 1, Training Loss: 3.5480153560638428\n",
            "Epoch: 1, Training Loss: 3.2788639068603516\n",
            "Epoch: 1, Training Loss: 3.185544729232788\n",
            "Epoch: 1, Training Loss: 3.3817319869995117\n",
            "Epoch: 1, Training Loss: 3.4096388816833496\n",
            "Epoch: 1, Training Loss: 3.422218084335327\n",
            "Epoch: 1, Training Loss: 3.652663230895996\n",
            "Epoch: 1, Training Loss: 3.230764627456665\n",
            "Epoch: 1, Training Loss: 3.7389976978302\n",
            "Epoch: 1, Training Loss: 3.4338552951812744\n",
            "Epoch: 1, Training Loss: 3.4555721282958984\n",
            "Epoch: 1, Training Loss: 3.9520301818847656\n",
            "Epoch: 1, Training Loss: 3.5542030334472656\n",
            "Epoch: 1, Training Loss: 3.6682963371276855\n",
            "Epoch: 1, Training Loss: 3.697972297668457\n",
            "Epoch: 1, Training Loss: 3.3819174766540527\n",
            "Epoch: 1, Training Loss: 3.3347244262695312\n",
            "Epoch: 1, Training Loss: 3.4961206912994385\n",
            "Epoch: 1, Training Loss: 3.449108839035034\n",
            "Epoch: 1, Training Loss: 3.4555773735046387\n",
            "Epoch: 1, Training Loss: 3.4330763816833496\n",
            "Epoch: 1, Training Loss: 3.2552309036254883\n",
            "Epoch: 1, Training Loss: 3.453871011734009\n",
            "Epoch: 1, Training Loss: 3.2690956592559814\n",
            "Epoch: 1, Training Loss: 3.5712649822235107\n",
            "Epoch: 1, Training Loss: 3.79805064201355\n",
            "Epoch: 1, Training Loss: 3.4114091396331787\n",
            "Epoch: 1, Training Loss: 3.83925461769104\n",
            "Epoch: 1, Training Loss: 3.419548988342285\n",
            "Epoch: 1, Training Loss: 3.4326298236846924\n",
            "Epoch: 1, Training Loss: 3.4841902256011963\n",
            "Epoch: 1, Training Loss: 3.394306182861328\n",
            "Epoch: 1, Training Loss: 3.368720054626465\n",
            "Epoch: 1, Training Loss: 3.5875062942504883\n",
            "Epoch: 1, Training Loss: 3.6283607482910156\n",
            "Epoch: 1, Training Loss: 3.3589186668395996\n",
            "Epoch: 1, Training Loss: 3.568938732147217\n",
            "Epoch: 1, Training Loss: 3.5147886276245117\n",
            "Epoch: 1, Training Loss: 3.5501012802124023\n",
            "Epoch: 1, Training Loss: 3.4358811378479004\n",
            "Epoch: 1, Training Loss: 3.3722927570343018\n",
            "Epoch: 1, Training Loss: 3.638584852218628\n",
            "Epoch: 1, Training Loss: 3.5066699981689453\n",
            "Epoch: 1, Training Loss: 3.298954725265503\n",
            "Epoch: 1, Training Loss: 3.3575034141540527\n",
            "Epoch: 1, Training Loss: 3.2873694896698\n",
            "Epoch: 1, Training Loss: 3.6185429096221924\n",
            "Epoch: 1, Training Loss: 3.4844493865966797\n",
            "Epoch: 1, Training Loss: 3.494260311126709\n",
            "Epoch: 1, Training Loss: 3.362457036972046\n",
            "Epoch: 1, Training Loss: 3.50956654548645\n",
            "Epoch: 1, Training Loss: 3.765638589859009\n",
            "Epoch: 1, Training Loss: 3.369767189025879\n",
            "Epoch: 1, Training Loss: 3.5343852043151855\n",
            "Epoch: 1, Training Loss: 3.395242929458618\n",
            "Epoch: 1, Training Loss: 3.436258554458618\n",
            "Epoch: 1, Training Loss: 3.3762130737304688\n",
            "Epoch: 1, Training Loss: 3.457771062850952\n",
            "Epoch: 1, Training Loss: 3.6055171489715576\n",
            "Epoch: 1, Training Loss: 3.416177988052368\n",
            "Epoch: 1, Training Loss: 3.3609957695007324\n",
            "Epoch: 1, Training Loss: 3.3342220783233643\n",
            "Epoch: 1, Training Loss: 3.5870797634124756\n",
            "Epoch: 1, Training Loss: 3.3688580989837646\n",
            "Epoch: 1, Training Loss: 3.3152987957000732\n",
            "Epoch: 1, Training Loss: 3.738845109939575\n",
            "Epoch: 1, Training Loss: 3.2473721504211426\n",
            "Epoch: 1, Training Loss: 3.2713351249694824\n",
            "Epoch: 1, Training Loss: 3.4292027950286865\n",
            "Epoch: 1, Training Loss: 3.2994906902313232\n",
            "Epoch: 1, Training Loss: 3.1619040966033936\n",
            "Epoch: 1, Training Loss: 3.543440341949463\n",
            "Epoch: 1, Training Loss: 3.553100347518921\n",
            "Epoch: 1, Training Loss: 3.535003662109375\n",
            "Epoch: 1, Training Loss: 3.4144744873046875\n",
            "Epoch: 1, Training Loss: 3.4184112548828125\n",
            "Epoch: 1, Training Loss: 3.368410110473633\n",
            "Epoch: 1, Training Loss: 3.392448902130127\n",
            "Epoch: 1, Training Loss: 3.5147695541381836\n",
            "Epoch: 1, Training Loss: 3.459846258163452\n",
            "Epoch: 1, Training Loss: 3.424241542816162\n",
            "Epoch: 1, Training Loss: 3.7418625354766846\n",
            "Epoch: 1, Training Loss: 3.461494207382202\n",
            "Epoch: 1, Training Loss: 3.3319029808044434\n",
            "Epoch: 1, Training Loss: 3.2743847370147705\n",
            "Epoch: 1, Training Loss: 3.661823272705078\n",
            "Epoch: 1, Training Loss: 3.5442819595336914\n",
            "Epoch: 1, Training Loss: 3.4308550357818604\n",
            "Epoch: 1, Training Loss: 3.695124864578247\n",
            "Epoch: 1, Training Loss: 3.4867660999298096\n",
            "Epoch: 1, Training Loss: 3.3119609355926514\n",
            "Epoch: 1, Training Loss: 3.7250564098358154\n",
            "Epoch: 1, Training Loss: 3.3891549110412598\n",
            "Epoch: 1, Training Loss: 3.243234872817993\n",
            "Epoch: 1, Training Loss: 3.3798840045928955\n",
            "Epoch: 1, Training Loss: 3.4797348976135254\n",
            "Epoch: 1, Training Loss: 3.300509214401245\n",
            "Epoch: 1, Training Loss: 3.3281540870666504\n",
            "Epoch: 1, Training Loss: 3.2413647174835205\n",
            "Epoch: 1, Training Loss: 3.498122215270996\n",
            "Epoch: 1, Training Loss: 3.3645505905151367\n",
            "Epoch: 1, Training Loss: 3.2893788814544678\n",
            "Epoch: 1, Training Loss: 3.4479410648345947\n",
            "Epoch: 1, Training Loss: 3.5159149169921875\n",
            "Epoch: 1, Training Loss: 3.419581413269043\n",
            "Epoch: 1, Training Loss: 3.274505376815796\n",
            "Epoch: 1, Training Loss: 3.6869723796844482\n",
            "Epoch: 1, Training Loss: 3.5780906677246094\n",
            "Epoch: 1, Training Loss: 3.4004623889923096\n",
            "Epoch: 1, Training Loss: 3.5096402168273926\n",
            "Epoch: 1, Training Loss: 3.7212398052215576\n",
            "Epoch: 1, Training Loss: 3.385577917098999\n",
            "Epoch: 1, Training Loss: 3.2871267795562744\n",
            "Epoch: 1, Training Loss: 3.298482894897461\n",
            "Epoch: 1, Training Loss: 3.516193151473999\n",
            "Epoch: 1, Training Loss: 3.4567623138427734\n",
            "Epoch: 1, Training Loss: 3.486713171005249\n",
            "Epoch: 1, Training Loss: 3.58988356590271\n",
            "Epoch: 1, Training Loss: 3.3048462867736816\n",
            "Epoch: 1, Training Loss: 3.5804953575134277\n",
            "Epoch: 1, Training Loss: 3.0639030933380127\n",
            "Epoch: 1, Training Loss: 3.2245898246765137\n",
            "Epoch: 1, Training Loss: 3.2048916816711426\n",
            "Epoch: 1, Training Loss: 3.330381155014038\n",
            "Epoch: 1, Training Loss: 3.576738119125366\n",
            "Epoch: 1, Training Loss: 3.460019588470459\n",
            "Epoch: 1, Training Loss: 3.251305341720581\n",
            "Epoch: 1, Training Loss: 3.4493770599365234\n",
            "Epoch: 1, Training Loss: 3.5062193870544434\n",
            "Epoch: 1, Training Loss: 3.566056966781616\n",
            "Epoch: 1, Training Loss: 3.4995005130767822\n",
            "Epoch: 1, Training Loss: 3.406151056289673\n",
            "Epoch: 1, Training Loss: 3.3179025650024414\n",
            "Epoch: 1, Training Loss: 3.3979249000549316\n",
            "Epoch: 1, Training Loss: 3.5082130432128906\n",
            "Epoch: 1, Training Loss: 3.265406370162964\n",
            "Epoch: 1, Training Loss: 3.326934814453125\n",
            "Epoch: 1, Training Loss: 3.449070692062378\n",
            "Epoch: 1, Training Loss: 3.320249080657959\n",
            "Epoch: 1, Training Loss: 3.4337427616119385\n",
            "Epoch: 1, Training Loss: 3.420341968536377\n",
            "Epoch: 1, Training Loss: 3.5966193675994873\n",
            "Epoch: 1, Training Loss: 3.394199848175049\n",
            "Epoch: 1, Training Loss: 3.3063459396362305\n",
            "Epoch: 1, Training Loss: 3.594454526901245\n",
            "Epoch: 1, Training Loss: 3.4961185455322266\n",
            "Epoch: 1, Training Loss: 3.5493528842926025\n",
            "Epoch: 1, Training Loss: 3.4393112659454346\n",
            "Epoch: 1, Training Loss: 3.2766194343566895\n",
            "Epoch: 1, Training Loss: 3.4737048149108887\n",
            "Epoch: 1, Training Loss: 3.2843542098999023\n",
            "Epoch: 1, Training Loss: 3.380364418029785\n",
            "Epoch: 1, Training Loss: 3.3894410133361816\n",
            "Epoch: 1, Training Loss: 3.5410475730895996\n",
            "Epoch: 1, Training Loss: 3.2352826595306396\n",
            "Epoch: 1, Training Loss: 3.2516682147979736\n",
            "Epoch: 1, Training Loss: 3.5239014625549316\n",
            "Epoch: 1, Training Loss: 3.3789987564086914\n",
            "Epoch: 1, Training Loss: 3.492007255554199\n",
            "Epoch: 1, Training Loss: 3.3804666996002197\n",
            "Epoch: 1, Training Loss: 3.5019261837005615\n",
            "Epoch: 1, Training Loss: 3.398555278778076\n",
            "Epoch: 1, Training Loss: 3.2778728008270264\n",
            "Epoch: 1, Training Loss: 3.541790723800659\n",
            "Epoch: 1, Training Loss: 3.210057020187378\n",
            "Epoch: 1, Training Loss: 3.2027859687805176\n",
            "Epoch: 1, Training Loss: 3.422077178955078\n",
            "Epoch: 1, Training Loss: 3.3297576904296875\n",
            "Epoch: 1, Training Loss: 3.4609928131103516\n",
            "Epoch: 1, Training Loss: 3.529341220855713\n",
            "Epoch: 1, Training Loss: 3.0617856979370117\n",
            "Epoch: 1, Training Loss: 3.540745973587036\n",
            "Epoch: 1, Training Loss: 3.4855222702026367\n",
            "Epoch: 1, Training Loss: 3.6956746578216553\n",
            "Epoch: 1, Training Loss: 3.2954602241516113\n",
            "Epoch: 1, Training Loss: 3.1774964332580566\n",
            "Epoch: 1, Training Loss: 3.3706724643707275\n",
            "Epoch: 1, Training Loss: 3.036811351776123\n",
            "Epoch: 1, Training Loss: 3.6129839420318604\n",
            "Epoch: 1, Training Loss: 3.5669362545013428\n",
            "Epoch: 1, Training Loss: 3.394988536834717\n",
            "Epoch: 1, Training Loss: 3.5037732124328613\n",
            "Epoch: 1, Training Loss: 3.44600248336792\n",
            "Epoch: 1, Training Loss: 3.5925185680389404\n",
            "Epoch: 1, Training Loss: 3.544863224029541\n",
            "Epoch: 1, Training Loss: 3.4490652084350586\n",
            "Epoch: 1, Training Loss: 3.3675849437713623\n",
            "Epoch: 1, Training Loss: 3.3294031620025635\n",
            "Epoch: 1, Training Loss: 3.0773210525512695\n",
            "Epoch: 1, Training Loss: 3.3716940879821777\n",
            "Epoch: 1, Training Loss: 3.2510461807250977\n",
            "Epoch: 1, Training Loss: 3.6159255504608154\n",
            "Epoch: 1, Training Loss: 3.567761182785034\n",
            "Epoch: 1, Training Loss: 3.1938130855560303\n",
            "Epoch: 1, Training Loss: 3.6541147232055664\n",
            "Epoch: 1, Training Loss: 3.5087552070617676\n",
            "Epoch: 1, Training Loss: 3.2460124492645264\n",
            "Epoch: 1, Training Loss: 3.257575750350952\n",
            "Epoch: 1, Training Loss: 3.4636542797088623\n",
            "Epoch: 1, Training Loss: 3.3177220821380615\n",
            "Epoch: 1, Training Loss: 3.537871837615967\n",
            "Epoch: 1, Training Loss: 3.3767688274383545\n",
            "Epoch: 1, Training Loss: 3.4094443321228027\n",
            "Epoch: 1, Training Loss: 2.907625436782837\n",
            "Epoch: 1, Training Loss: 3.409773349761963\n",
            "Epoch: 1, Training Loss: 3.667449712753296\n",
            "Epoch: 1, Training Loss: 3.5095694065093994\n",
            "Epoch: 1, Training Loss: 3.3821897506713867\n",
            "Epoch: 1, Training Loss: 3.388949394226074\n",
            "Epoch: 1, Training Loss: 3.4243109226226807\n",
            "Epoch: 1, Training Loss: 3.3494198322296143\n",
            "Epoch: 1, Training Loss: 3.4809422492980957\n",
            "Epoch: 1, Training Loss: 3.4572372436523438\n",
            "Epoch: 1, Training Loss: 3.472734212875366\n",
            "Epoch: 1, Training Loss: 3.5020527839660645\n",
            "Epoch: 1, Training Loss: 3.338963747024536\n",
            "Epoch: 1, Training Loss: 3.3867239952087402\n",
            "Epoch: 1, Training Loss: 3.1770517826080322\n",
            "Epoch: 1, Training Loss: 3.4418482780456543\n",
            "Epoch: 1, Training Loss: 3.386152744293213\n",
            "Epoch: 1, Training Loss: 3.569315195083618\n",
            "Epoch: 1, Training Loss: 3.391561269760132\n",
            "Epoch: 1, Training Loss: 3.1611080169677734\n",
            "Epoch: 1, Training Loss: 3.365844249725342\n",
            "Epoch: 1, Training Loss: 3.487736701965332\n",
            "Epoch: 1, Training Loss: 3.213291645050049\n",
            "Epoch: 1, Training Loss: 3.6754159927368164\n",
            "Epoch: 1, Training Loss: 3.368983507156372\n",
            "Epoch: 1, Training Loss: 3.224241018295288\n",
            "Epoch: 1, Training Loss: 3.267503499984741\n",
            "Epoch: 1, Training Loss: 3.4800543785095215\n",
            "Epoch: 1, Training Loss: 3.336196184158325\n",
            "Epoch: 1, Training Loss: 3.4384849071502686\n",
            "Epoch: 1, Training Loss: 3.258551836013794\n",
            "Epoch: 1, Training Loss: 3.4398186206817627\n",
            "Epoch: 1, Training Loss: 3.2590670585632324\n",
            "Epoch: 1, Training Loss: 3.2188735008239746\n",
            "Epoch: 1, Training Loss: 3.362215995788574\n",
            "Epoch: 1, Training Loss: 3.3298306465148926\n",
            "Epoch: 1, Training Loss: 3.575230360031128\n",
            "Epoch: 1, Training Loss: 3.4079275131225586\n",
            "Epoch: 1, Training Loss: 3.326603889465332\n",
            "Epoch: 1, Training Loss: 3.495885133743286\n",
            "Epoch: 1, Training Loss: 3.479288339614868\n",
            "Epoch: 1, Training Loss: 3.3652870655059814\n",
            "Epoch: 1, Training Loss: 3.329836845397949\n",
            "Epoch: 1, Training Loss: 3.4077274799346924\n",
            "Epoch: 1, Training Loss: 3.2129907608032227\n",
            "Epoch: 1, Training Loss: 3.370096206665039\n",
            "Epoch: 1, Training Loss: 3.417781352996826\n",
            "Epoch: 1, Training Loss: 3.3711981773376465\n",
            "Epoch: 1, Training Loss: 3.3599936962127686\n",
            "Epoch: 1, Training Loss: 3.2092716693878174\n",
            "Epoch: 1, Training Loss: 3.3122034072875977\n",
            "Epoch: 1, Training Loss: 3.67348575592041\n",
            "Epoch: 1, Training Loss: 3.3203251361846924\n",
            "Epoch: 1, Training Loss: 3.53910756111145\n",
            "Epoch: 1, Training Loss: 3.5012500286102295\n",
            "Epoch: 1, Training Loss: 3.4971065521240234\n",
            "Epoch: 1, Training Loss: 3.399402618408203\n",
            "Epoch: 1, Training Loss: 3.4514400959014893\n",
            "Epoch: 1, Training Loss: 3.250312328338623\n",
            "Epoch: 1, Training Loss: 3.193983554840088\n",
            "Epoch: 1, Training Loss: 3.446392059326172\n",
            "Epoch: 1, Training Loss: 3.098655939102173\n",
            "Epoch: 1, Training Loss: 3.2467517852783203\n",
            "Epoch: 1, Training Loss: 3.3754427433013916\n",
            "Epoch: 1, Training Loss: 3.347856044769287\n",
            "Epoch: 1, Training Loss: 3.607491970062256\n",
            "Epoch: 1, Training Loss: 3.2019543647766113\n",
            "Epoch: 1, Training Loss: 3.5008246898651123\n",
            "Epoch: 1, Training Loss: 3.4766831398010254\n",
            "Epoch: 1, Training Loss: 3.229649543762207\n",
            "Epoch: 1, Training Loss: 3.328854560852051\n",
            "Epoch: 1, Training Loss: 3.2046477794647217\n",
            "Epoch: 1, Training Loss: 3.014387369155884\n",
            "Epoch: 1, Training Loss: 3.140291452407837\n",
            "Epoch: 1, Training Loss: 3.287478446960449\n",
            "Epoch: 1, Training Loss: 3.236117362976074\n",
            "Epoch: 1, Training Loss: 3.1518666744232178\n",
            "Epoch: 1, Training Loss: 3.327237844467163\n",
            "Epoch: 1, Training Loss: 3.26289963722229\n",
            "Epoch: 1, Training Loss: 3.2484421730041504\n",
            "Epoch: 1, Training Loss: 3.0101168155670166\n",
            "Epoch: 1, Training Loss: 3.2102043628692627\n",
            "Epoch: 1, Training Loss: 3.5210165977478027\n",
            "Epoch: 1, Training Loss: 3.4402272701263428\n",
            "Epoch: 1, Training Loss: 3.3536148071289062\n",
            "Epoch: 1, Training Loss: 3.402440071105957\n",
            "Epoch: 1, Training Loss: 3.46634578704834\n",
            "Epoch: 1, Training Loss: 3.0528643131256104\n",
            "Epoch: 1, Training Loss: 3.619961738586426\n",
            "Epoch: 1, Training Loss: 3.301041841506958\n",
            "Epoch: 1, Training Loss: 3.608630657196045\n",
            "Epoch: 1, Training Loss: 3.3920912742614746\n",
            "Epoch: 1, Training Loss: 3.051412343978882\n",
            "Epoch: 1, Training Loss: 3.2415263652801514\n",
            "Epoch: 1, Training Loss: 3.3975701332092285\n",
            "Epoch: 1, Training Loss: 3.290656566619873\n",
            "Epoch: 1, Training Loss: 3.174837350845337\n",
            "Epoch: 1, Training Loss: 3.2165069580078125\n",
            "Epoch: 1, Training Loss: 3.075660228729248\n",
            "Epoch: 1, Training Loss: 3.2895994186401367\n",
            "Epoch: 1, Training Loss: 3.404141664505005\n",
            "Epoch: 1, Training Loss: 3.1344428062438965\n",
            "Epoch: 1, Training Loss: 3.027798891067505\n",
            "Epoch: 1, Training Loss: 3.4164390563964844\n",
            "Epoch: 1, Training Loss: 3.2097790241241455\n",
            "Epoch: 1, Training Loss: 3.2979729175567627\n",
            "Epoch: 1, Training Loss: 3.479036331176758\n",
            "Epoch: 1, Training Loss: 3.5196542739868164\n",
            "Epoch: 1, Training Loss: 3.2665915489196777\n",
            "Epoch: 1, Training Loss: 3.540024995803833\n",
            "Epoch: 1, Training Loss: 3.5977396965026855\n",
            "Epoch: 1, Training Loss: 3.399064064025879\n",
            "Epoch: 1, Training Loss: 3.840594530105591\n",
            "Epoch: 1, Training Loss: 3.3279571533203125\n",
            "Epoch: 1, Training Loss: 3.250516891479492\n",
            "Epoch: 1, Training Loss: 3.5723047256469727\n",
            "Epoch: 1, Training Loss: 3.4265859127044678\n",
            "Epoch: 1, Training Loss: 3.2011313438415527\n",
            "Epoch: 1, Training Loss: 3.0690245628356934\n",
            "Epoch: 1, Training Loss: 3.5947186946868896\n",
            "Epoch: 1, Training Loss: 3.310108184814453\n",
            "Epoch: 1, Training Loss: 3.2854959964752197\n",
            "Epoch: 1, Training Loss: 3.401496648788452\n",
            "Epoch: 1, Training Loss: 3.507859468460083\n",
            "Epoch: 1, Training Loss: 3.297924041748047\n",
            "Epoch: 1, Training Loss: 3.1543643474578857\n",
            "Epoch: 1, Training Loss: 3.3337457180023193\n",
            "Epoch: 1, Training Loss: 2.989734172821045\n",
            "Epoch: 1, Training Loss: 3.2103676795959473\n",
            "Epoch: 1, Training Loss: 3.308088541030884\n",
            "Epoch: 1, Training Loss: 3.18058180809021\n",
            "Epoch: 1, Training Loss: 3.3641843795776367\n",
            "Epoch: 1, Training Loss: 3.1192564964294434\n",
            "Epoch: 1, Training Loss: 3.408302068710327\n",
            "Epoch: 1, Training Loss: 3.417965888977051\n",
            "Epoch: 1, Training Loss: 3.2383716106414795\n",
            "Epoch: 1, Training Loss: 3.185197591781616\n",
            "Epoch: 1, Training Loss: 3.5076375007629395\n",
            "Epoch: 1, Training Loss: 3.321169137954712\n",
            "Epoch: 1, Training Loss: 3.3718583583831787\n",
            "Epoch: 1, Training Loss: 3.1870408058166504\n",
            "Epoch: 1, Training Loss: 3.568335771560669\n",
            "Epoch: 1, Training Loss: 3.4097495079040527\n",
            "Epoch: 1, Training Loss: 3.2817869186401367\n",
            "Epoch: 1, Training Loss: 3.375938653945923\n",
            "Epoch: 1, Training Loss: 3.305586099624634\n",
            "Epoch: 1, Training Loss: 3.2057318687438965\n",
            "Epoch: 1, Training Loss: 3.118732213973999\n",
            "Epoch: 1, Training Loss: 3.3764359951019287\n",
            "Epoch: 1, Training Loss: 3.3138604164123535\n",
            "Epoch: 1, Training Loss: 3.2887134552001953\n",
            "Epoch: 1, Training Loss: 3.2740285396575928\n",
            "Epoch: 1, Training Loss: 3.2839853763580322\n",
            "Epoch: 1, Training Loss: 3.25036358833313\n",
            "Epoch: 1, Training Loss: 3.4782755374908447\n",
            "Epoch: 1, Training Loss: 3.165653944015503\n",
            "Epoch: 1, Training Loss: 3.3548741340637207\n",
            "Epoch: 1, Training Loss: 3.3958628177642822\n",
            "Epoch: 1, Training Loss: 3.1552419662475586\n",
            "Epoch: 1, Training Loss: 3.1411960124969482\n",
            "Epoch: 1, Training Loss: 3.3966989517211914\n",
            "Epoch: 1, Training Loss: 3.1076109409332275\n",
            "Epoch: 1, Training Loss: 3.2519073486328125\n",
            "Epoch: 1, Training Loss: 3.2900643348693848\n",
            "Epoch: 1, Training Loss: 3.185049057006836\n",
            "Epoch: 1, Training Loss: 3.3228261470794678\n",
            "Epoch: 1, Training Loss: 3.115311622619629\n",
            "Epoch: 1, Training Loss: 3.1470530033111572\n",
            "Epoch: 1, Training Loss: 3.324958086013794\n",
            "Epoch: 1, Training Loss: 3.2511751651763916\n",
            "Epoch: 1, Training Loss: 3.3338754177093506\n",
            "Epoch: 1, Training Loss: 3.4169490337371826\n",
            "Epoch: 1, Training Loss: 3.4012629985809326\n",
            "Epoch: 1, Training Loss: 3.1828572750091553\n",
            "Epoch: 1, Training Loss: 2.9340507984161377\n",
            "Epoch: 1, Training Loss: 3.2600975036621094\n",
            "Epoch: 1, Training Loss: 3.574653148651123\n",
            "Epoch: 1, Training Loss: 3.5078861713409424\n",
            "Epoch: 1, Training Loss: 3.408322811126709\n",
            "Epoch: 1, Training Loss: 3.124997138977051\n",
            "Epoch: 1, Training Loss: 3.1396350860595703\n",
            "Epoch: 1, Training Loss: 3.1981852054595947\n",
            "Epoch: 1, Training Loss: 3.274914026260376\n",
            "Epoch: 1, Training Loss: 3.5037059783935547\n",
            "Epoch: 1, Training Loss: 3.280332326889038\n",
            "Epoch: 1, Training Loss: 3.124009609222412\n",
            "Epoch: 1, Training Loss: 3.219926595687866\n",
            "Epoch: 1, Training Loss: 3.391625165939331\n",
            "Epoch: 1, Training Loss: 3.1007578372955322\n",
            "Epoch: 1, Training Loss: 2.977851629257202\n",
            "Epoch: 1, Training Loss: 3.0973100662231445\n",
            "Epoch: 1, Training Loss: 3.238740921020508\n",
            "Epoch: 1, Training Loss: 3.359165906906128\n",
            "Epoch: 1, Training Loss: 3.153640031814575\n",
            "Epoch: 1, Training Loss: 3.2743866443634033\n",
            "Epoch: 1, Training Loss: 3.406938076019287\n",
            "Epoch: 1, Training Loss: 3.2890124320983887\n",
            "Epoch: 1, Training Loss: 3.294438123703003\n",
            "Epoch: 1, Training Loss: 3.124669313430786\n",
            "Epoch: 1, Training Loss: 3.22607684135437\n",
            "Epoch: 1, Training Loss: 3.2817039489746094\n",
            "Epoch: 1, Training Loss: 3.1458306312561035\n",
            "Epoch: 1, Training Loss: 3.723757028579712\n",
            "Epoch: 1, Training Loss: 3.331160068511963\n",
            "Epoch: 1, Training Loss: 3.37239670753479\n",
            "Epoch: 1, Training Loss: 3.2497291564941406\n",
            "Epoch: 1, Training Loss: 2.8813374042510986\n",
            "Epoch: 1, Training Loss: 3.467698097229004\n",
            "Epoch: 1, Training Loss: 3.356215000152588\n",
            "Epoch: 1, Training Loss: 3.126199722290039\n",
            "Epoch: 1, Training Loss: 3.2982521057128906\n",
            "Epoch: 1, Training Loss: 3.2188572883605957\n",
            "Epoch: 1, Training Loss: 3.3930742740631104\n",
            "Epoch: 1, Training Loss: 3.3715412616729736\n",
            "Epoch: 1, Training Loss: 3.5223000049591064\n",
            "Epoch: 1, Training Loss: 3.226043462753296\n",
            "Epoch: 1, Training Loss: 3.187743902206421\n",
            "Epoch: 1, Training Loss: 3.0747392177581787\n",
            "Epoch: 1, Training Loss: 3.269563913345337\n",
            "Epoch: 1, Training Loss: 3.3385777473449707\n",
            "Epoch: 1, Training Loss: 3.1401305198669434\n",
            "Epoch: 1, Training Loss: 2.8236470222473145\n",
            "Epoch: 1, Training Loss: 3.278810739517212\n",
            "Epoch: 1, Training Loss: 3.412980794906616\n",
            "Epoch: 1, Training Loss: 3.4510183334350586\n",
            "Epoch: 1, Training Loss: 3.087117910385132\n",
            "Epoch: 1, Training Loss: 3.173375368118286\n",
            "Epoch: 1, Training Loss: 3.2745320796966553\n",
            "Epoch: 1, Training Loss: 3.2843689918518066\n",
            "Epoch: 1, Training Loss: 3.0263941287994385\n",
            "Epoch: 1, Training Loss: 3.1737332344055176\n",
            "Epoch: 1, Training Loss: 3.307223081588745\n",
            "Epoch: 1, Training Loss: 3.306680679321289\n",
            "Epoch: 1, Training Loss: 3.1815149784088135\n",
            "Epoch: 1, Training Loss: 3.340940237045288\n",
            "Epoch: 1, Training Loss: 3.0854644775390625\n",
            "Epoch: 1, Training Loss: 3.49180269241333\n",
            "Epoch: 1, Training Loss: 3.2889039516448975\n",
            "Epoch: 1, Training Loss: 3.14735746383667\n",
            "Epoch: 1, Training Loss: 3.1025311946868896\n",
            "Epoch: 1, Training Loss: 3.3889122009277344\n",
            "Epoch: 1, Training Loss: 3.5375473499298096\n",
            "Epoch: 1, Training Loss: 3.2390947341918945\n",
            "Epoch: 1, Training Loss: 3.1682119369506836\n",
            "Epoch: 1, Training Loss: 3.440814733505249\n",
            "Epoch: 1, Training Loss: 3.1395769119262695\n",
            "Epoch: 1, Training Loss: 3.0756092071533203\n",
            "Epoch: 1, Training Loss: 2.9970390796661377\n",
            "Epoch: 1, Training Loss: 3.3396294116973877\n",
            "Epoch: 1, Training Loss: 3.203280210494995\n",
            "Epoch: 1, Training Loss: 2.9854865074157715\n",
            "Epoch: 1, Training Loss: 3.255492687225342\n",
            "Epoch: 1, Training Loss: 3.484163761138916\n",
            "Epoch: 1, Training Loss: 3.1609535217285156\n",
            "Epoch: 1, Training Loss: 3.3099169731140137\n",
            "Epoch: 1, Training Loss: 3.1387240886688232\n",
            "Epoch: 1, Training Loss: 3.529693365097046\n",
            "Epoch: 1, Training Loss: 3.2821195125579834\n",
            "Epoch: 1, Training Loss: 3.3889319896698\n",
            "Epoch: 1, Training Loss: 3.331968307495117\n",
            "Epoch: 1, Training Loss: 3.2640039920806885\n",
            "Epoch: 1, Training Loss: 3.2719509601593018\n",
            "Epoch: 1, Training Loss: 3.3651649951934814\n",
            "Epoch: 1, Training Loss: 3.2991693019866943\n",
            "Epoch: 1, Training Loss: 3.2835705280303955\n",
            "Epoch: 1, Training Loss: 3.0916240215301514\n",
            "Epoch: 1, Training Loss: 3.19126033782959\n",
            "Epoch: 1, Training Loss: 3.1476588249206543\n",
            "Epoch: 1, Training Loss: 3.3038225173950195\n",
            "Epoch: 1, Training Loss: 3.3006134033203125\n",
            "Epoch: 1, Training Loss: 3.153290033340454\n",
            "Epoch: 1, Training Loss: 3.1828396320343018\n",
            "Epoch: 1, Training Loss: 3.131671905517578\n",
            "Epoch: 1, Training Loss: 3.194425106048584\n",
            "Epoch: 1, Training Loss: 3.2547061443328857\n",
            "Epoch: 1, Training Loss: 3.372460126876831\n",
            "Epoch: 1, Training Loss: 3.205246686935425\n",
            "Epoch: 1, Training Loss: 2.956984519958496\n",
            "Epoch: 1, Training Loss: 3.098665714263916\n",
            "Epoch: 1, Training Loss: 3.274728298187256\n",
            "Epoch: 1, Training Loss: 3.164210319519043\n",
            "Epoch: 1, Training Loss: 3.145437479019165\n",
            "Epoch: 1, Training Loss: 3.427476406097412\n",
            "Epoch: 1, Training Loss: 3.38173770904541\n",
            "Epoch: 1, Training Loss: 3.1379809379577637\n",
            "Epoch: 1, Training Loss: 3.31134033203125\n",
            "Epoch: 1, Training Loss: 3.372800350189209\n",
            "Epoch: 1, Training Loss: 3.228990077972412\n",
            "Epoch: 1, Training Loss: 3.2355611324310303\n",
            "Epoch: 1, Training Loss: 3.063469886779785\n",
            "Epoch: 1, Training Loss: 3.409014940261841\n",
            "Epoch: 1, Training Loss: 3.1037251949310303\n",
            "Epoch: 1, Training Loss: 3.178063154220581\n",
            "Epoch: 1, Training Loss: 3.153365135192871\n",
            "Epoch: 1, Training Loss: 3.23349928855896\n",
            "Epoch: 1, Training Loss: 2.8578341007232666\n",
            "Epoch: 1, Training Loss: 3.126317262649536\n",
            "Epoch: 1, Training Loss: 3.27766489982605\n",
            "Epoch: 1, Validation Loss: 3.053093194961548\n",
            "Epoch: 1, Validation Loss: 3.0541508197784424\n",
            "Epoch: 1, Validation Loss: 3.0333924293518066\n",
            "Epoch: 1, Validation Loss: 2.6278393268585205\n",
            "Epoch: 1, Validation Loss: 2.9860751628875732\n",
            "Epoch: 1, Validation Loss: 3.0293455123901367\n",
            "Epoch: 1, Validation Loss: 2.9329535961151123\n",
            "Epoch: 1, Validation Loss: 3.1759862899780273\n",
            "Epoch: 1, Validation Loss: 3.065173625946045\n",
            "Epoch: 1, Validation Loss: 3.116919994354248\n",
            "Epoch: 1, Validation Loss: 2.904330015182495\n",
            "Epoch: 1, Validation Loss: 3.304440975189209\n",
            "Epoch: 1, Validation Loss: 2.9345195293426514\n",
            "Epoch: 1, Validation Loss: 2.985579013824463\n",
            "Epoch: 1, Validation Loss: 3.0593864917755127\n",
            "Epoch: 1, Validation Loss: 2.9941229820251465\n",
            "Epoch: 1, Validation Loss: 2.9228711128234863\n",
            "Epoch: 1, Validation Loss: 2.9464054107666016\n",
            "Epoch: 1, Validation Loss: 3.1841771602630615\n",
            "Epoch: 1, Validation Loss: 3.2844760417938232\n",
            "Epoch: 1, Validation Loss: 3.0276997089385986\n",
            "Epoch: 1, Validation Loss: 2.803974151611328\n",
            "Epoch: 1, Validation Loss: 3.195729970932007\n",
            "Epoch: 1, Validation Loss: 3.262964963912964\n",
            "Epoch: 1, Validation Loss: 2.7460427284240723\n",
            "Epoch: 1, Validation Loss: 2.955343246459961\n",
            "Epoch: 1, Validation Loss: 3.055342197418213\n",
            "Epoch: 1, Validation Loss: 3.12121844291687\n",
            "Epoch: 1, Validation Loss: 3.13236403465271\n",
            "Epoch: 1, Validation Loss: 3.0633487701416016\n",
            "Epoch: 1, Validation Loss: 3.2242043018341064\n",
            "Epoch: 1, Validation Loss: 3.0916717052459717\n",
            "Epoch: 1, Validation Loss: 2.9868366718292236\n",
            "Epoch: 1, Validation Loss: 2.979681968688965\n",
            "Epoch: 1, Validation Loss: 3.1486880779266357\n",
            "Epoch: 1, Validation Loss: 2.8598225116729736\n",
            "Epoch: 1, Validation Loss: 3.03083872795105\n",
            "Epoch: 1, Validation Loss: 3.0547876358032227\n",
            "Epoch: 1, Validation Loss: 2.888356924057007\n",
            "Epoch: 1, Validation Loss: 3.1279137134552\n",
            "Epoch: 1, Validation Loss: 3.108795642852783\n",
            "Epoch: 1, Validation Loss: 3.099740982055664\n",
            "Epoch: 1, Validation Loss: 3.187150001525879\n",
            "Epoch: 1, Validation Loss: 3.282989978790283\n",
            "Epoch: 1, Validation Loss: 2.994755506515503\n",
            "Epoch: 1, Validation Loss: 2.973086357116699\n",
            "Epoch: 1, Validation Loss: 2.95343279838562\n",
            "Epoch: 1, Validation Loss: 3.123889923095703\n",
            "Epoch: 1, Validation Loss: 3.080458164215088\n",
            "Epoch: 1, Validation Loss: 2.9774279594421387\n",
            "Epoch: 1, Validation Loss: 2.947960138320923\n",
            "Epoch: 1, Validation Loss: 2.8157522678375244\n",
            "Epoch: 1, Validation Loss: 3.279670476913452\n",
            "Epoch: 1, Validation Loss: 3.3349523544311523\n",
            "Epoch: 1, Validation Loss: 2.9185755252838135\n",
            "Epoch: 1, Validation Loss: 3.074387311935425\n",
            "Epoch: 1, Validation Loss: 3.203967332839966\n",
            "Epoch: 1, Validation Loss: 3.3227484226226807\n",
            "Epoch: 1, Validation Loss: 3.179403066635132\n",
            "Epoch: 1, Validation Loss: 2.947033166885376\n",
            "Epoch: 1, Validation Loss: 3.1635935306549072\n",
            "Epoch: 1, Validation Loss: 3.0208001136779785\n",
            "Epoch: 1, Validation Loss: 3.233457565307617\n",
            "Epoch: 1, Validation Loss: 2.941566228866577\n",
            "Epoch: 1, Validation Loss: 2.8210508823394775\n",
            "Epoch: 1, Validation Loss: 2.9375112056732178\n",
            "Epoch: 1, Validation Loss: 3.243663787841797\n",
            "Epoch: 1, Validation Loss: 3.1692049503326416\n",
            "Epoch: 1, Validation Loss: 3.1558146476745605\n",
            "Epoch: 1, Validation Loss: 3.097034454345703\n",
            "Epoch: 1, Validation Loss: 3.0770699977874756\n",
            "Epoch: 1, Validation Loss: 2.9468133449554443\n",
            "Epoch: 1, Validation Loss: 3.257134437561035\n",
            "Epoch: 1, Validation Loss: 3.254525661468506\n",
            "Epoch: 1, Validation Loss: 2.9764740467071533\n",
            "Epoch: 1, Validation Loss: 3.02588152885437\n",
            "Epoch: 1, Validation Loss: 3.0032591819763184\n",
            "Epoch: 1, Validation Loss: 2.7665340900421143\n",
            "Epoch: 1, Validation Loss: 3.309025287628174\n",
            "Epoch: 1, Validation Loss: 2.808537244796753\n",
            "Epoch: 1, Validation Loss: 3.2277088165283203\n",
            "Epoch: 1, Validation Loss: 3.066740036010742\n",
            "Epoch: 1, Validation Loss: 3.000638961791992\n",
            "Epoch: 1, Validation Loss: 3.099919080734253\n",
            "Epoch: 1, Validation Loss: 2.942648410797119\n",
            "Epoch: 1, Validation Loss: 2.947747230529785\n",
            "Epoch: 1, Validation Loss: 3.3783254623413086\n",
            "Epoch: 1, Validation Loss: 2.987211227416992\n",
            "Epoch: 1, Validation Loss: 3.117312431335449\n",
            "Epoch: 1, Validation Loss: 2.994706869125366\n",
            "Epoch: 1, Validation Loss: 3.0188674926757812\n",
            "Epoch: 1, Validation Loss: 3.106837034225464\n",
            "Epoch: 1, Validation Loss: 3.106762170791626\n",
            "Epoch: 1, Validation Loss: 3.3242671489715576\n",
            "Epoch: 1, Validation Loss: 3.314161539077759\n",
            "Epoch: 1, Validation Loss: 2.976076364517212\n",
            "Epoch: 1, Validation Loss: 3.008920907974243\n",
            "Epoch: 1, Validation Loss: 3.132517099380493\n",
            "Epoch: 1, Validation Loss: 3.1861274242401123\n",
            "Epoch: 1, Validation Loss: 3.0857579708099365\n",
            "Epoch: 1, Validation Loss: 3.3141238689422607\n",
            "Epoch: 1, Validation Loss: 3.0701396465301514\n",
            "Epoch: 1, Validation Loss: 3.0949180126190186\n",
            "Epoch: 1, Validation Loss: 3.029137134552002\n",
            "Epoch: 1, Validation Loss: 2.984276533126831\n",
            "Epoch: 1, Validation Loss: 2.8972978591918945\n",
            "Epoch: 1, Validation Loss: 3.2418885231018066\n",
            "Epoch: 1, Validation Loss: 3.1600210666656494\n",
            "Epoch: 1, Validation Loss: 2.935511350631714\n",
            "Epoch: 1, Validation Loss: 3.250897169113159\n",
            "Epoch: 1, Validation Loss: 2.9214320182800293\n",
            "Epoch: 1, Validation Loss: 3.1873762607574463\n",
            "Epoch: 1, Validation Loss: 3.1743550300598145\n",
            "Epoch: 1, Validation Loss: 3.1049251556396484\n",
            "Epoch: 1, Validation Loss: 3.0461623668670654\n",
            "Epoch: 1, Validation Loss: 2.945993423461914\n",
            "Epoch: 1, Validation Loss: 2.746300458908081\n",
            "Epoch: 1, Validation Loss: 2.9639225006103516\n",
            "Epoch: 1, Validation Loss: 2.9080560207366943\n",
            "Epoch: 1, Validation Loss: 3.0787193775177\n",
            "Epoch: 1, Validation Loss: 3.163970470428467\n",
            "Epoch: 1, Validation Loss: 3.1402440071105957\n",
            "Epoch: 1, Validation Loss: 3.1797304153442383\n",
            "Epoch: 1, Validation Loss: 2.9083409309387207\n",
            "Epoch: 1, Validation Loss: 3.2493436336517334\n",
            "Epoch: 1, Validation Loss: 3.0793776512145996\n",
            "Epoch: 1, Validation Loss: 3.028730630874634\n",
            "Epoch: 1, Validation Loss: 2.7579500675201416\n",
            "Epoch: 1, Validation Loss: 2.9964683055877686\n",
            "Epoch: 1, Validation Loss: 3.1173160076141357\n",
            "Epoch: 1, Validation Loss: 3.2452895641326904\n",
            "Epoch: 1, Validation Loss: 3.2540085315704346\n",
            "Epoch: 1, Validation Loss: 2.872826337814331\n",
            "Epoch: 1, Validation Loss: 3.166835069656372\n",
            "Epoch: 1, Validation Loss: 3.060553789138794\n",
            "Epoch: 1, Validation Loss: 3.0887646675109863\n",
            "Epoch: 1, Validation Loss: 2.947122573852539\n",
            "Epoch: 1, Validation Loss: 3.0821566581726074\n",
            "Epoch: 1, Validation Loss: 2.9276413917541504\n",
            "Epoch: 1, Validation Loss: 3.1110613346099854\n",
            "Epoch: 1, Validation Loss: 3.050617218017578\n",
            "Epoch: 1, Validation Loss: 3.1711525917053223\n",
            "Epoch: 1, Validation Loss: 2.8959500789642334\n",
            "Epoch: 1, Validation Loss: 3.028933048248291\n",
            "Epoch: 1, Validation Loss: 2.8988969326019287\n",
            "Epoch: 1, Validation Loss: 2.8739137649536133\n",
            "Epoch: 1, Validation Loss: 3.036011219024658\n",
            "Epoch: 1, Validation Loss: 2.915085792541504\n",
            "Epoch: 1, Validation Loss: 2.9219064712524414\n",
            "Epoch: 1, Validation Loss: 2.979511260986328\n",
            "Epoch: 1, Validation Loss: 3.232072353363037\n",
            "Epoch: 1, Validation Loss: 3.050718069076538\n",
            "Epoch: 1, Validation Loss: 3.0533597469329834\n",
            "Epoch: 1, Validation Loss: 3.426867723464966\n",
            "Epoch: 1, Validation Loss: 3.0311975479125977\n",
            "Epoch: 1, Validation Loss: 3.0305356979370117\n",
            "Epoch: 1, Validation Loss: 2.973906993865967\n",
            "Epoch: 1, Validation Loss: 3.1473212242126465\n",
            "Epoch: 1, Validation Loss: 2.7916464805603027\n",
            "Epoch: 1, Validation Loss: 3.163604259490967\n",
            "Epoch: 1, Validation Loss: 2.8202638626098633\n",
            "Epoch: 1, Validation Loss: 2.9602408409118652\n",
            "Epoch: 1, Validation Loss: 3.002251386642456\n",
            "Epoch: 1, Validation Loss: 3.105741500854492\n",
            "Epoch: 1, Validation Loss: 3.122979164123535\n",
            "Epoch: 1, Validation Loss: 3.0292704105377197\n",
            "Epoch: 1, Validation Loss: 3.3022561073303223\n",
            "Epoch: 1, Validation Loss: 3.303955554962158\n",
            "Epoch: 1, Validation Loss: 2.926560878753662\n",
            "Epoch: 1, Validation Loss: 3.2962586879730225\n",
            "Epoch: 1, Validation Loss: 3.0206241607666016\n",
            "Epoch: 1, Validation Loss: 2.8774783611297607\n",
            "Epoch: 1, Validation Loss: 3.016282558441162\n",
            "Epoch: 1, Validation Loss: 2.9792981147766113\n",
            "Epoch: 1, Validation Loss: 2.9332351684570312\n",
            "Epoch: 1, Validation Loss: 3.1422386169433594\n",
            "Epoch: 1, Validation Loss: 2.8761868476867676\n",
            "Epoch: 1, Validation Loss: 3.049959421157837\n",
            "Epoch: 1, Validation Loss: 3.109029531478882\n",
            "Epoch: 1, Validation Loss: 3.2547109127044678\n",
            "Epoch: 1, Validation Loss: 3.0793001651763916\n",
            "Epoch: 1, Validation Loss: 3.036996841430664\n",
            "Epoch: 1, Validation Loss: 2.997758388519287\n",
            "Epoch: 1, Validation Loss: 3.1308770179748535\n",
            "Epoch: 1, Validation Loss: 3.4765517711639404\n",
            "Epoch: 1, Validation Loss: 3.0735950469970703\n",
            "Epoch: 1, Validation Loss: 3.180258274078369\n",
            "Epoch: 1, Validation Loss: 3.1620917320251465\n",
            "Epoch: 1, Validation Loss: 3.0318589210510254\n",
            "Epoch: 1, Validation Loss: 2.9115078449249268\n",
            "Epoch: 1, Validation Loss: 3.2051846981048584\n",
            "Epoch: 1, Validation Loss: 3.010607957839966\n",
            "Epoch: 1, Validation Loss: 2.8672657012939453\n",
            "Epoch: 1, Validation Loss: 3.0725133419036865\n",
            "Epoch: 1, Validation Loss: 3.125828742980957\n",
            "Epoch: 1, Validation Loss: 2.9929659366607666\n",
            "Epoch: 1, Validation Loss: 3.0579028129577637\n",
            "Epoch: 1, Validation Loss: 3.0618176460266113\n",
            "Epoch: 1, Validation Loss: 3.2496137619018555\n",
            "Epoch: 1, Validation Loss: 3.0586822032928467\n",
            "Epoch: 1, Validation Loss: 3.0816361904144287\n",
            "Epoch: 1, Validation Loss: 3.3204660415649414\n",
            "Epoch: 1, Validation Loss: 2.9965994358062744\n",
            "Epoch: 1, Validation Loss: 2.989445924758911\n",
            "Epoch: 1, Validation Loss: 3.045508861541748\n",
            "Epoch: 1, Validation Loss: 2.9894752502441406\n",
            "Epoch: 1, Validation Loss: 3.0373172760009766\n",
            "Epoch: 1, Validation Loss: 3.0809452533721924\n",
            "Epoch: 1, Validation Loss: 2.911226987838745\n",
            "Epoch: 1, Validation Loss: 2.9018502235412598\n",
            "Epoch: 1, Validation Loss: 3.0937552452087402\n",
            "Epoch: 1, Validation Loss: 3.0788865089416504\n",
            "Epoch: 1, Validation Loss: 3.0691978931427\n",
            "Epoch: 1, Validation Loss: 3.1223528385162354\n",
            "Epoch: 1, Validation Loss: 3.2233428955078125\n",
            "Epoch: 1, Validation Loss: 3.2004504203796387\n",
            "Epoch: 1, Validation Loss: 3.1394143104553223\n",
            "Epoch: 1, Validation Loss: 2.881354331970215\n",
            "Epoch: 1, Validation Loss: 3.0217702388763428\n",
            "Epoch: 1, Validation Loss: 2.9640917778015137\n",
            "Epoch: 1, Validation Loss: 3.123969793319702\n",
            "Epoch: 1, Validation Loss: 2.9618399143218994\n",
            "Epoch: 1, Validation Loss: 2.9992237091064453\n",
            "Epoch: 1, Validation Loss: 3.0141849517822266\n",
            "Epoch: 2\n",
            "------------------------------\n",
            "Epoch: 2, Training Loss: 3.0991692543029785\n",
            "Epoch: 2, Training Loss: 3.279474973678589\n",
            "Epoch: 2, Training Loss: 3.294243097305298\n",
            "Epoch: 2, Training Loss: 3.142552614212036\n",
            "Epoch: 2, Training Loss: 3.1812965869903564\n",
            "Epoch: 2, Training Loss: 2.920567512512207\n",
            "Epoch: 2, Training Loss: 3.461186170578003\n",
            "Epoch: 2, Training Loss: 3.292910099029541\n",
            "Epoch: 2, Training Loss: 3.3566970825195312\n",
            "Epoch: 2, Training Loss: 3.218316078186035\n",
            "Epoch: 2, Training Loss: 3.2180631160736084\n",
            "Epoch: 2, Training Loss: 3.0069639682769775\n",
            "Epoch: 2, Training Loss: 2.9206507205963135\n",
            "Epoch: 2, Training Loss: 3.234839916229248\n",
            "Epoch: 2, Training Loss: 2.8821163177490234\n",
            "Epoch: 2, Training Loss: 3.3723535537719727\n",
            "Epoch: 2, Training Loss: 3.151113510131836\n",
            "Epoch: 2, Training Loss: 3.2065064907073975\n",
            "Epoch: 2, Training Loss: 3.1945810317993164\n",
            "Epoch: 2, Training Loss: 3.225743293762207\n",
            "Epoch: 2, Training Loss: 3.0570738315582275\n",
            "Epoch: 2, Training Loss: 3.3563437461853027\n",
            "Epoch: 2, Training Loss: 3.2270638942718506\n",
            "Epoch: 2, Training Loss: 3.2071757316589355\n",
            "Epoch: 2, Training Loss: 3.2630715370178223\n",
            "Epoch: 2, Training Loss: 3.3507895469665527\n",
            "Epoch: 2, Training Loss: 3.2620110511779785\n",
            "Epoch: 2, Training Loss: 3.221052646636963\n",
            "Epoch: 2, Training Loss: 2.8852217197418213\n",
            "Epoch: 2, Training Loss: 3.0500566959381104\n",
            "Epoch: 2, Training Loss: 3.109729051589966\n",
            "Epoch: 2, Training Loss: 3.0818090438842773\n",
            "Epoch: 2, Training Loss: 3.233051061630249\n",
            "Epoch: 2, Training Loss: 3.2059977054595947\n",
            "Epoch: 2, Training Loss: 3.0178332328796387\n",
            "Epoch: 2, Training Loss: 3.2393999099731445\n",
            "Epoch: 2, Training Loss: 3.170457601547241\n",
            "Epoch: 2, Training Loss: 3.1077237129211426\n",
            "Epoch: 2, Training Loss: 3.2301552295684814\n",
            "Epoch: 2, Training Loss: 3.1080777645111084\n",
            "Epoch: 2, Training Loss: 3.223644733428955\n",
            "Epoch: 2, Training Loss: 3.331566333770752\n",
            "Epoch: 2, Training Loss: 3.100236177444458\n",
            "Epoch: 2, Training Loss: 3.33516001701355\n",
            "Epoch: 2, Training Loss: 3.0636980533599854\n",
            "Epoch: 2, Training Loss: 3.0095088481903076\n",
            "Epoch: 2, Training Loss: 3.3717801570892334\n",
            "Epoch: 2, Training Loss: 3.1011574268341064\n",
            "Epoch: 2, Training Loss: 3.32085919380188\n",
            "Epoch: 2, Training Loss: 3.0782876014709473\n",
            "Epoch: 2, Training Loss: 3.3413846492767334\n",
            "Epoch: 2, Training Loss: 3.0593254566192627\n",
            "Epoch: 2, Training Loss: 3.0044145584106445\n",
            "Epoch: 2, Training Loss: 3.405817985534668\n",
            "Epoch: 2, Training Loss: 3.1924943923950195\n",
            "Epoch: 2, Training Loss: 3.7181754112243652\n",
            "Epoch: 2, Training Loss: 3.476470708847046\n",
            "Epoch: 2, Training Loss: 2.903386116027832\n",
            "Epoch: 2, Training Loss: 3.041229724884033\n",
            "Epoch: 2, Training Loss: 3.009823799133301\n",
            "Epoch: 2, Training Loss: 3.065701961517334\n",
            "Epoch: 2, Training Loss: 2.9993228912353516\n",
            "Epoch: 2, Training Loss: 3.059802293777466\n",
            "Epoch: 2, Training Loss: 3.1590511798858643\n",
            "Epoch: 2, Training Loss: 3.187406063079834\n",
            "Epoch: 2, Training Loss: 2.958421230316162\n",
            "Epoch: 2, Training Loss: 3.2423880100250244\n",
            "Epoch: 2, Training Loss: 3.0485622882843018\n",
            "Epoch: 2, Training Loss: 3.147467851638794\n",
            "Epoch: 2, Training Loss: 3.2261548042297363\n",
            "Epoch: 2, Training Loss: 3.491184711456299\n",
            "Epoch: 2, Training Loss: 2.8774170875549316\n",
            "Epoch: 2, Training Loss: 3.1061854362487793\n",
            "Epoch: 2, Training Loss: 3.1132171154022217\n",
            "Epoch: 2, Training Loss: 3.1345536708831787\n",
            "Epoch: 2, Training Loss: 2.9533727169036865\n",
            "Epoch: 2, Training Loss: 2.961031675338745\n",
            "Epoch: 2, Training Loss: 3.268073320388794\n",
            "Epoch: 2, Training Loss: 3.3278205394744873\n",
            "Epoch: 2, Training Loss: 3.1904561519622803\n",
            "Epoch: 2, Training Loss: 3.15071964263916\n",
            "Epoch: 2, Training Loss: 3.1207196712493896\n",
            "Epoch: 2, Training Loss: 3.1108627319335938\n",
            "Epoch: 2, Training Loss: 3.1895110607147217\n",
            "Epoch: 2, Training Loss: 3.008793830871582\n",
            "Epoch: 2, Training Loss: 3.0512473583221436\n",
            "Epoch: 2, Training Loss: 3.2484421730041504\n",
            "Epoch: 2, Training Loss: 3.1897284984588623\n",
            "Epoch: 2, Training Loss: 3.2307231426239014\n",
            "Epoch: 2, Training Loss: 3.075711965560913\n",
            "Epoch: 2, Training Loss: 3.1422224044799805\n",
            "Epoch: 2, Training Loss: 3.260082483291626\n",
            "Epoch: 2, Training Loss: 3.2010653018951416\n",
            "Epoch: 2, Training Loss: 3.080289125442505\n",
            "Epoch: 2, Training Loss: 2.945317506790161\n",
            "Epoch: 2, Training Loss: 3.05100417137146\n",
            "Epoch: 2, Training Loss: 3.37552809715271\n",
            "Epoch: 2, Training Loss: 3.265939712524414\n",
            "Epoch: 2, Training Loss: 3.0012404918670654\n",
            "Epoch: 2, Training Loss: 3.0225234031677246\n",
            "Epoch: 2, Training Loss: 3.3304128646850586\n",
            "Epoch: 2, Training Loss: 3.1487159729003906\n",
            "Epoch: 2, Training Loss: 2.8723292350769043\n",
            "Epoch: 2, Training Loss: 3.0113344192504883\n",
            "Epoch: 2, Training Loss: 3.1002726554870605\n",
            "Epoch: 2, Training Loss: 2.9524364471435547\n",
            "Epoch: 2, Training Loss: 3.085432291030884\n",
            "Epoch: 2, Training Loss: 3.00689959526062\n",
            "Epoch: 2, Training Loss: 3.013984441757202\n",
            "Epoch: 2, Training Loss: 2.968831777572632\n",
            "Epoch: 2, Training Loss: 3.0163350105285645\n",
            "Epoch: 2, Training Loss: 2.8580589294433594\n",
            "Epoch: 2, Training Loss: 3.059659481048584\n",
            "Epoch: 2, Training Loss: 3.160121440887451\n",
            "Epoch: 2, Training Loss: 3.274949312210083\n",
            "Epoch: 2, Training Loss: 3.145059823989868\n",
            "Epoch: 2, Training Loss: 2.8834750652313232\n",
            "Epoch: 2, Training Loss: 3.2247366905212402\n",
            "Epoch: 2, Training Loss: 2.9778709411621094\n",
            "Epoch: 2, Training Loss: 3.1336116790771484\n",
            "Epoch: 2, Training Loss: 3.2573037147521973\n",
            "Epoch: 2, Training Loss: 3.010584831237793\n",
            "Epoch: 2, Training Loss: 3.1846911907196045\n",
            "Epoch: 2, Training Loss: 3.1889381408691406\n",
            "Epoch: 2, Training Loss: 3.456205368041992\n",
            "Epoch: 2, Training Loss: 3.204024076461792\n",
            "Epoch: 2, Training Loss: 2.9753236770629883\n",
            "Epoch: 2, Training Loss: 2.854027032852173\n",
            "Epoch: 2, Training Loss: 2.9994184970855713\n",
            "Epoch: 2, Training Loss: 3.2940292358398438\n",
            "Epoch: 2, Training Loss: 3.0162417888641357\n",
            "Epoch: 2, Training Loss: 3.2035980224609375\n",
            "Epoch: 2, Training Loss: 3.270134449005127\n",
            "Epoch: 2, Training Loss: 3.432032346725464\n",
            "Epoch: 2, Training Loss: 3.2742276191711426\n",
            "Epoch: 2, Training Loss: 3.1808712482452393\n",
            "Epoch: 2, Training Loss: 3.255213737487793\n",
            "Epoch: 2, Training Loss: 3.2269484996795654\n",
            "Epoch: 2, Training Loss: 2.8969833850860596\n",
            "Epoch: 2, Training Loss: 3.2988271713256836\n",
            "Epoch: 2, Training Loss: 3.0377566814422607\n",
            "Epoch: 2, Training Loss: 2.953951597213745\n",
            "Epoch: 2, Training Loss: 2.867779493331909\n",
            "Epoch: 2, Training Loss: 3.0903196334838867\n",
            "Epoch: 2, Training Loss: 3.0503499507904053\n",
            "Epoch: 2, Training Loss: 2.9897658824920654\n",
            "Epoch: 2, Training Loss: 3.073270559310913\n",
            "Epoch: 2, Training Loss: 3.3899729251861572\n",
            "Epoch: 2, Training Loss: 3.0903496742248535\n",
            "Epoch: 2, Training Loss: 3.2212612628936768\n",
            "Epoch: 2, Training Loss: 3.210430860519409\n",
            "Epoch: 2, Training Loss: 2.971555709838867\n",
            "Epoch: 2, Training Loss: 3.1117570400238037\n",
            "Epoch: 2, Training Loss: 2.904644250869751\n",
            "Epoch: 2, Training Loss: 3.1672534942626953\n",
            "Epoch: 2, Training Loss: 3.25950026512146\n",
            "Epoch: 2, Training Loss: 3.032229423522949\n",
            "Epoch: 2, Training Loss: 2.8636059761047363\n",
            "Epoch: 2, Training Loss: 3.0217397212982178\n",
            "Epoch: 2, Training Loss: 3.0782992839813232\n",
            "Epoch: 2, Training Loss: 3.042724132537842\n",
            "Epoch: 2, Training Loss: 3.3444931507110596\n",
            "Epoch: 2, Training Loss: 2.8096117973327637\n",
            "Epoch: 2, Training Loss: 3.110123872756958\n",
            "Epoch: 2, Training Loss: 3.1299331188201904\n",
            "Epoch: 2, Training Loss: 3.14544677734375\n",
            "Epoch: 2, Training Loss: 3.2234766483306885\n",
            "Epoch: 2, Training Loss: 3.1240506172180176\n",
            "Epoch: 2, Training Loss: 3.1560218334198\n",
            "Epoch: 2, Training Loss: 3.279669761657715\n",
            "Epoch: 2, Training Loss: 3.050563097000122\n",
            "Epoch: 2, Training Loss: 2.8731002807617188\n",
            "Epoch: 2, Training Loss: 3.218499183654785\n",
            "Epoch: 2, Training Loss: 2.9208247661590576\n",
            "Epoch: 2, Training Loss: 3.087161064147949\n",
            "Epoch: 2, Training Loss: 3.27445387840271\n",
            "Epoch: 2, Training Loss: 3.0824944972991943\n",
            "Epoch: 2, Training Loss: 3.280164957046509\n",
            "Epoch: 2, Training Loss: 3.3460657596588135\n",
            "Epoch: 2, Training Loss: 3.022655487060547\n",
            "Epoch: 2, Training Loss: 3.1103076934814453\n",
            "Epoch: 2, Training Loss: 3.496962308883667\n",
            "Epoch: 2, Training Loss: 3.007612466812134\n",
            "Epoch: 2, Training Loss: 3.2058889865875244\n",
            "Epoch: 2, Training Loss: 2.8976595401763916\n",
            "Epoch: 2, Training Loss: 2.9854767322540283\n",
            "Epoch: 2, Training Loss: 2.9728891849517822\n",
            "Epoch: 2, Training Loss: 2.7030532360076904\n",
            "Epoch: 2, Training Loss: 3.1443629264831543\n",
            "Epoch: 2, Training Loss: 3.146362781524658\n",
            "Epoch: 2, Training Loss: 3.336646318435669\n",
            "Epoch: 2, Training Loss: 3.362236976623535\n",
            "Epoch: 2, Training Loss: 3.336503744125366\n",
            "Epoch: 2, Training Loss: 3.2431671619415283\n",
            "Epoch: 2, Training Loss: 3.082655668258667\n",
            "Epoch: 2, Training Loss: 3.1286275386810303\n",
            "Epoch: 2, Training Loss: 3.0690674781799316\n",
            "Epoch: 2, Training Loss: 2.9678635597229004\n",
            "Epoch: 2, Training Loss: 3.2359213829040527\n",
            "Epoch: 2, Training Loss: 3.1549975872039795\n",
            "Epoch: 2, Training Loss: 3.429699659347534\n",
            "Epoch: 2, Training Loss: 3.2652435302734375\n",
            "Epoch: 2, Training Loss: 3.195204019546509\n",
            "Epoch: 2, Training Loss: 2.9320621490478516\n",
            "Epoch: 2, Training Loss: 2.931877851486206\n",
            "Epoch: 2, Training Loss: 3.123699188232422\n",
            "Epoch: 2, Training Loss: 3.123002290725708\n",
            "Epoch: 2, Training Loss: 2.897242546081543\n",
            "Epoch: 2, Training Loss: 3.076716661453247\n",
            "Epoch: 2, Training Loss: 3.182936191558838\n",
            "Epoch: 2, Training Loss: 3.160722017288208\n",
            "Epoch: 2, Training Loss: 3.1538591384887695\n",
            "Epoch: 2, Training Loss: 2.9624454975128174\n",
            "Epoch: 2, Training Loss: 3.313948631286621\n",
            "Epoch: 2, Training Loss: 2.9324307441711426\n",
            "Epoch: 2, Training Loss: 2.9854815006256104\n",
            "Epoch: 2, Training Loss: 3.0402088165283203\n",
            "Epoch: 2, Training Loss: 3.0713276863098145\n",
            "Epoch: 2, Training Loss: 2.9436352252960205\n",
            "Epoch: 2, Training Loss: 2.8371734619140625\n",
            "Epoch: 2, Training Loss: 3.126904010772705\n",
            "Epoch: 2, Training Loss: 3.202981948852539\n",
            "Epoch: 2, Training Loss: 3.3708691596984863\n",
            "Epoch: 2, Training Loss: 3.233445882797241\n",
            "Epoch: 2, Training Loss: 3.015709638595581\n",
            "Epoch: 2, Training Loss: 3.265333414077759\n",
            "Epoch: 2, Training Loss: 3.2408769130706787\n",
            "Epoch: 2, Training Loss: 3.237051010131836\n",
            "Epoch: 2, Training Loss: 2.8242077827453613\n",
            "Epoch: 2, Training Loss: 3.0887277126312256\n",
            "Epoch: 2, Training Loss: 3.2441444396972656\n",
            "Epoch: 2, Training Loss: 3.045804977416992\n",
            "Epoch: 2, Training Loss: 3.1293554306030273\n",
            "Epoch: 2, Training Loss: 3.1378846168518066\n",
            "Epoch: 2, Training Loss: 3.0426437854766846\n",
            "Epoch: 2, Training Loss: 2.8915722370147705\n",
            "Epoch: 2, Training Loss: 3.053865909576416\n",
            "Epoch: 2, Training Loss: 2.773946523666382\n",
            "Epoch: 2, Training Loss: 3.2332980632781982\n",
            "Epoch: 2, Training Loss: 3.2044997215270996\n",
            "Epoch: 2, Training Loss: 3.3045337200164795\n",
            "Epoch: 2, Training Loss: 3.105116844177246\n",
            "Epoch: 2, Training Loss: 3.029426097869873\n",
            "Epoch: 2, Training Loss: 3.0737857818603516\n",
            "Epoch: 2, Training Loss: 3.1679909229278564\n",
            "Epoch: 2, Training Loss: 3.274881362915039\n",
            "Epoch: 2, Training Loss: 2.9872820377349854\n",
            "Epoch: 2, Training Loss: 3.087528944015503\n",
            "Epoch: 2, Training Loss: 3.2411632537841797\n",
            "Epoch: 2, Training Loss: 3.1769134998321533\n",
            "Epoch: 2, Training Loss: 3.4290523529052734\n",
            "Epoch: 2, Training Loss: 3.230149269104004\n",
            "Epoch: 2, Training Loss: 3.070727586746216\n",
            "Epoch: 2, Training Loss: 3.0662577152252197\n",
            "Epoch: 2, Training Loss: 3.071265697479248\n",
            "Epoch: 2, Training Loss: 2.9971506595611572\n",
            "Epoch: 2, Training Loss: 3.155478000640869\n",
            "Epoch: 2, Training Loss: 3.0787508487701416\n",
            "Epoch: 2, Training Loss: 3.272918701171875\n",
            "Epoch: 2, Training Loss: 3.263451337814331\n",
            "Epoch: 2, Training Loss: 3.191387891769409\n",
            "Epoch: 2, Training Loss: 3.0718204975128174\n",
            "Epoch: 2, Training Loss: 3.2456650733947754\n",
            "Epoch: 2, Training Loss: 2.937415599822998\n",
            "Epoch: 2, Training Loss: 3.2926175594329834\n",
            "Epoch: 2, Training Loss: 2.8609700202941895\n",
            "Epoch: 2, Training Loss: 3.216979503631592\n",
            "Epoch: 2, Training Loss: 3.1264264583587646\n",
            "Epoch: 2, Training Loss: 3.0364322662353516\n",
            "Epoch: 2, Training Loss: 2.7995033264160156\n",
            "Epoch: 2, Training Loss: 2.848524570465088\n",
            "Epoch: 2, Training Loss: 3.0009918212890625\n",
            "Epoch: 2, Training Loss: 2.9717044830322266\n",
            "Epoch: 2, Training Loss: 2.990253210067749\n",
            "Epoch: 2, Training Loss: 3.16517972946167\n",
            "Epoch: 2, Training Loss: 3.0226802825927734\n",
            "Epoch: 2, Training Loss: 3.159489393234253\n",
            "Epoch: 2, Training Loss: 3.1692683696746826\n",
            "Epoch: 2, Training Loss: 3.1054890155792236\n",
            "Epoch: 2, Training Loss: 3.0891990661621094\n",
            "Epoch: 2, Training Loss: 2.920551061630249\n",
            "Epoch: 2, Training Loss: 2.8442811965942383\n",
            "Epoch: 2, Training Loss: 3.0319113731384277\n",
            "Epoch: 2, Training Loss: 2.9712271690368652\n",
            "Epoch: 2, Training Loss: 3.201993942260742\n",
            "Epoch: 2, Training Loss: 2.9265050888061523\n",
            "Epoch: 2, Training Loss: 2.960012197494507\n",
            "Epoch: 2, Training Loss: 3.1345107555389404\n",
            "Epoch: 2, Training Loss: 2.8996243476867676\n",
            "Epoch: 2, Training Loss: 3.2403478622436523\n",
            "Epoch: 2, Training Loss: 3.1969213485717773\n",
            "Epoch: 2, Training Loss: 3.202751874923706\n",
            "Epoch: 2, Training Loss: 3.0651698112487793\n",
            "Epoch: 2, Training Loss: 2.7946481704711914\n",
            "Epoch: 2, Training Loss: 3.1709108352661133\n",
            "Epoch: 2, Training Loss: 3.143218994140625\n",
            "Epoch: 2, Training Loss: 2.8904507160186768\n",
            "Epoch: 2, Training Loss: 3.075526475906372\n",
            "Epoch: 2, Training Loss: 3.4517059326171875\n",
            "Epoch: 2, Training Loss: 3.1004343032836914\n",
            "Epoch: 2, Training Loss: 3.1579554080963135\n",
            "Epoch: 2, Training Loss: 2.9749128818511963\n",
            "Epoch: 2, Training Loss: 3.071418046951294\n",
            "Epoch: 2, Training Loss: 2.8749427795410156\n",
            "Epoch: 2, Training Loss: 2.9574859142303467\n",
            "Epoch: 2, Training Loss: 3.1232614517211914\n",
            "Epoch: 2, Training Loss: 2.9397053718566895\n",
            "Epoch: 2, Training Loss: 3.0760176181793213\n",
            "Epoch: 2, Training Loss: 3.0899369716644287\n",
            "Epoch: 2, Training Loss: 3.0268478393554688\n",
            "Epoch: 2, Training Loss: 3.0249526500701904\n",
            "Epoch: 2, Training Loss: 2.952442169189453\n",
            "Epoch: 2, Training Loss: 3.141439199447632\n",
            "Epoch: 2, Training Loss: 2.9477429389953613\n",
            "Epoch: 2, Training Loss: 3.1860599517822266\n",
            "Epoch: 2, Training Loss: 3.4140429496765137\n",
            "Epoch: 2, Training Loss: 2.9133667945861816\n",
            "Epoch: 2, Training Loss: 3.2973244190216064\n",
            "Epoch: 2, Training Loss: 3.1332125663757324\n",
            "Epoch: 2, Training Loss: 3.040242910385132\n",
            "Epoch: 2, Training Loss: 2.957192897796631\n",
            "Epoch: 2, Training Loss: 3.0021884441375732\n",
            "Epoch: 2, Training Loss: 3.2043769359588623\n",
            "Epoch: 2, Training Loss: 3.087038278579712\n",
            "Epoch: 2, Training Loss: 3.024545669555664\n",
            "Epoch: 2, Training Loss: 2.9675583839416504\n",
            "Epoch: 2, Training Loss: 3.0653693675994873\n",
            "Epoch: 2, Training Loss: 2.9418294429779053\n",
            "Epoch: 2, Training Loss: 2.8659651279449463\n",
            "Epoch: 2, Training Loss: 3.0852560997009277\n",
            "Epoch: 2, Training Loss: 2.8718671798706055\n",
            "Epoch: 2, Training Loss: 3.044236898422241\n",
            "Epoch: 2, Training Loss: 2.700824022293091\n",
            "Epoch: 2, Training Loss: 2.763481378555298\n",
            "Epoch: 2, Training Loss: 3.246642589569092\n",
            "Epoch: 2, Training Loss: 2.9482834339141846\n",
            "Epoch: 2, Training Loss: 2.9831364154815674\n",
            "Epoch: 2, Training Loss: 3.2817487716674805\n",
            "Epoch: 2, Training Loss: 3.051205635070801\n",
            "Epoch: 2, Training Loss: 3.002365827560425\n",
            "Epoch: 2, Training Loss: 2.737532138824463\n",
            "Epoch: 2, Training Loss: 2.849027633666992\n",
            "Epoch: 2, Training Loss: 2.702009439468384\n",
            "Epoch: 2, Training Loss: 3.1122961044311523\n",
            "Epoch: 2, Training Loss: 2.8377737998962402\n",
            "Epoch: 2, Training Loss: 3.2182230949401855\n",
            "Epoch: 2, Training Loss: 2.859541893005371\n",
            "Epoch: 2, Training Loss: 3.1064727306365967\n",
            "Epoch: 2, Training Loss: 3.316272497177124\n",
            "Epoch: 2, Training Loss: 2.81412935256958\n",
            "Epoch: 2, Training Loss: 3.0672647953033447\n",
            "Epoch: 2, Training Loss: 3.233673572540283\n",
            "Epoch: 2, Training Loss: 3.232697010040283\n",
            "Epoch: 2, Training Loss: 2.9239566326141357\n",
            "Epoch: 2, Training Loss: 2.831899642944336\n",
            "Epoch: 2, Training Loss: 3.1408958435058594\n",
            "Epoch: 2, Training Loss: 2.9541471004486084\n",
            "Epoch: 2, Training Loss: 3.1281065940856934\n",
            "Epoch: 2, Training Loss: 3.1443634033203125\n",
            "Epoch: 2, Training Loss: 2.978808641433716\n",
            "Epoch: 2, Training Loss: 3.1128180027008057\n",
            "Epoch: 2, Training Loss: 2.9844775199890137\n",
            "Epoch: 2, Training Loss: 3.0221753120422363\n",
            "Epoch: 2, Training Loss: 3.166402816772461\n",
            "Epoch: 2, Training Loss: 3.0341217517852783\n",
            "Epoch: 2, Training Loss: 2.975764751434326\n",
            "Epoch: 2, Training Loss: 3.1624107360839844\n",
            "Epoch: 2, Training Loss: 3.2518866062164307\n",
            "Epoch: 2, Training Loss: 2.687016725540161\n",
            "Epoch: 2, Training Loss: 2.961475133895874\n",
            "Epoch: 2, Training Loss: 3.1226937770843506\n",
            "Epoch: 2, Training Loss: 2.9720001220703125\n",
            "Epoch: 2, Training Loss: 3.1951026916503906\n",
            "Epoch: 2, Training Loss: 3.1646440029144287\n",
            "Epoch: 2, Training Loss: 2.9919495582580566\n",
            "Epoch: 2, Training Loss: 2.8204989433288574\n",
            "Epoch: 2, Training Loss: 3.044496536254883\n",
            "Epoch: 2, Training Loss: 2.8658218383789062\n",
            "Epoch: 2, Training Loss: 3.0062975883483887\n",
            "Epoch: 2, Training Loss: 3.2650043964385986\n",
            "Epoch: 2, Training Loss: 3.1898033618927\n",
            "Epoch: 2, Training Loss: 2.706061601638794\n",
            "Epoch: 2, Training Loss: 3.276299476623535\n",
            "Epoch: 2, Training Loss: 3.080249547958374\n",
            "Epoch: 2, Training Loss: 3.0561976432800293\n",
            "Epoch: 2, Training Loss: 2.786646842956543\n",
            "Epoch: 2, Training Loss: 3.1539502143859863\n",
            "Epoch: 2, Training Loss: 3.051847457885742\n",
            "Epoch: 2, Training Loss: 3.1420371532440186\n",
            "Epoch: 2, Training Loss: 2.9458255767822266\n",
            "Epoch: 2, Training Loss: 3.0786943435668945\n",
            "Epoch: 2, Training Loss: 2.9821066856384277\n",
            "Epoch: 2, Training Loss: 2.884563446044922\n",
            "Epoch: 2, Training Loss: 2.9055044651031494\n",
            "Epoch: 2, Training Loss: 3.2277019023895264\n",
            "Epoch: 2, Training Loss: 3.0756590366363525\n",
            "Epoch: 2, Training Loss: 3.0215816497802734\n",
            "Epoch: 2, Training Loss: 3.1944761276245117\n",
            "Epoch: 2, Training Loss: 3.027801752090454\n",
            "Epoch: 2, Training Loss: 3.0118868350982666\n",
            "Epoch: 2, Training Loss: 2.994809865951538\n",
            "Epoch: 2, Training Loss: 2.9208383560180664\n",
            "Epoch: 2, Training Loss: 3.0738143920898438\n",
            "Epoch: 2, Training Loss: 2.845475673675537\n",
            "Epoch: 2, Training Loss: 3.2275142669677734\n",
            "Epoch: 2, Training Loss: 2.9699482917785645\n",
            "Epoch: 2, Training Loss: 2.9943132400512695\n",
            "Epoch: 2, Training Loss: 3.2637906074523926\n",
            "Epoch: 2, Training Loss: 3.0237224102020264\n",
            "Epoch: 2, Training Loss: 3.1052849292755127\n",
            "Epoch: 2, Training Loss: 3.161691427230835\n",
            "Epoch: 2, Training Loss: 3.140899658203125\n",
            "Epoch: 2, Training Loss: 2.975557804107666\n",
            "Epoch: 2, Training Loss: 2.9600865840911865\n",
            "Epoch: 2, Training Loss: 2.9558587074279785\n",
            "Epoch: 2, Training Loss: 2.8217110633850098\n",
            "Epoch: 2, Training Loss: 2.8799827098846436\n",
            "Epoch: 2, Training Loss: 2.9905953407287598\n",
            "Epoch: 2, Training Loss: 3.1071410179138184\n",
            "Epoch: 2, Training Loss: 3.0252208709716797\n",
            "Epoch: 2, Training Loss: 3.008960723876953\n",
            "Epoch: 2, Training Loss: 3.045731782913208\n",
            "Epoch: 2, Training Loss: 2.9665825366973877\n",
            "Epoch: 2, Training Loss: 3.0422444343566895\n",
            "Epoch: 2, Training Loss: 3.0252175331115723\n",
            "Epoch: 2, Training Loss: 2.8221402168273926\n",
            "Epoch: 2, Training Loss: 3.04573392868042\n",
            "Epoch: 2, Training Loss: 3.0752062797546387\n",
            "Epoch: 2, Training Loss: 3.0811572074890137\n",
            "Epoch: 2, Training Loss: 2.918571710586548\n",
            "Epoch: 2, Training Loss: 3.066189765930176\n",
            "Epoch: 2, Training Loss: 2.9405057430267334\n",
            "Epoch: 2, Training Loss: 3.034336566925049\n",
            "Epoch: 2, Training Loss: 3.2039992809295654\n",
            "Epoch: 2, Training Loss: 2.826139450073242\n",
            "Epoch: 2, Training Loss: 2.819161891937256\n",
            "Epoch: 2, Training Loss: 2.853666305541992\n",
            "Epoch: 2, Training Loss: 3.143341302871704\n",
            "Epoch: 2, Training Loss: 3.1324362754821777\n",
            "Epoch: 2, Training Loss: 2.923945903778076\n",
            "Epoch: 2, Training Loss: 2.854390859603882\n",
            "Epoch: 2, Training Loss: 3.200974941253662\n",
            "Epoch: 2, Training Loss: 3.009945869445801\n",
            "Epoch: 2, Training Loss: 2.8312251567840576\n",
            "Epoch: 2, Training Loss: 3.392794132232666\n",
            "Epoch: 2, Training Loss: 2.8495237827301025\n",
            "Epoch: 2, Training Loss: 3.1829874515533447\n",
            "Epoch: 2, Training Loss: 2.96235728263855\n",
            "Epoch: 2, Training Loss: 3.0977320671081543\n",
            "Epoch: 2, Training Loss: 3.0451464653015137\n",
            "Epoch: 2, Training Loss: 3.0311594009399414\n",
            "Epoch: 2, Training Loss: 3.127614736557007\n",
            "Epoch: 2, Training Loss: 3.0581858158111572\n",
            "Epoch: 2, Training Loss: 3.0676932334899902\n",
            "Epoch: 2, Training Loss: 2.9133541584014893\n",
            "Epoch: 2, Training Loss: 2.9541170597076416\n",
            "Epoch: 2, Training Loss: 2.9282264709472656\n",
            "Epoch: 2, Training Loss: 3.1633405685424805\n",
            "Epoch: 2, Training Loss: 2.961488962173462\n",
            "Epoch: 2, Training Loss: 3.224827289581299\n",
            "Epoch: 2, Training Loss: 3.081906795501709\n",
            "Epoch: 2, Training Loss: 3.326220750808716\n",
            "Epoch: 2, Training Loss: 2.7033042907714844\n",
            "Epoch: 2, Training Loss: 3.13375186920166\n",
            "Epoch: 2, Training Loss: 2.9801700115203857\n",
            "Epoch: 2, Training Loss: 3.1798348426818848\n",
            "Epoch: 2, Training Loss: 2.898838758468628\n",
            "Epoch: 2, Training Loss: 2.9672794342041016\n",
            "Epoch: 2, Training Loss: 3.074528455734253\n",
            "Epoch: 2, Training Loss: 3.085318088531494\n",
            "Epoch: 2, Training Loss: 3.0240139961242676\n",
            "Epoch: 2, Training Loss: 2.87671160697937\n",
            "Epoch: 2, Training Loss: 3.073573589324951\n",
            "Epoch: 2, Training Loss: 3.160087823867798\n",
            "Epoch: 2, Training Loss: 3.0558629035949707\n",
            "Epoch: 2, Training Loss: 3.0151607990264893\n",
            "Epoch: 2, Training Loss: 2.9150943756103516\n",
            "Epoch: 2, Training Loss: 3.016754388809204\n",
            "Epoch: 2, Training Loss: 2.967088460922241\n",
            "Epoch: 2, Training Loss: 3.227949857711792\n",
            "Epoch: 2, Training Loss: 3.0565788745880127\n",
            "Epoch: 2, Training Loss: 3.0182976722717285\n",
            "Epoch: 2, Training Loss: 3.106212854385376\n",
            "Epoch: 2, Training Loss: 2.959383010864258\n",
            "Epoch: 2, Training Loss: 3.117377519607544\n",
            "Epoch: 2, Training Loss: 3.09631609916687\n",
            "Epoch: 2, Training Loss: 2.819493532180786\n",
            "Epoch: 2, Training Loss: 2.7852554321289062\n",
            "Epoch: 2, Training Loss: 2.7864248752593994\n",
            "Epoch: 2, Training Loss: 3.1698572635650635\n",
            "Epoch: 2, Training Loss: 3.0250134468078613\n",
            "Epoch: 2, Training Loss: 2.9427294731140137\n",
            "Epoch: 2, Training Loss: 3.3262405395507812\n",
            "Epoch: 2, Training Loss: 2.9171793460845947\n",
            "Epoch: 2, Training Loss: 2.9811995029449463\n",
            "Epoch: 2, Training Loss: 2.899334192276001\n",
            "Epoch: 2, Training Loss: 2.823167324066162\n",
            "Epoch: 2, Training Loss: 2.791877508163452\n",
            "Epoch: 2, Training Loss: 3.0346128940582275\n",
            "Epoch: 2, Training Loss: 2.86230206489563\n",
            "Epoch: 2, Training Loss: 3.06846284866333\n",
            "Epoch: 2, Training Loss: 2.8441760540008545\n",
            "Epoch: 2, Training Loss: 3.1370038986206055\n",
            "Epoch: 2, Training Loss: 3.0003089904785156\n",
            "Epoch: 2, Training Loss: 2.9829494953155518\n",
            "Epoch: 2, Training Loss: 3.0918374061584473\n",
            "Epoch: 2, Training Loss: 3.132002353668213\n",
            "Epoch: 2, Training Loss: 3.1548044681549072\n",
            "Epoch: 2, Training Loss: 2.9693169593811035\n",
            "Epoch: 2, Training Loss: 2.8560190200805664\n",
            "Epoch: 2, Training Loss: 3.013946771621704\n",
            "Epoch: 2, Training Loss: 3.149472713470459\n",
            "Epoch: 2, Training Loss: 2.806952476501465\n",
            "Epoch: 2, Training Loss: 3.025012969970703\n",
            "Epoch: 2, Training Loss: 2.9325098991394043\n",
            "Epoch: 2, Training Loss: 2.7411487102508545\n",
            "Epoch: 2, Training Loss: 2.8405325412750244\n",
            "Epoch: 2, Training Loss: 2.8228275775909424\n",
            "Epoch: 2, Training Loss: 2.794346570968628\n",
            "Epoch: 2, Training Loss: 2.9971749782562256\n",
            "Epoch: 2, Training Loss: 2.6982829570770264\n",
            "Epoch: 2, Training Loss: 2.834249973297119\n",
            "Epoch: 2, Training Loss: 3.0721323490142822\n",
            "Epoch: 2, Training Loss: 2.929957866668701\n",
            "Epoch: 2, Training Loss: 2.8188629150390625\n",
            "Epoch: 2, Training Loss: 2.8907620906829834\n",
            "Epoch: 2, Training Loss: 2.780984401702881\n",
            "Epoch: 2, Training Loss: 2.8995909690856934\n",
            "Epoch: 2, Training Loss: 3.0550310611724854\n",
            "Epoch: 2, Training Loss: 3.114020586013794\n",
            "Epoch: 2, Training Loss: 2.858490467071533\n",
            "Epoch: 2, Training Loss: 2.929600477218628\n",
            "Epoch: 2, Training Loss: 2.8831498622894287\n",
            "Epoch: 2, Training Loss: 3.1668663024902344\n",
            "Epoch: 2, Training Loss: 2.83406662940979\n",
            "Epoch: 2, Training Loss: 3.2725040912628174\n",
            "Epoch: 2, Training Loss: 3.1014256477355957\n",
            "Epoch: 2, Training Loss: 2.8847172260284424\n",
            "Epoch: 2, Training Loss: 2.859732151031494\n",
            "Epoch: 2, Training Loss: 3.029890775680542\n",
            "Epoch: 2, Training Loss: 3.2073488235473633\n",
            "Epoch: 2, Training Loss: 2.570265769958496\n",
            "Epoch: 2, Training Loss: 2.842740058898926\n",
            "Epoch: 2, Training Loss: 2.892394542694092\n",
            "Epoch: 2, Training Loss: 3.193495750427246\n",
            "Epoch: 2, Training Loss: 2.941709280014038\n",
            "Epoch: 2, Training Loss: 3.0655112266540527\n",
            "Epoch: 2, Training Loss: 3.1864781379699707\n",
            "Epoch: 2, Training Loss: 3.112622022628784\n",
            "Epoch: 2, Training Loss: 2.644643783569336\n",
            "Epoch: 2, Training Loss: 3.152236223220825\n",
            "Epoch: 2, Training Loss: 2.8866629600524902\n",
            "Epoch: 2, Training Loss: 2.7208104133605957\n",
            "Epoch: 2, Training Loss: 3.3223795890808105\n",
            "Epoch: 2, Training Loss: 2.9911718368530273\n",
            "Epoch: 2, Training Loss: 2.86641263961792\n",
            "Epoch: 2, Training Loss: 2.7781684398651123\n",
            "Epoch: 2, Training Loss: 2.7577872276306152\n",
            "Epoch: 2, Training Loss: 3.1749696731567383\n",
            "Epoch: 2, Training Loss: 2.8791818618774414\n",
            "Epoch: 2, Training Loss: 2.797180414199829\n",
            "Epoch: 2, Training Loss: 2.8443400859832764\n",
            "Epoch: 2, Training Loss: 2.929769515991211\n",
            "Epoch: 2, Training Loss: 3.326944589614868\n",
            "Epoch: 2, Training Loss: 3.0622785091400146\n",
            "Epoch: 2, Training Loss: 3.210878372192383\n",
            "Epoch: 2, Training Loss: 2.9260826110839844\n",
            "Epoch: 2, Training Loss: 3.0141584873199463\n",
            "Epoch: 2, Training Loss: 2.992643117904663\n",
            "Epoch: 2, Training Loss: 2.8871498107910156\n",
            "Epoch: 2, Training Loss: 3.0661563873291016\n",
            "Epoch: 2, Training Loss: 2.9921090602874756\n",
            "Epoch: 2, Training Loss: 2.75205659866333\n",
            "Epoch: 2, Training Loss: 2.7868361473083496\n",
            "Epoch: 2, Training Loss: 2.6638050079345703\n",
            "Epoch: 2, Training Loss: 3.081820249557495\n",
            "Epoch: 2, Training Loss: 2.9679222106933594\n",
            "Epoch: 2, Training Loss: 2.948679208755493\n",
            "Epoch: 2, Training Loss: 3.0996522903442383\n",
            "Epoch: 2, Training Loss: 2.9528374671936035\n",
            "Epoch: 2, Training Loss: 2.8417701721191406\n",
            "Epoch: 2, Training Loss: 2.8197333812713623\n",
            "Epoch: 2, Training Loss: 2.745037317276001\n",
            "Epoch: 2, Training Loss: 3.02376651763916\n",
            "Epoch: 2, Training Loss: 2.805447578430176\n",
            "Epoch: 2, Training Loss: 3.260436773300171\n",
            "Epoch: 2, Training Loss: 3.0996510982513428\n",
            "Epoch: 2, Training Loss: 2.8770744800567627\n",
            "Epoch: 2, Training Loss: 3.070702075958252\n",
            "Epoch: 2, Training Loss: 3.039076805114746\n",
            "Epoch: 2, Training Loss: 2.6921072006225586\n",
            "Epoch: 2, Training Loss: 2.601344585418701\n",
            "Epoch: 2, Training Loss: 3.0328190326690674\n",
            "Epoch: 2, Training Loss: 3.074744701385498\n",
            "Epoch: 2, Training Loss: 3.0304534435272217\n",
            "Epoch: 2, Training Loss: 2.8847053050994873\n",
            "Epoch: 2, Training Loss: 2.843801736831665\n",
            "Epoch: 2, Training Loss: 2.8688273429870605\n",
            "Epoch: 2, Training Loss: 2.950303077697754\n",
            "Epoch: 2, Training Loss: 2.8304758071899414\n",
            "Epoch: 2, Training Loss: 2.897536039352417\n",
            "Epoch: 2, Training Loss: 2.9139304161071777\n",
            "Epoch: 2, Training Loss: 3.057290554046631\n",
            "Epoch: 2, Training Loss: 2.988748788833618\n",
            "Epoch: 2, Training Loss: 3.1252236366271973\n",
            "Epoch: 2, Training Loss: 2.875\n",
            "Epoch: 2, Training Loss: 2.9556407928466797\n",
            "Epoch: 2, Training Loss: 3.0374975204467773\n",
            "Epoch: 2, Training Loss: 2.977309226989746\n",
            "Epoch: 2, Training Loss: 3.1542718410491943\n",
            "Epoch: 2, Training Loss: 3.034729242324829\n",
            "Epoch: 2, Training Loss: 2.8297674655914307\n",
            "Epoch: 2, Training Loss: 3.4168779850006104\n",
            "Epoch: 2, Training Loss: 2.977090358734131\n",
            "Epoch: 2, Training Loss: 3.159637451171875\n",
            "Epoch: 2, Training Loss: 3.125211715698242\n",
            "Epoch: 2, Training Loss: 2.9807748794555664\n",
            "Epoch: 2, Training Loss: 2.7468390464782715\n",
            "Epoch: 2, Training Loss: 2.8356409072875977\n",
            "Epoch: 2, Training Loss: 2.6764450073242188\n",
            "Epoch: 2, Training Loss: 3.0670557022094727\n",
            "Epoch: 2, Training Loss: 2.7516307830810547\n",
            "Epoch: 2, Training Loss: 3.1313934326171875\n",
            "Epoch: 2, Training Loss: 2.8545031547546387\n",
            "Epoch: 2, Training Loss: 2.8959734439849854\n",
            "Epoch: 2, Training Loss: 3.0119028091430664\n",
            "Epoch: 2, Training Loss: 2.8436269760131836\n",
            "Epoch: 2, Training Loss: 3.0719149112701416\n",
            "Epoch: 2, Training Loss: 3.005530595779419\n",
            "Epoch: 2, Training Loss: 3.0771045684814453\n",
            "Epoch: 2, Training Loss: 2.968306064605713\n",
            "Epoch: 2, Training Loss: 3.003603458404541\n",
            "Epoch: 2, Training Loss: 2.9202723503112793\n",
            "Epoch: 2, Training Loss: 3.015986442565918\n",
            "Epoch: 2, Training Loss: 2.7832279205322266\n",
            "Epoch: 2, Training Loss: 2.9096100330352783\n",
            "Epoch: 2, Training Loss: 3.02254581451416\n",
            "Epoch: 2, Training Loss: 2.9340012073516846\n",
            "Epoch: 2, Training Loss: 2.5478076934814453\n",
            "Epoch: 2, Training Loss: 2.997014045715332\n",
            "Epoch: 2, Training Loss: 2.6451590061187744\n",
            "Epoch: 2, Training Loss: 2.7712464332580566\n",
            "Epoch: 2, Training Loss: 3.3219926357269287\n",
            "Epoch: 2, Training Loss: 2.934251546859741\n",
            "Epoch: 2, Training Loss: 2.830990791320801\n",
            "Epoch: 2, Training Loss: 2.688455820083618\n",
            "Epoch: 2, Training Loss: 2.9358022212982178\n",
            "Epoch: 2, Training Loss: 2.856708288192749\n",
            "Epoch: 2, Training Loss: 2.8941938877105713\n",
            "Epoch: 2, Training Loss: 3.035367727279663\n",
            "Epoch: 2, Training Loss: 3.025330066680908\n",
            "Epoch: 2, Training Loss: 3.0237793922424316\n",
            "Epoch: 2, Training Loss: 3.191480875015259\n",
            "Epoch: 2, Training Loss: 2.9898152351379395\n",
            "Epoch: 2, Training Loss: 3.015491485595703\n",
            "Epoch: 2, Training Loss: 2.737894058227539\n",
            "Epoch: 2, Training Loss: 2.427619218826294\n",
            "Epoch: 2, Training Loss: 3.1059226989746094\n",
            "Epoch: 2, Training Loss: 2.983341932296753\n",
            "Epoch: 2, Training Loss: 2.8633251190185547\n",
            "Epoch: 2, Training Loss: 2.760378837585449\n",
            "Epoch: 2, Training Loss: 2.712559700012207\n",
            "Epoch: 2, Training Loss: 2.7857353687286377\n",
            "Epoch: 2, Training Loss: 2.7124345302581787\n",
            "Epoch: 2, Training Loss: 2.835035562515259\n",
            "Epoch: 2, Training Loss: 2.9218413829803467\n",
            "Epoch: 2, Training Loss: 2.937432289123535\n",
            "Epoch: 2, Training Loss: 3.058295726776123\n",
            "Epoch: 2, Training Loss: 2.963317632675171\n",
            "Epoch: 2, Training Loss: 2.74924373626709\n",
            "Epoch: 2, Training Loss: 2.6707210540771484\n",
            "Epoch: 2, Training Loss: 3.002533197402954\n",
            "Epoch: 2, Training Loss: 2.938976526260376\n",
            "Epoch: 2, Training Loss: 2.5063846111297607\n",
            "Epoch: 2, Training Loss: 2.8934977054595947\n",
            "Epoch: 2, Training Loss: 2.800935745239258\n",
            "Epoch: 2, Training Loss: 2.738363742828369\n",
            "Epoch: 2, Training Loss: 2.9437057971954346\n",
            "Epoch: 2, Training Loss: 2.7279903888702393\n",
            "Epoch: 2, Training Loss: 2.8783047199249268\n",
            "Epoch: 2, Training Loss: 2.7410330772399902\n",
            "Epoch: 2, Training Loss: 2.8525755405426025\n",
            "Epoch: 2, Training Loss: 2.7645270824432373\n",
            "Epoch: 2, Training Loss: 2.975947141647339\n",
            "Epoch: 2, Training Loss: 2.921154499053955\n",
            "Epoch: 2, Training Loss: 3.1601057052612305\n",
            "Epoch: 2, Training Loss: 2.9194681644439697\n",
            "Epoch: 2, Training Loss: 2.79360032081604\n",
            "Epoch: 2, Training Loss: 2.800381660461426\n",
            "Epoch: 2, Training Loss: 2.9088850021362305\n",
            "Epoch: 2, Training Loss: 2.7265493869781494\n",
            "Epoch: 2, Training Loss: 2.8901870250701904\n",
            "Epoch: 2, Training Loss: 3.2100162506103516\n",
            "Epoch: 2, Training Loss: 2.9020895957946777\n",
            "Epoch: 2, Training Loss: 3.0909929275512695\n",
            "Epoch: 2, Training Loss: 2.8276727199554443\n",
            "Epoch: 2, Training Loss: 3.1023499965667725\n",
            "Epoch: 2, Training Loss: 2.941667318344116\n",
            "Epoch: 2, Training Loss: 2.9551479816436768\n",
            "Epoch: 2, Training Loss: 2.8321938514709473\n",
            "Epoch: 2, Training Loss: 3.033796787261963\n",
            "Epoch: 2, Training Loss: 2.833714008331299\n",
            "Epoch: 2, Training Loss: 3.0693209171295166\n",
            "Epoch: 2, Training Loss: 3.0727174282073975\n",
            "Epoch: 2, Training Loss: 2.809920310974121\n",
            "Epoch: 2, Training Loss: 2.7383387088775635\n",
            "Epoch: 2, Training Loss: 3.0722548961639404\n",
            "Epoch: 2, Training Loss: 2.9846835136413574\n",
            "Epoch: 2, Training Loss: 2.92182993888855\n",
            "Epoch: 2, Training Loss: 2.718400001525879\n",
            "Epoch: 2, Training Loss: 2.803933620452881\n",
            "Epoch: 2, Training Loss: 2.8498940467834473\n",
            "Epoch: 2, Training Loss: 2.7055537700653076\n",
            "Epoch: 2, Training Loss: 2.696777820587158\n",
            "Epoch: 2, Training Loss: 2.6378653049468994\n",
            "Epoch: 2, Training Loss: 2.7096006870269775\n",
            "Epoch: 2, Training Loss: 3.022855281829834\n",
            "Epoch: 2, Training Loss: 2.889765977859497\n",
            "Epoch: 2, Training Loss: 2.937744379043579\n",
            "Epoch: 2, Training Loss: 3.000342845916748\n",
            "Epoch: 2, Training Loss: 2.9380013942718506\n",
            "Epoch: 2, Training Loss: 2.75138521194458\n",
            "Epoch: 2, Training Loss: 2.921227216720581\n",
            "Epoch: 2, Training Loss: 2.9802284240722656\n",
            "Epoch: 2, Training Loss: 2.586381673812866\n",
            "Epoch: 2, Training Loss: 2.865734100341797\n",
            "Epoch: 2, Training Loss: 3.081519365310669\n",
            "Epoch: 2, Training Loss: 2.9162163734436035\n",
            "Epoch: 2, Training Loss: 3.274634838104248\n",
            "Epoch: 2, Training Loss: 3.007615089416504\n",
            "Epoch: 2, Training Loss: 2.9880926609039307\n",
            "Epoch: 2, Training Loss: 2.736450672149658\n",
            "Epoch: 2, Training Loss: 2.9829070568084717\n",
            "Epoch: 2, Training Loss: 2.709258556365967\n",
            "Epoch: 2, Training Loss: 2.783555269241333\n",
            "Epoch: 2, Training Loss: 2.9411661624908447\n",
            "Epoch: 2, Training Loss: 2.957202911376953\n",
            "Epoch: 2, Training Loss: 2.697680711746216\n",
            "Epoch: 2, Training Loss: 3.049307346343994\n",
            "Epoch: 2, Training Loss: 2.9712917804718018\n",
            "Epoch: 2, Training Loss: 2.7931969165802\n",
            "Epoch: 2, Training Loss: 2.7730278968811035\n",
            "Epoch: 2, Training Loss: 2.851827621459961\n",
            "Epoch: 2, Training Loss: 3.1922242641448975\n",
            "Epoch: 2, Training Loss: 3.0250940322875977\n",
            "Epoch: 2, Training Loss: 2.967805862426758\n",
            "Epoch: 2, Training Loss: 2.7833290100097656\n",
            "Epoch: 2, Training Loss: 2.8157243728637695\n",
            "Epoch: 2, Training Loss: 2.740180730819702\n",
            "Epoch: 2, Training Loss: 2.7342920303344727\n",
            "Epoch: 2, Training Loss: 2.716681480407715\n",
            "Epoch: 2, Training Loss: 2.926879644393921\n",
            "Epoch: 2, Training Loss: 2.859497308731079\n",
            "Epoch: 2, Training Loss: 3.148336410522461\n",
            "Epoch: 2, Training Loss: 2.8824875354766846\n",
            "Epoch: 2, Training Loss: 2.9405131340026855\n",
            "Epoch: 2, Training Loss: 3.0939536094665527\n",
            "Epoch: 2, Training Loss: 3.043706178665161\n",
            "Epoch: 2, Training Loss: 2.801607608795166\n",
            "Epoch: 2, Training Loss: 2.885923385620117\n",
            "Epoch: 2, Training Loss: 2.807840585708618\n",
            "Epoch: 2, Training Loss: 2.9628701210021973\n",
            "Epoch: 2, Training Loss: 2.7957406044006348\n",
            "Epoch: 2, Training Loss: 2.7561779022216797\n",
            "Epoch: 2, Training Loss: 3.15915584564209\n",
            "Epoch: 2, Training Loss: 3.0256662368774414\n",
            "Epoch: 2, Training Loss: 2.944676637649536\n",
            "Epoch: 2, Training Loss: 2.886348247528076\n",
            "Epoch: 2, Training Loss: 2.873683214187622\n",
            "Epoch: 2, Training Loss: 2.8966546058654785\n",
            "Epoch: 2, Training Loss: 2.8531930446624756\n",
            "Epoch: 2, Training Loss: 2.808720350265503\n",
            "Epoch: 2, Training Loss: 2.9263017177581787\n",
            "Epoch: 2, Training Loss: 2.7739980220794678\n",
            "Epoch: 2, Training Loss: 2.6432712078094482\n",
            "Epoch: 2, Training Loss: 2.831712245941162\n",
            "Epoch: 2, Training Loss: 2.854968786239624\n",
            "Epoch: 2, Training Loss: 3.060532331466675\n",
            "Epoch: 2, Training Loss: 3.078472852706909\n",
            "Epoch: 2, Training Loss: 3.0746471881866455\n",
            "Epoch: 2, Training Loss: 3.0061287879943848\n",
            "Epoch: 2, Training Loss: 2.7069029808044434\n",
            "Epoch: 2, Training Loss: 2.8090171813964844\n",
            "Epoch: 2, Training Loss: 3.0143449306488037\n",
            "Epoch: 2, Training Loss: 2.62837553024292\n",
            "Epoch: 2, Training Loss: 2.860236167907715\n",
            "Epoch: 2, Training Loss: 2.853093147277832\n",
            "Epoch: 2, Training Loss: 2.9447619915008545\n",
            "Epoch: 2, Training Loss: 2.690088987350464\n",
            "Epoch: 2, Training Loss: 2.8566060066223145\n",
            "Epoch: 2, Training Loss: 2.9225292205810547\n",
            "Epoch: 2, Training Loss: 3.0080385208129883\n",
            "Epoch: 2, Training Loss: 2.8622841835021973\n",
            "Epoch: 2, Training Loss: 2.9335155487060547\n",
            "Epoch: 2, Training Loss: 3.0567705631256104\n",
            "Epoch: 2, Training Loss: 2.7571823596954346\n",
            "Epoch: 2, Training Loss: 3.128849506378174\n",
            "Epoch: 2, Training Loss: 2.9870777130126953\n",
            "Epoch: 2, Training Loss: 3.037811756134033\n",
            "Epoch: 2, Training Loss: 2.9170074462890625\n",
            "Epoch: 2, Training Loss: 2.8308932781219482\n",
            "Epoch: 2, Training Loss: 2.792815923690796\n",
            "Epoch: 2, Training Loss: 3.022030830383301\n",
            "Epoch: 2, Training Loss: 2.9142563343048096\n",
            "Epoch: 2, Training Loss: 2.967383861541748\n",
            "Epoch: 2, Training Loss: 2.968034267425537\n",
            "Epoch: 2, Training Loss: 3.047621965408325\n",
            "Epoch: 2, Training Loss: 2.8477623462677\n",
            "Epoch: 2, Training Loss: 3.035059690475464\n",
            "Epoch: 2, Training Loss: 2.826051712036133\n",
            "Epoch: 2, Training Loss: 2.5817859172821045\n",
            "Epoch: 2, Training Loss: 2.969970464706421\n",
            "Epoch: 2, Training Loss: 3.0715627670288086\n",
            "Epoch: 2, Training Loss: 2.742359161376953\n",
            "Epoch: 2, Training Loss: 2.961883783340454\n",
            "Epoch: 2, Training Loss: 2.9953010082244873\n",
            "Epoch: 2, Training Loss: 2.814453363418579\n",
            "Epoch: 2, Training Loss: 2.866016149520874\n",
            "Epoch: 2, Training Loss: 2.9042270183563232\n",
            "Epoch: 2, Training Loss: 2.773453712463379\n",
            "Epoch: 2, Training Loss: 3.1360151767730713\n",
            "Epoch: 2, Training Loss: 2.8976807594299316\n",
            "Epoch: 2, Training Loss: 2.79473876953125\n",
            "Epoch: 2, Training Loss: 2.966644048690796\n",
            "Epoch: 2, Training Loss: 2.856358051300049\n",
            "Epoch: 2, Training Loss: 2.941638708114624\n",
            "Epoch: 2, Training Loss: 2.738373279571533\n",
            "Epoch: 2, Training Loss: 2.7721457481384277\n",
            "Epoch: 2, Training Loss: 2.5440309047698975\n",
            "Epoch: 2, Training Loss: 2.9696080684661865\n",
            "Epoch: 2, Training Loss: 2.790891170501709\n",
            "Epoch: 2, Training Loss: 2.9951374530792236\n",
            "Epoch: 2, Training Loss: 2.80928897857666\n",
            "Epoch: 2, Training Loss: 2.5430097579956055\n",
            "Epoch: 2, Training Loss: 2.5539636611938477\n",
            "Epoch: 2, Training Loss: 2.6389520168304443\n",
            "Epoch: 2, Training Loss: 3.028181552886963\n",
            "Epoch: 2, Training Loss: 2.599346160888672\n",
            "Epoch: 2, Training Loss: 2.781925916671753\n",
            "Epoch: 2, Training Loss: 2.817214250564575\n",
            "Epoch: 2, Training Loss: 2.9852094650268555\n",
            "Epoch: 2, Training Loss: 2.916879892349243\n",
            "Epoch: 2, Training Loss: 2.8791866302490234\n",
            "Epoch: 2, Training Loss: 2.889338970184326\n",
            "Epoch: 2, Training Loss: 2.9305694103240967\n",
            "Epoch: 2, Training Loss: 3.000326156616211\n",
            "Epoch: 2, Training Loss: 2.8851983547210693\n",
            "Epoch: 2, Training Loss: 2.892655611038208\n",
            "Epoch: 2, Training Loss: 2.839503288269043\n",
            "Epoch: 2, Training Loss: 2.874263286590576\n",
            "Epoch: 2, Training Loss: 2.635833263397217\n",
            "Epoch: 2, Training Loss: 2.9799246788024902\n",
            "Epoch: 2, Training Loss: 3.1265106201171875\n",
            "Epoch: 2, Training Loss: 2.748642921447754\n",
            "Epoch: 2, Training Loss: 2.8629391193389893\n",
            "Epoch: 2, Training Loss: 3.0732524394989014\n",
            "Epoch: 2, Training Loss: 3.1921629905700684\n",
            "Epoch: 2, Training Loss: 2.8602144718170166\n",
            "Epoch: 2, Training Loss: 2.955933094024658\n",
            "Epoch: 2, Training Loss: 2.681100606918335\n",
            "Epoch: 2, Training Loss: 2.9620752334594727\n",
            "Epoch: 2, Training Loss: 2.7874245643615723\n",
            "Epoch: 2, Training Loss: 2.992037296295166\n",
            "Epoch: 2, Training Loss: 2.772597551345825\n",
            "Epoch: 2, Training Loss: 2.6754729747772217\n",
            "Epoch: 2, Training Loss: 2.9258341789245605\n",
            "Epoch: 2, Training Loss: 2.9061737060546875\n",
            "Epoch: 2, Training Loss: 2.8996939659118652\n",
            "Epoch: 2, Training Loss: 2.741713523864746\n",
            "Epoch: 2, Training Loss: 2.796186685562134\n",
            "Epoch: 2, Training Loss: 3.1166927814483643\n",
            "Epoch: 2, Training Loss: 3.0258491039276123\n",
            "Epoch: 2, Training Loss: 2.836577892303467\n",
            "Epoch: 2, Training Loss: 3.0432372093200684\n",
            "Epoch: 2, Training Loss: 2.827188491821289\n",
            "Epoch: 2, Training Loss: 2.929513931274414\n",
            "Epoch: 2, Training Loss: 2.8939647674560547\n",
            "Epoch: 2, Training Loss: 2.705080270767212\n",
            "Epoch: 2, Training Loss: 2.618600845336914\n",
            "Epoch: 2, Training Loss: 3.023157835006714\n",
            "Epoch: 2, Training Loss: 2.690711259841919\n",
            "Epoch: 2, Training Loss: 2.803244113922119\n",
            "Epoch: 2, Training Loss: 2.6788551807403564\n",
            "Epoch: 2, Training Loss: 2.910632371902466\n",
            "Epoch: 2, Training Loss: 2.835632562637329\n",
            "Epoch: 2, Training Loss: 2.837259531021118\n",
            "Epoch: 2, Training Loss: 2.6140596866607666\n",
            "Epoch: 2, Training Loss: 2.865734338760376\n",
            "Epoch: 2, Training Loss: 2.7774720191955566\n",
            "Epoch: 2, Training Loss: 2.885152816772461\n",
            "Epoch: 2, Training Loss: 2.737053394317627\n",
            "Epoch: 2, Training Loss: 2.9203569889068604\n",
            "Epoch: 2, Training Loss: 3.019097089767456\n",
            "Epoch: 2, Training Loss: 2.7584240436553955\n",
            "Epoch: 2, Training Loss: 2.7830185890197754\n",
            "Epoch: 2, Training Loss: 2.9259424209594727\n",
            "Epoch: 2, Training Loss: 3.1046385765075684\n",
            "Epoch: 2, Training Loss: 3.0664429664611816\n",
            "Epoch: 2, Training Loss: 2.677204132080078\n",
            "Epoch: 2, Training Loss: 2.797545909881592\n",
            "Epoch: 2, Training Loss: 2.725569486618042\n",
            "Epoch: 2, Training Loss: 2.952328681945801\n",
            "Epoch: 2, Training Loss: 2.961609363555908\n",
            "Epoch: 2, Training Loss: 2.6225459575653076\n",
            "Epoch: 2, Training Loss: 2.8505136966705322\n",
            "Epoch: 2, Training Loss: 2.7065155506134033\n",
            "Epoch: 2, Training Loss: 2.8557543754577637\n",
            "Epoch: 2, Training Loss: 2.5779683589935303\n",
            "Epoch: 2, Training Loss: 2.9358036518096924\n",
            "Epoch: 2, Training Loss: 3.0021934509277344\n",
            "Epoch: 2, Training Loss: 2.7622005939483643\n",
            "Epoch: 2, Training Loss: 2.8370285034179688\n",
            "Epoch: 2, Training Loss: 2.8833200931549072\n",
            "Epoch: 2, Training Loss: 2.9696273803710938\n",
            "Epoch: 2, Training Loss: 2.5112357139587402\n",
            "Epoch: 2, Training Loss: 2.5031661987304688\n",
            "Epoch: 2, Training Loss: 2.689329147338867\n",
            "Epoch: 2, Training Loss: 2.6352758407592773\n",
            "Epoch: 2, Training Loss: 2.659921884536743\n",
            "Epoch: 2, Training Loss: 2.7782199382781982\n",
            "Epoch: 2, Training Loss: 3.087209463119507\n",
            "Epoch: 2, Training Loss: 2.859092950820923\n",
            "Epoch: 2, Training Loss: 2.8025362491607666\n",
            "Epoch: 2, Training Loss: 2.674590587615967\n",
            "Epoch: 2, Training Loss: 2.8050589561462402\n",
            "Epoch: 2, Training Loss: 2.782721757888794\n",
            "Epoch: 2, Training Loss: 2.7089595794677734\n",
            "Epoch: 2, Training Loss: 2.9728870391845703\n",
            "Epoch: 2, Training Loss: 2.9951846599578857\n",
            "Epoch: 2, Training Loss: 2.7411983013153076\n",
            "Epoch: 2, Training Loss: 2.8288862705230713\n",
            "Epoch: 2, Training Loss: 2.853654384613037\n",
            "Epoch: 2, Training Loss: 2.9102866649627686\n",
            "Epoch: 2, Training Loss: 2.825239419937134\n",
            "Epoch: 2, Training Loss: 2.4691569805145264\n",
            "Epoch: 2, Training Loss: 3.009171962738037\n",
            "Epoch: 2, Training Loss: 2.63761830329895\n",
            "Epoch: 2, Training Loss: 3.019305467605591\n",
            "Epoch: 2, Training Loss: 2.7144548892974854\n",
            "Epoch: 2, Training Loss: 3.0972015857696533\n",
            "Epoch: 2, Training Loss: 2.548431634902954\n",
            "Epoch: 2, Training Loss: 2.64815616607666\n",
            "Epoch: 2, Training Loss: 2.9416861534118652\n",
            "Epoch: 2, Training Loss: 2.9436678886413574\n",
            "Epoch: 2, Training Loss: 2.9534623622894287\n",
            "Epoch: 2, Training Loss: 2.8761515617370605\n",
            "Epoch: 2, Training Loss: 2.7862937450408936\n",
            "Epoch: 2, Training Loss: 3.0472376346588135\n",
            "Epoch: 2, Training Loss: 2.9897372722625732\n",
            "Epoch: 2, Training Loss: 3.0712430477142334\n",
            "Epoch: 2, Training Loss: 2.69123911857605\n",
            "Epoch: 2, Training Loss: 2.7098889350891113\n",
            "Epoch: 2, Training Loss: 3.1033687591552734\n",
            "Epoch: 2, Training Loss: 2.7546558380126953\n",
            "Epoch: 2, Training Loss: 3.310840606689453\n",
            "Epoch: 2, Training Loss: 2.8587145805358887\n",
            "Epoch: 2, Training Loss: 2.925905466079712\n",
            "Epoch: 2, Training Loss: 2.805345296859741\n",
            "Epoch: 2, Training Loss: 2.8271634578704834\n",
            "Epoch: 2, Training Loss: 2.716958999633789\n",
            "Epoch: 2, Training Loss: 3.032200574874878\n",
            "Epoch: 2, Training Loss: 2.7543654441833496\n",
            "Epoch: 2, Training Loss: 2.885300397872925\n",
            "Epoch: 2, Training Loss: 2.897249698638916\n",
            "Epoch: 2, Training Loss: 2.6948142051696777\n",
            "Epoch: 2, Training Loss: 2.661658525466919\n",
            "Epoch: 2, Training Loss: 2.632861375808716\n",
            "Epoch: 2, Training Loss: 2.864605188369751\n",
            "Epoch: 2, Training Loss: 2.8719606399536133\n",
            "Epoch: 2, Training Loss: 2.7939581871032715\n",
            "Epoch: 2, Training Loss: 3.031376361846924\n",
            "Epoch: 2, Training Loss: 2.819701671600342\n",
            "Epoch: 2, Training Loss: 2.7293026447296143\n",
            "Epoch: 2, Training Loss: 3.09369158744812\n",
            "Epoch: 2, Training Loss: 2.8936543464660645\n",
            "Epoch: 2, Training Loss: 2.696373224258423\n",
            "Epoch: 2, Training Loss: 2.75049090385437\n",
            "Epoch: 2, Training Loss: 2.9732062816619873\n",
            "Epoch: 2, Training Loss: 2.989612340927124\n",
            "Epoch: 2, Training Loss: 2.769509792327881\n",
            "Epoch: 2, Training Loss: 3.1610958576202393\n",
            "Epoch: 2, Training Loss: 2.8386950492858887\n",
            "Epoch: 2, Training Loss: 2.4638752937316895\n",
            "Epoch: 2, Training Loss: 2.9248814582824707\n",
            "Epoch: 2, Training Loss: 2.79874587059021\n",
            "Epoch: 2, Training Loss: 2.8193199634552\n",
            "Epoch: 2, Training Loss: 3.0248281955718994\n",
            "Epoch: 2, Training Loss: 2.630481004714966\n",
            "Epoch: 2, Training Loss: 2.9365482330322266\n",
            "Epoch: 2, Training Loss: 2.7656095027923584\n",
            "Epoch: 2, Training Loss: 2.9020323753356934\n",
            "Epoch: 2, Training Loss: 2.8889639377593994\n",
            "Epoch: 2, Training Loss: 2.845787525177002\n",
            "Epoch: 2, Training Loss: 2.7885937690734863\n",
            "Epoch: 2, Training Loss: 2.8724517822265625\n",
            "Epoch: 2, Training Loss: 3.0033302307128906\n",
            "Epoch: 2, Training Loss: 2.6671183109283447\n",
            "Epoch: 2, Training Loss: 2.7151901721954346\n",
            "Epoch: 2, Training Loss: 2.990776777267456\n",
            "Epoch: 2, Training Loss: 2.648824691772461\n",
            "Epoch: 2, Training Loss: 2.68845272064209\n",
            "Epoch: 2, Training Loss: 2.810030221939087\n",
            "Epoch: 2, Training Loss: 2.9463577270507812\n",
            "Epoch: 2, Training Loss: 2.8011841773986816\n",
            "Epoch: 2, Training Loss: 2.897223949432373\n",
            "Epoch: 2, Training Loss: 2.5644896030426025\n",
            "Epoch: 2, Training Loss: 2.636992931365967\n",
            "Epoch: 2, Training Loss: 2.5442686080932617\n",
            "Epoch: 2, Training Loss: 2.676711320877075\n",
            "Epoch: 2, Training Loss: 2.7092249393463135\n",
            "Epoch: 2, Training Loss: 3.1539039611816406\n",
            "Epoch: 2, Training Loss: 2.8562862873077393\n",
            "Epoch: 2, Training Loss: 2.9986026287078857\n",
            "Epoch: 2, Training Loss: 3.011924982070923\n",
            "Epoch: 2, Training Loss: 2.8210387229919434\n",
            "Epoch: 2, Training Loss: 2.507282257080078\n",
            "Epoch: 2, Training Loss: 2.7982242107391357\n",
            "Epoch: 2, Training Loss: 2.883490562438965\n",
            "Epoch: 2, Training Loss: 2.6979920864105225\n",
            "Epoch: 2, Training Loss: 2.715782403945923\n",
            "Epoch: 2, Training Loss: 2.845896005630493\n",
            "Epoch: 2, Training Loss: 2.9192638397216797\n",
            "Epoch: 2, Training Loss: 2.8467257022857666\n",
            "Epoch: 2, Training Loss: 2.7352423667907715\n",
            "Epoch: 2, Training Loss: 2.5953497886657715\n",
            "Epoch: 2, Training Loss: 2.870382785797119\n",
            "Epoch: 2, Training Loss: 2.7947728633880615\n",
            "Epoch: 2, Training Loss: 2.562467098236084\n",
            "Epoch: 2, Training Loss: 2.71158504486084\n",
            "Epoch: 2, Training Loss: 2.5459117889404297\n",
            "Epoch: 2, Training Loss: 2.9417169094085693\n",
            "Epoch: 2, Training Loss: 2.6736629009246826\n",
            "Epoch: 2, Training Loss: 2.8828470706939697\n",
            "Epoch: 2, Training Loss: 2.7781074047088623\n",
            "Epoch: 2, Training Loss: 2.808651924133301\n",
            "Epoch: 2, Training Loss: 2.73134446144104\n",
            "Epoch: 2, Training Loss: 2.892390251159668\n",
            "Epoch: 2, Training Loss: 2.8388545513153076\n",
            "Epoch: 2, Training Loss: 3.0348753929138184\n",
            "Epoch: 2, Training Loss: 2.8565213680267334\n",
            "Epoch: 2, Training Loss: 2.8772807121276855\n",
            "Epoch: 2, Training Loss: 2.578843355178833\n",
            "Epoch: 2, Training Loss: 2.873922348022461\n",
            "Epoch: 2, Training Loss: 2.53267502784729\n",
            "Epoch: 2, Training Loss: 2.923602819442749\n",
            "Epoch: 2, Training Loss: 3.008955240249634\n",
            "Epoch: 2, Training Loss: 2.6818225383758545\n",
            "Epoch: 2, Training Loss: 2.674501419067383\n",
            "Epoch: 2, Training Loss: 2.7290422916412354\n",
            "Epoch: 2, Training Loss: 2.782146453857422\n",
            "Epoch: 2, Training Loss: 2.513211727142334\n",
            "Epoch: 2, Training Loss: 2.860450267791748\n",
            "Epoch: 2, Training Loss: 2.999751329421997\n",
            "Epoch: 2, Training Loss: 2.8638381958007812\n",
            "Epoch: 2, Training Loss: 2.9196040630340576\n",
            "Epoch: 2, Training Loss: 2.8582842350006104\n",
            "Epoch: 2, Training Loss: 2.4976086616516113\n",
            "Epoch: 2, Training Loss: 2.900153160095215\n",
            "Epoch: 2, Training Loss: 3.0589888095855713\n",
            "Epoch: 2, Training Loss: 2.7009644508361816\n",
            "Epoch: 2, Training Loss: 2.572502851486206\n",
            "Epoch: 2, Training Loss: 2.7982497215270996\n",
            "Epoch: 2, Training Loss: 3.067073106765747\n",
            "Epoch: 2, Training Loss: 2.8883473873138428\n",
            "Epoch: 2, Training Loss: 2.660714864730835\n",
            "Epoch: 2, Training Loss: 2.846372365951538\n",
            "Epoch: 2, Training Loss: 2.596804141998291\n",
            "Epoch: 2, Training Loss: 2.500309705734253\n",
            "Epoch: 2, Training Loss: 2.6895432472229004\n",
            "Epoch: 2, Training Loss: 2.673919677734375\n",
            "Epoch: 2, Training Loss: 2.665121555328369\n",
            "Epoch: 2, Training Loss: 2.6787612438201904\n",
            "Epoch: 2, Training Loss: 2.990382194519043\n",
            "Epoch: 2, Training Loss: 2.6424641609191895\n",
            "Epoch: 2, Training Loss: 2.6085238456726074\n",
            "Epoch: 2, Training Loss: 2.9131643772125244\n",
            "Epoch: 2, Training Loss: 3.082454204559326\n",
            "Epoch: 2, Training Loss: 2.875988006591797\n",
            "Epoch: 2, Training Loss: 2.642998456954956\n",
            "Epoch: 2, Training Loss: 2.6447694301605225\n",
            "Epoch: 2, Training Loss: 2.743597984313965\n",
            "Epoch: 2, Training Loss: 2.8922579288482666\n",
            "Epoch: 2, Training Loss: 2.7137396335601807\n",
            "Epoch: 2, Training Loss: 3.093499183654785\n",
            "Epoch: 2, Training Loss: 2.8578274250030518\n",
            "Epoch: 2, Training Loss: 2.694655656814575\n",
            "Epoch: 2, Training Loss: 2.6585395336151123\n",
            "Epoch: 2, Training Loss: 2.6193501949310303\n",
            "Epoch: 2, Training Loss: 2.860226631164551\n",
            "Epoch: 2, Training Loss: 2.7870612144470215\n",
            "Epoch: 2, Training Loss: 2.7778940200805664\n",
            "Epoch: 2, Training Loss: 2.5548341274261475\n",
            "Epoch: 2, Training Loss: 2.805898904800415\n",
            "Epoch: 2, Training Loss: 2.660876750946045\n",
            "Epoch: 2, Training Loss: 2.704979658126831\n",
            "Epoch: 2, Training Loss: 2.830359697341919\n",
            "Epoch: 2, Training Loss: 2.8271100521087646\n",
            "Epoch: 2, Training Loss: 2.850780487060547\n",
            "Epoch: 2, Training Loss: 2.687041759490967\n",
            "Epoch: 2, Training Loss: 2.80769419670105\n",
            "Epoch: 2, Training Loss: 2.905994176864624\n",
            "Epoch: 2, Training Loss: 2.8556056022644043\n",
            "Epoch: 2, Training Loss: 2.794145107269287\n",
            "Epoch: 2, Training Loss: 2.6969094276428223\n",
            "Epoch: 2, Training Loss: 2.7790207862854004\n",
            "Epoch: 2, Training Loss: 2.510758638381958\n",
            "Epoch: 2, Training Loss: 2.6531834602355957\n",
            "Epoch: 2, Training Loss: 2.740713357925415\n",
            "Epoch: 2, Training Loss: 2.7217962741851807\n",
            "Epoch: 2, Training Loss: 2.758132219314575\n",
            "Epoch: 2, Training Loss: 2.9234488010406494\n",
            "Epoch: 2, Training Loss: 2.7475943565368652\n",
            "Epoch: 2, Training Loss: 2.8400633335113525\n",
            "Epoch: 2, Training Loss: 2.9030747413635254\n",
            "Epoch: 2, Training Loss: 2.6043615341186523\n",
            "Epoch: 2, Training Loss: 2.981227159500122\n",
            "Epoch: 2, Training Loss: 2.664520740509033\n",
            "Epoch: 2, Training Loss: 3.0716657638549805\n",
            "Epoch: 2, Training Loss: 2.726604700088501\n",
            "Epoch: 2, Training Loss: 2.4968175888061523\n",
            "Epoch: 2, Training Loss: 2.6127076148986816\n",
            "Epoch: 2, Training Loss: 2.6543829441070557\n",
            "Epoch: 2, Training Loss: 2.5959866046905518\n",
            "Epoch: 2, Training Loss: 2.693892240524292\n",
            "Epoch: 2, Training Loss: 2.612865924835205\n",
            "Epoch: 2, Training Loss: 2.509159564971924\n",
            "Epoch: 2, Training Loss: 2.7499630451202393\n",
            "Epoch: 2, Training Loss: 2.958697557449341\n",
            "Epoch: 2, Training Loss: 2.679272413253784\n",
            "Epoch: 2, Training Loss: 2.4604897499084473\n",
            "Epoch: 2, Training Loss: 2.574445962905884\n",
            "Epoch: 2, Training Loss: 2.703190565109253\n",
            "Epoch: 2, Training Loss: 2.9805748462677\n",
            "Epoch: 2, Training Loss: 2.6152186393737793\n",
            "Epoch: 2, Training Loss: 2.4148056507110596\n",
            "Epoch: 2, Training Loss: 3.2279961109161377\n",
            "Epoch: 2, Training Loss: 2.7980775833129883\n",
            "Epoch: 2, Training Loss: 3.0304386615753174\n",
            "Epoch: 2, Training Loss: 2.861274242401123\n",
            "Epoch: 2, Training Loss: 2.5076913833618164\n",
            "Epoch: 2, Training Loss: 2.8199236392974854\n",
            "Epoch: 2, Training Loss: 2.6752421855926514\n",
            "Epoch: 2, Training Loss: 2.9997475147247314\n",
            "Epoch: 2, Training Loss: 3.0638515949249268\n",
            "Epoch: 2, Training Loss: 2.6603622436523438\n",
            "Epoch: 2, Training Loss: 2.9255387783050537\n",
            "Epoch: 2, Training Loss: 2.7777018547058105\n",
            "Epoch: 2, Training Loss: 2.789370059967041\n",
            "Epoch: 2, Training Loss: 2.5281479358673096\n",
            "Epoch: 2, Training Loss: 2.8183705806732178\n",
            "Epoch: 2, Training Loss: 2.7337350845336914\n",
            "Epoch: 2, Training Loss: 2.4349565505981445\n",
            "Epoch: 2, Training Loss: 2.5727670192718506\n",
            "Epoch: 2, Training Loss: 2.675590753555298\n",
            "Epoch: 2, Training Loss: 3.031426429748535\n",
            "Epoch: 2, Training Loss: 2.9381890296936035\n",
            "Epoch: 2, Training Loss: 2.8455066680908203\n",
            "Epoch: 2, Training Loss: 2.573503255844116\n",
            "Epoch: 2, Training Loss: 2.501070261001587\n",
            "Epoch: 2, Training Loss: 2.803232192993164\n",
            "Epoch: 2, Training Loss: 3.039109230041504\n",
            "Epoch: 2, Training Loss: 2.751798152923584\n",
            "Epoch: 2, Training Loss: 2.8261680603027344\n",
            "Epoch: 2, Training Loss: 2.959479331970215\n",
            "Epoch: 2, Training Loss: 2.9471194744110107\n",
            "Epoch: 2, Training Loss: 2.641252279281616\n",
            "Epoch: 2, Training Loss: 2.693289041519165\n",
            "Epoch: 2, Training Loss: 2.8406856060028076\n",
            "Epoch: 2, Training Loss: 2.718717575073242\n",
            "Epoch: 2, Training Loss: 2.660554885864258\n",
            "Epoch: 2, Training Loss: 2.5015642642974854\n",
            "Epoch: 2, Training Loss: 2.6830332279205322\n",
            "Epoch: 2, Training Loss: 2.832447052001953\n",
            "Epoch: 2, Training Loss: 2.555880308151245\n",
            "Epoch: 2, Training Loss: 2.7947826385498047\n",
            "Epoch: 2, Training Loss: 2.681861400604248\n",
            "Epoch: 2, Training Loss: 2.827592134475708\n",
            "Epoch: 2, Training Loss: 2.54659366607666\n",
            "Epoch: 2, Training Loss: 2.833000421524048\n",
            "Epoch: 2, Training Loss: 2.6725049018859863\n",
            "Epoch: 2, Training Loss: 2.870933771133423\n",
            "Epoch: 2, Training Loss: 2.5154311656951904\n",
            "Epoch: 2, Training Loss: 2.712806224822998\n",
            "Epoch: 2, Training Loss: 2.88558292388916\n",
            "Epoch: 2, Training Loss: 2.802126884460449\n",
            "Epoch: 2, Training Loss: 2.8971786499023438\n",
            "Epoch: 2, Training Loss: 2.412405252456665\n",
            "Epoch: 2, Training Loss: 2.840595006942749\n",
            "Epoch: 2, Training Loss: 2.9444291591644287\n",
            "Epoch: 2, Training Loss: 2.795222520828247\n",
            "Epoch: 2, Training Loss: 2.7276713848114014\n",
            "Epoch: 2, Training Loss: 2.68930721282959\n",
            "Epoch: 2, Training Loss: 2.4184577465057373\n",
            "Epoch: 2, Training Loss: 2.7762060165405273\n",
            "Epoch: 2, Training Loss: 2.7388715744018555\n",
            "Epoch: 2, Training Loss: 2.9692935943603516\n",
            "Epoch: 2, Training Loss: 3.1382815837860107\n",
            "Epoch: 2, Training Loss: 2.7185754776000977\n",
            "Epoch: 2, Training Loss: 2.6524922847747803\n",
            "Epoch: 2, Training Loss: 3.0842621326446533\n",
            "Epoch: 2, Training Loss: 2.6681172847747803\n",
            "Epoch: 2, Training Loss: 2.8604159355163574\n",
            "Epoch: 2, Training Loss: 2.65588116645813\n",
            "Epoch: 2, Training Loss: 2.801102876663208\n",
            "Epoch: 2, Training Loss: 2.8494904041290283\n",
            "Epoch: 2, Training Loss: 2.624812602996826\n",
            "Epoch: 2, Training Loss: 2.772352695465088\n",
            "Epoch: 2, Training Loss: 2.5319292545318604\n",
            "Epoch: 2, Training Loss: 2.5263004302978516\n",
            "Epoch: 2, Training Loss: 2.558659553527832\n",
            "Epoch: 2, Training Loss: 2.713907241821289\n",
            "Epoch: 2, Training Loss: 2.8503518104553223\n",
            "Epoch: 2, Training Loss: 2.4245123863220215\n",
            "Epoch: 2, Training Loss: 2.5306501388549805\n",
            "Epoch: 2, Training Loss: 2.5785958766937256\n",
            "Epoch: 2, Training Loss: 2.5223047733306885\n",
            "Epoch: 2, Training Loss: 2.600567579269409\n",
            "Epoch: 2, Training Loss: 2.7073323726654053\n",
            "Epoch: 2, Training Loss: 2.6081271171569824\n",
            "Epoch: 2, Training Loss: 2.8713855743408203\n",
            "Epoch: 2, Training Loss: 2.7616262435913086\n",
            "Epoch: 2, Training Loss: 2.717949867248535\n",
            "Epoch: 2, Training Loss: 2.638157367706299\n",
            "Epoch: 2, Training Loss: 2.9558639526367188\n",
            "Epoch: 2, Training Loss: 2.8561198711395264\n",
            "Epoch: 2, Training Loss: 2.628997325897217\n",
            "Epoch: 2, Training Loss: 2.6383135318756104\n",
            "Epoch: 2, Training Loss: 2.6359620094299316\n",
            "Epoch: 2, Training Loss: 2.637228488922119\n",
            "Epoch: 2, Training Loss: 2.6603872776031494\n",
            "Epoch: 2, Training Loss: 2.5344550609588623\n",
            "Epoch: 2, Training Loss: 2.721651792526245\n",
            "Epoch: 2, Training Loss: 2.682905673980713\n",
            "Epoch: 2, Training Loss: 2.721815824508667\n",
            "Epoch: 2, Training Loss: 2.800926685333252\n",
            "Epoch: 2, Training Loss: 2.5524816513061523\n",
            "Epoch: 2, Training Loss: 2.6809375286102295\n",
            "Epoch: 2, Training Loss: 2.621255397796631\n",
            "Epoch: 2, Training Loss: 2.7723615169525146\n",
            "Epoch: 2, Training Loss: 2.772273540496826\n",
            "Epoch: 2, Training Loss: 2.7646610736846924\n",
            "Epoch: 2, Training Loss: 2.839228630065918\n",
            "Epoch: 2, Training Loss: 2.4849653244018555\n",
            "Epoch: 2, Training Loss: 2.4888229370117188\n",
            "Epoch: 2, Training Loss: 2.8305957317352295\n",
            "Epoch: 2, Training Loss: 2.856290578842163\n",
            "Epoch: 2, Training Loss: 2.556903600692749\n",
            "Epoch: 2, Training Loss: 2.7877697944641113\n",
            "Epoch: 2, Training Loss: 2.9366326332092285\n",
            "Epoch: 2, Training Loss: 2.5963516235351562\n",
            "Epoch: 2, Training Loss: 2.6630351543426514\n",
            "Epoch: 2, Training Loss: 2.7901997566223145\n",
            "Epoch: 2, Training Loss: 2.9624135494232178\n",
            "Epoch: 2, Training Loss: 2.7048654556274414\n",
            "Epoch: 2, Training Loss: 2.9743473529815674\n",
            "Epoch: 2, Training Loss: 2.672419548034668\n",
            "Epoch: 2, Training Loss: 2.6534390449523926\n",
            "Epoch: 2, Training Loss: 2.95501708984375\n",
            "Epoch: 2, Training Loss: 2.883981704711914\n",
            "Epoch: 2, Training Loss: 2.7347209453582764\n",
            "Epoch: 2, Training Loss: 2.617475986480713\n",
            "Epoch: 2, Training Loss: 2.8715016841888428\n",
            "Epoch: 2, Training Loss: 2.586660385131836\n",
            "Epoch: 2, Training Loss: 2.904824733734131\n",
            "Epoch: 2, Training Loss: 2.641697883605957\n",
            "Epoch: 2, Training Loss: 2.925422430038452\n",
            "Epoch: 2, Training Loss: 3.0347630977630615\n",
            "Epoch: 2, Training Loss: 2.7245829105377197\n",
            "Epoch: 2, Training Loss: 2.399357557296753\n",
            "Epoch: 2, Training Loss: 2.5656487941741943\n",
            "Epoch: 2, Training Loss: 2.794647216796875\n",
            "Epoch: 2, Training Loss: 3.0475425720214844\n",
            "Epoch: 2, Training Loss: 2.9280896186828613\n",
            "Epoch: 2, Training Loss: 2.5576515197753906\n",
            "Epoch: 2, Training Loss: 2.770226001739502\n",
            "Epoch: 2, Training Loss: 2.8585360050201416\n",
            "Epoch: 2, Training Loss: 2.774277687072754\n",
            "Epoch: 2, Training Loss: 2.682248830795288\n",
            "Epoch: 2, Training Loss: 2.4405789375305176\n",
            "Epoch: 2, Training Loss: 2.61262583732605\n",
            "Epoch: 2, Training Loss: 2.7357089519500732\n",
            "Epoch: 2, Training Loss: 2.7151622772216797\n",
            "Epoch: 2, Training Loss: 2.671868085861206\n",
            "Epoch: 2, Training Loss: 2.6991539001464844\n",
            "Epoch: 2, Training Loss: 2.7159857749938965\n",
            "Epoch: 2, Training Loss: 2.81916880607605\n",
            "Epoch: 2, Training Loss: 2.6485354900360107\n",
            "Epoch: 2, Training Loss: 2.6452159881591797\n",
            "Epoch: 2, Training Loss: 2.4959523677825928\n",
            "Epoch: 2, Training Loss: 2.662639856338501\n",
            "Epoch: 2, Training Loss: 2.683950424194336\n",
            "Epoch: 2, Training Loss: 2.8047099113464355\n",
            "Epoch: 2, Training Loss: 2.618023157119751\n",
            "Epoch: 2, Training Loss: 2.734750747680664\n",
            "Epoch: 2, Training Loss: 2.680434226989746\n",
            "Epoch: 2, Training Loss: 2.797394275665283\n",
            "Epoch: 2, Training Loss: 2.7746713161468506\n",
            "Epoch: 2, Training Loss: 2.4961862564086914\n",
            "Epoch: 2, Training Loss: 2.5304770469665527\n",
            "Epoch: 2, Training Loss: 2.4671220779418945\n",
            "Epoch: 2, Training Loss: 2.669292688369751\n",
            "Epoch: 2, Training Loss: 2.91884446144104\n",
            "Epoch: 2, Training Loss: 2.8512206077575684\n",
            "Epoch: 2, Training Loss: 2.8440866470336914\n",
            "Epoch: 2, Training Loss: 2.8164865970611572\n",
            "Epoch: 2, Training Loss: 2.5327699184417725\n",
            "Epoch: 2, Training Loss: 3.1913201808929443\n",
            "Epoch: 2, Training Loss: 2.652907133102417\n",
            "Epoch: 2, Training Loss: 2.9094626903533936\n",
            "Epoch: 2, Training Loss: 2.788564920425415\n",
            "Epoch: 2, Training Loss: 2.895644187927246\n",
            "Epoch: 2, Training Loss: 2.648981809616089\n",
            "Epoch: 2, Training Loss: 2.8278040885925293\n",
            "Epoch: 2, Training Loss: 2.719197988510132\n",
            "Epoch: 2, Training Loss: 2.638986110687256\n",
            "Epoch: 2, Training Loss: 2.562067985534668\n",
            "Epoch: 2, Training Loss: 2.7796249389648438\n",
            "Epoch: 2, Training Loss: 2.653860569000244\n",
            "Epoch: 2, Training Loss: 2.7613751888275146\n",
            "Epoch: 2, Training Loss: 2.843156099319458\n",
            "Epoch: 2, Training Loss: 2.6826999187469482\n",
            "Epoch: 2, Training Loss: 2.768296480178833\n",
            "Epoch: 2, Training Loss: 2.8081812858581543\n",
            "Epoch: 2, Training Loss: 2.7632415294647217\n",
            "Epoch: 2, Training Loss: 2.637775421142578\n",
            "Epoch: 2, Training Loss: 2.842686653137207\n",
            "Epoch: 2, Training Loss: 2.7554879188537598\n",
            "Epoch: 2, Training Loss: 2.5219876766204834\n",
            "Epoch: 2, Training Loss: 2.4914047718048096\n",
            "Epoch: 2, Training Loss: 2.332902193069458\n",
            "Epoch: 2, Training Loss: 2.528456211090088\n",
            "Epoch: 2, Training Loss: 2.6802995204925537\n",
            "Epoch: 2, Training Loss: 2.6895134449005127\n",
            "Epoch: 2, Training Loss: 2.9790029525756836\n",
            "Epoch: 2, Training Loss: 2.7438418865203857\n",
            "Epoch: 2, Training Loss: 2.9152109622955322\n",
            "Epoch: 2, Training Loss: 2.726895570755005\n",
            "Epoch: 2, Training Loss: 2.830382823944092\n",
            "Epoch: 2, Training Loss: 2.5997979640960693\n",
            "Epoch: 2, Training Loss: 2.8252439498901367\n",
            "Epoch: 2, Training Loss: 2.9042863845825195\n",
            "Epoch: 2, Training Loss: 2.5462193489074707\n",
            "Epoch: 2, Training Loss: 2.6056535243988037\n",
            "Epoch: 2, Training Loss: 2.8981926441192627\n",
            "Epoch: 2, Training Loss: 2.7719404697418213\n",
            "Epoch: 2, Training Loss: 2.3741097450256348\n",
            "Epoch: 2, Training Loss: 2.6813037395477295\n",
            "Epoch: 2, Training Loss: 2.587127923965454\n",
            "Epoch: 2, Training Loss: 2.712946891784668\n",
            "Epoch: 2, Training Loss: 2.806229829788208\n",
            "Epoch: 2, Training Loss: 2.834897994995117\n",
            "Epoch: 2, Training Loss: 2.762486457824707\n",
            "Epoch: 2, Training Loss: 2.833009719848633\n",
            "Epoch: 2, Training Loss: 2.832646369934082\n",
            "Epoch: 2, Training Loss: 2.410205841064453\n",
            "Epoch: 2, Training Loss: 2.6911511421203613\n",
            "Epoch: 2, Training Loss: 2.5866456031799316\n",
            "Epoch: 2, Training Loss: 2.625854253768921\n",
            "Epoch: 2, Training Loss: 2.8314709663391113\n",
            "Epoch: 2, Training Loss: 2.761263847351074\n",
            "Epoch: 2, Training Loss: 2.760476589202881\n",
            "Epoch: 2, Training Loss: 2.727677583694458\n",
            "Epoch: 2, Training Loss: 2.8995959758758545\n",
            "Epoch: 2, Training Loss: 2.802889108657837\n",
            "Epoch: 2, Training Loss: 2.5970005989074707\n",
            "Epoch: 2, Training Loss: 2.742450714111328\n",
            "Epoch: 2, Training Loss: 2.581308603286743\n",
            "Epoch: 2, Training Loss: 3.0972468852996826\n",
            "Epoch: 2, Training Loss: 2.8745615482330322\n",
            "Epoch: 2, Training Loss: 2.7659084796905518\n",
            "Epoch: 2, Training Loss: 2.970107316970825\n",
            "Epoch: 2, Training Loss: 2.8104171752929688\n",
            "Epoch: 2, Training Loss: 2.7895689010620117\n",
            "Epoch: 2, Training Loss: 2.468689203262329\n",
            "Epoch: 2, Training Loss: 3.0013203620910645\n",
            "Epoch: 2, Training Loss: 2.3986544609069824\n",
            "Epoch: 2, Training Loss: 2.62256121635437\n",
            "Epoch: 2, Training Loss: 2.7433927059173584\n",
            "Epoch: 2, Training Loss: 2.789470672607422\n",
            "Epoch: 2, Training Loss: 3.0018348693847656\n",
            "Epoch: 2, Training Loss: 2.6493747234344482\n",
            "Epoch: 2, Training Loss: 2.769620418548584\n",
            "Epoch: 2, Training Loss: 2.824226140975952\n",
            "Epoch: 2, Training Loss: 2.7473373413085938\n",
            "Epoch: 2, Training Loss: 2.6889142990112305\n",
            "Epoch: 2, Training Loss: 2.58771014213562\n",
            "Epoch: 2, Training Loss: 2.7443416118621826\n",
            "Epoch: 2, Training Loss: 2.9014945030212402\n",
            "Epoch: 2, Training Loss: 2.855125904083252\n",
            "Epoch: 2, Training Loss: 2.5357909202575684\n",
            "Epoch: 2, Training Loss: 2.7780942916870117\n",
            "Epoch: 2, Training Loss: 2.730219602584839\n",
            "Epoch: 2, Training Loss: 2.5783278942108154\n",
            "Epoch: 2, Training Loss: 2.653686285018921\n",
            "Epoch: 2, Training Loss: 2.708850860595703\n",
            "Epoch: 2, Training Loss: 2.676576614379883\n",
            "Epoch: 2, Training Loss: 2.5594284534454346\n",
            "Epoch: 2, Training Loss: 2.5022130012512207\n",
            "Epoch: 2, Training Loss: 2.82663893699646\n",
            "Epoch: 2, Training Loss: 2.7488348484039307\n",
            "Epoch: 2, Training Loss: 2.589244842529297\n",
            "Epoch: 2, Training Loss: 2.601090908050537\n",
            "Epoch: 2, Training Loss: 2.867352247238159\n",
            "Epoch: 2, Training Loss: 2.754824161529541\n",
            "Epoch: 2, Training Loss: 2.736823320388794\n",
            "Epoch: 2, Training Loss: 2.558222532272339\n",
            "Epoch: 2, Training Loss: 2.8836772441864014\n",
            "Epoch: 2, Training Loss: 2.7032299041748047\n",
            "Epoch: 2, Training Loss: 3.049684524536133\n",
            "Epoch: 2, Training Loss: 2.6029109954833984\n",
            "Epoch: 2, Training Loss: 2.8972742557525635\n",
            "Epoch: 2, Training Loss: 2.898275375366211\n",
            "Epoch: 2, Training Loss: 2.5280232429504395\n",
            "Epoch: 2, Training Loss: 2.6702606678009033\n",
            "Epoch: 2, Training Loss: 2.624737024307251\n",
            "Epoch: 2, Training Loss: 3.0022809505462646\n",
            "Epoch: 2, Training Loss: 2.5941901206970215\n",
            "Epoch: 2, Training Loss: 2.67533278465271\n",
            "Epoch: 2, Training Loss: 2.831533193588257\n",
            "Epoch: 2, Training Loss: 2.8209152221679688\n",
            "Epoch: 2, Training Loss: 2.790726661682129\n",
            "Epoch: 2, Training Loss: 2.540391445159912\n",
            "Epoch: 2, Training Loss: 2.513545274734497\n",
            "Epoch: 2, Training Loss: 2.7337422370910645\n",
            "Epoch: 2, Training Loss: 2.3463473320007324\n",
            "Epoch: 2, Training Loss: 2.9168734550476074\n",
            "Epoch: 2, Training Loss: 2.826840877532959\n",
            "Epoch: 2, Training Loss: 2.5625250339508057\n",
            "Epoch: 2, Training Loss: 2.617732286453247\n",
            "Epoch: 2, Training Loss: 2.4558451175689697\n",
            "Epoch: 2, Training Loss: 2.5739338397979736\n",
            "Epoch: 2, Training Loss: 2.63606595993042\n",
            "Epoch: 2, Training Loss: 2.6140010356903076\n",
            "Epoch: 2, Training Loss: 2.9012091159820557\n",
            "Epoch: 2, Training Loss: 2.7799782752990723\n",
            "Epoch: 2, Training Loss: 2.53580641746521\n",
            "Epoch: 2, Training Loss: 2.6350889205932617\n",
            "Epoch: 2, Training Loss: 2.657518148422241\n",
            "Epoch: 2, Training Loss: 2.6768856048583984\n",
            "Epoch: 2, Training Loss: 2.75232195854187\n",
            "Epoch: 2, Training Loss: 2.8075692653656006\n",
            "Epoch: 2, Training Loss: 2.4176158905029297\n",
            "Epoch: 2, Training Loss: 2.6725597381591797\n",
            "Epoch: 2, Training Loss: 2.7108731269836426\n",
            "Epoch: 2, Training Loss: 2.900683641433716\n",
            "Epoch: 2, Training Loss: 2.4679946899414062\n",
            "Epoch: 2, Training Loss: 2.8989181518554688\n",
            "Epoch: 2, Training Loss: 2.7996270656585693\n",
            "Epoch: 2, Training Loss: 2.7780680656433105\n",
            "Epoch: 2, Training Loss: 2.954643726348877\n",
            "Epoch: 2, Training Loss: 3.0179038047790527\n",
            "Epoch: 2, Training Loss: 2.6465442180633545\n",
            "Epoch: 2, Training Loss: 3.0407958030700684\n",
            "Epoch: 2, Training Loss: 2.70678973197937\n",
            "Epoch: 2, Training Loss: 3.005823850631714\n",
            "Epoch: 2, Training Loss: 2.7188034057617188\n",
            "Epoch: 2, Training Loss: 2.6539359092712402\n",
            "Epoch: 2, Training Loss: 2.6873955726623535\n",
            "Epoch: 2, Training Loss: 2.7831926345825195\n",
            "Epoch: 2, Training Loss: 2.77836275100708\n",
            "Epoch: 2, Training Loss: 2.5662853717803955\n",
            "Epoch: 2, Training Loss: 2.6793172359466553\n",
            "Epoch: 2, Training Loss: 2.5515003204345703\n",
            "Epoch: 2, Training Loss: 2.6685972213745117\n",
            "Epoch: 2, Training Loss: 2.6755287647247314\n",
            "Epoch: 2, Training Loss: 2.6643197536468506\n",
            "Epoch: 2, Training Loss: 2.7183468341827393\n",
            "Epoch: 2, Training Loss: 2.766324758529663\n",
            "Epoch: 2, Training Loss: 2.653395652770996\n",
            "Epoch: 2, Training Loss: 2.743588447570801\n",
            "Epoch: 2, Training Loss: 2.723487138748169\n",
            "Epoch: 2, Training Loss: 2.5347299575805664\n",
            "Epoch: 2, Training Loss: 2.595949172973633\n",
            "Epoch: 2, Training Loss: 3.0900614261627197\n",
            "Epoch: 2, Training Loss: 2.7872493267059326\n",
            "Epoch: 2, Training Loss: 2.90124249458313\n",
            "Epoch: 2, Training Loss: 2.7885687351226807\n",
            "Epoch: 2, Training Loss: 2.8452224731445312\n",
            "Epoch: 2, Training Loss: 2.4246127605438232\n",
            "Epoch: 2, Training Loss: 2.5516488552093506\n",
            "Epoch: 2, Training Loss: 2.551128387451172\n",
            "Epoch: 2, Training Loss: 2.2066550254821777\n",
            "Epoch: 2, Training Loss: 2.677676200866699\n",
            "Epoch: 2, Training Loss: 2.5058279037475586\n",
            "Epoch: 2, Training Loss: 2.6802096366882324\n",
            "Epoch: 2, Training Loss: 2.592880964279175\n",
            "Epoch: 2, Training Loss: 2.5674781799316406\n",
            "Epoch: 2, Training Loss: 2.959087371826172\n",
            "Epoch: 2, Training Loss: 2.9016952514648438\n",
            "Epoch: 2, Training Loss: 2.6596784591674805\n",
            "Epoch: 2, Training Loss: 2.6657252311706543\n",
            "Epoch: 2, Training Loss: 2.526280164718628\n",
            "Epoch: 2, Training Loss: 2.6448497772216797\n",
            "Epoch: 2, Training Loss: 2.5048553943634033\n",
            "Epoch: 2, Training Loss: 2.7716054916381836\n",
            "Epoch: 2, Training Loss: 2.5391347408294678\n",
            "Epoch: 2, Training Loss: 2.571580410003662\n",
            "Epoch: 2, Training Loss: 2.6817407608032227\n",
            "Epoch: 2, Training Loss: 2.597172975540161\n",
            "Epoch: 2, Training Loss: 2.6669859886169434\n",
            "Epoch: 2, Training Loss: 2.7258529663085938\n",
            "Epoch: 2, Training Loss: 2.7507340908050537\n",
            "Epoch: 2, Training Loss: 2.8339028358459473\n",
            "Epoch: 2, Training Loss: 2.4111320972442627\n",
            "Epoch: 2, Training Loss: 3.0338897705078125\n",
            "Epoch: 2, Training Loss: 2.5068023204803467\n",
            "Epoch: 2, Training Loss: 2.71911883354187\n",
            "Epoch: 2, Training Loss: 2.605677843093872\n",
            "Epoch: 2, Training Loss: 2.417353868484497\n",
            "Epoch: 2, Training Loss: 2.6568422317504883\n",
            "Epoch: 2, Training Loss: 2.8138818740844727\n",
            "Epoch: 2, Training Loss: 2.5516114234924316\n",
            "Epoch: 2, Training Loss: 2.402998447418213\n",
            "Epoch: 2, Training Loss: 2.5367989540100098\n",
            "Epoch: 2, Training Loss: 2.379035711288452\n",
            "Epoch: 2, Training Loss: 2.73225998878479\n",
            "Epoch: 2, Training Loss: 2.698150634765625\n",
            "Epoch: 2, Training Loss: 2.5343027114868164\n",
            "Epoch: 2, Training Loss: 2.5959725379943848\n",
            "Epoch: 2, Training Loss: 2.534163475036621\n",
            "Epoch: 2, Training Loss: 2.6160666942596436\n",
            "Epoch: 2, Training Loss: 2.571439743041992\n",
            "Epoch: 2, Training Loss: 2.484353542327881\n",
            "Epoch: 2, Training Loss: 2.7761521339416504\n",
            "Epoch: 2, Training Loss: 2.691490650177002\n",
            "Epoch: 2, Training Loss: 2.8270370960235596\n",
            "Epoch: 2, Training Loss: 2.4097087383270264\n",
            "Epoch: 2, Training Loss: 2.8293399810791016\n",
            "Epoch: 2, Training Loss: 2.415465831756592\n",
            "Epoch: 2, Training Loss: 2.5742828845977783\n",
            "Epoch: 2, Training Loss: 2.9306864738464355\n",
            "Epoch: 2, Training Loss: 2.817840814590454\n",
            "Epoch: 2, Training Loss: 2.662694215774536\n",
            "Epoch: 2, Training Loss: 2.8358330726623535\n",
            "Epoch: 2, Training Loss: 2.754530906677246\n",
            "Epoch: 2, Training Loss: 2.544877052307129\n",
            "Epoch: 2, Training Loss: 2.8058910369873047\n",
            "Epoch: 2, Training Loss: 2.64249849319458\n",
            "Epoch: 2, Training Loss: 2.673522710800171\n",
            "Epoch: 2, Training Loss: 2.4705686569213867\n",
            "Epoch: 2, Training Loss: 2.862650156021118\n",
            "Epoch: 2, Training Loss: 2.465763568878174\n",
            "Epoch: 2, Training Loss: 2.584141731262207\n",
            "Epoch: 2, Training Loss: 2.5818064212799072\n",
            "Epoch: 2, Training Loss: 2.4683823585510254\n",
            "Epoch: 2, Training Loss: 2.719283103942871\n",
            "Epoch: 2, Training Loss: 2.4401705265045166\n",
            "Epoch: 2, Training Loss: 2.6097381114959717\n",
            "Epoch: 2, Training Loss: 2.5831246376037598\n",
            "Epoch: 2, Training Loss: 2.5940661430358887\n",
            "Epoch: 2, Training Loss: 2.2928566932678223\n",
            "Epoch: 2, Training Loss: 2.5344862937927246\n",
            "Epoch: 2, Training Loss: 2.6887402534484863\n",
            "Epoch: 2, Training Loss: 2.6348822116851807\n",
            "Epoch: 2, Training Loss: 2.9713661670684814\n",
            "Epoch: 2, Training Loss: 2.6123337745666504\n",
            "Epoch: 2, Training Loss: 2.8810110092163086\n",
            "Epoch: 2, Training Loss: 2.5790340900421143\n",
            "Epoch: 2, Training Loss: 2.634140729904175\n",
            "Epoch: 2, Training Loss: 2.7749438285827637\n",
            "Epoch: 2, Training Loss: 2.9919395446777344\n",
            "Epoch: 2, Training Loss: 2.7961738109588623\n",
            "Epoch: 2, Training Loss: 2.5192859172821045\n",
            "Epoch: 2, Training Loss: 2.551046133041382\n",
            "Epoch: 2, Training Loss: 2.6197168827056885\n",
            "Epoch: 2, Training Loss: 2.525186777114868\n",
            "Epoch: 2, Training Loss: 2.6670002937316895\n",
            "Epoch: 2, Training Loss: 2.354013681411743\n",
            "Epoch: 2, Training Loss: 2.619926691055298\n",
            "Epoch: 2, Training Loss: 2.4504199028015137\n",
            "Epoch: 2, Training Loss: 2.5576934814453125\n",
            "Epoch: 2, Training Loss: 2.4927616119384766\n",
            "Epoch: 2, Training Loss: 2.786226511001587\n",
            "Epoch: 2, Training Loss: 2.6577022075653076\n",
            "Epoch: 2, Training Loss: 2.65078067779541\n",
            "Epoch: 2, Training Loss: 2.745431423187256\n",
            "Epoch: 2, Training Loss: 2.4888954162597656\n",
            "Epoch: 2, Training Loss: 2.7598984241485596\n",
            "Epoch: 2, Training Loss: 2.7605583667755127\n",
            "Epoch: 2, Training Loss: 2.5400021076202393\n",
            "Epoch: 2, Training Loss: 2.65191912651062\n",
            "Epoch: 2, Training Loss: 2.770966053009033\n",
            "Epoch: 2, Training Loss: 2.6504549980163574\n",
            "Epoch: 2, Training Loss: 2.581737995147705\n",
            "Epoch: 2, Training Loss: 2.7394497394561768\n",
            "Epoch: 2, Training Loss: 2.5701823234558105\n",
            "Epoch: 2, Training Loss: 2.8049614429473877\n",
            "Epoch: 2, Training Loss: 2.4527957439422607\n",
            "Epoch: 2, Training Loss: 2.597494125366211\n",
            "Epoch: 2, Training Loss: 2.7470340728759766\n",
            "Epoch: 2, Training Loss: 2.682896614074707\n",
            "Epoch: 2, Training Loss: 2.7874927520751953\n",
            "Epoch: 2, Training Loss: 2.6642167568206787\n",
            "Epoch: 2, Training Loss: 2.6029369831085205\n",
            "Epoch: 2, Training Loss: 2.7614121437072754\n",
            "Epoch: 2, Training Loss: 2.5631585121154785\n",
            "Epoch: 2, Training Loss: 2.579838275909424\n",
            "Epoch: 2, Training Loss: 2.6433868408203125\n",
            "Epoch: 2, Training Loss: 2.944883346557617\n",
            "Epoch: 2, Training Loss: 2.749288558959961\n",
            "Epoch: 2, Training Loss: 2.815537214279175\n",
            "Epoch: 2, Training Loss: 2.742450475692749\n",
            "Epoch: 2, Training Loss: 2.446354866027832\n",
            "Epoch: 2, Training Loss: 2.6960792541503906\n",
            "Epoch: 2, Training Loss: 2.4823086261749268\n",
            "Epoch: 2, Training Loss: 2.7132647037506104\n",
            "Epoch: 2, Training Loss: 2.6311469078063965\n",
            "Epoch: 2, Training Loss: 2.2484309673309326\n",
            "Epoch: 2, Training Loss: 2.7197494506835938\n",
            "Epoch: 2, Training Loss: 2.4463963508605957\n",
            "Epoch: 2, Training Loss: 2.5870611667633057\n",
            "Epoch: 2, Training Loss: 2.628077983856201\n",
            "Epoch: 2, Training Loss: 2.537806749343872\n",
            "Epoch: 2, Training Loss: 2.6832752227783203\n",
            "Epoch: 2, Training Loss: 2.5735673904418945\n",
            "Epoch: 2, Training Loss: 2.519336462020874\n",
            "Epoch: 2, Training Loss: 2.8550026416778564\n",
            "Epoch: 2, Training Loss: 2.5222582817077637\n",
            "Epoch: 2, Training Loss: 2.7638163566589355\n",
            "Epoch: 2, Training Loss: 2.8998732566833496\n",
            "Epoch: 2, Training Loss: 2.7292163372039795\n",
            "Epoch: 2, Training Loss: 2.5100960731506348\n",
            "Epoch: 2, Training Loss: 2.7515316009521484\n",
            "Epoch: 2, Training Loss: 2.8905179500579834\n",
            "Epoch: 2, Training Loss: 2.625061273574829\n",
            "Epoch: 2, Training Loss: 2.4010581970214844\n",
            "Epoch: 2, Training Loss: 2.7251522541046143\n",
            "Epoch: 2, Training Loss: 2.7764358520507812\n",
            "Epoch: 2, Training Loss: 2.5756373405456543\n",
            "Epoch: 2, Training Loss: 2.5907626152038574\n",
            "Epoch: 2, Training Loss: 2.5991098880767822\n",
            "Epoch: 2, Training Loss: 2.5498032569885254\n",
            "Epoch: 2, Training Loss: 2.534917116165161\n",
            "Epoch: 2, Training Loss: 2.5362040996551514\n",
            "Epoch: 2, Training Loss: 2.556891679763794\n",
            "Epoch: 2, Training Loss: 2.869194507598877\n",
            "Epoch: 2, Training Loss: 2.5838892459869385\n",
            "Epoch: 2, Training Loss: 2.593202829360962\n",
            "Epoch: 2, Training Loss: 2.81396222114563\n",
            "Epoch: 2, Training Loss: 2.454857110977173\n",
            "Epoch: 2, Training Loss: 2.4933409690856934\n",
            "Epoch: 2, Training Loss: 2.730811834335327\n",
            "Epoch: 2, Training Loss: 2.4902267456054688\n",
            "Epoch: 2, Training Loss: 2.7383337020874023\n",
            "Epoch: 2, Training Loss: 2.471278429031372\n",
            "Epoch: 2, Training Loss: 2.799158811569214\n",
            "Epoch: 2, Training Loss: 2.711899995803833\n",
            "Epoch: 2, Training Loss: 2.8462109565734863\n",
            "Epoch: 2, Training Loss: 2.6725776195526123\n",
            "Epoch: 2, Training Loss: 2.7452077865600586\n",
            "Epoch: 2, Training Loss: 2.8019604682922363\n",
            "Epoch: 2, Training Loss: 2.528611660003662\n",
            "Epoch: 2, Training Loss: 2.73371958732605\n",
            "Epoch: 2, Training Loss: 2.608957290649414\n",
            "Epoch: 2, Training Loss: 2.8657641410827637\n",
            "Epoch: 2, Training Loss: 2.4794275760650635\n",
            "Epoch: 2, Training Loss: 2.776597499847412\n",
            "Epoch: 2, Training Loss: 2.758572578430176\n",
            "Epoch: 2, Training Loss: 2.6223440170288086\n",
            "Epoch: 2, Training Loss: 2.5660016536712646\n",
            "Epoch: 2, Training Loss: 2.8756089210510254\n",
            "Epoch: 2, Training Loss: 2.464543104171753\n",
            "Epoch: 2, Training Loss: 2.5513274669647217\n",
            "Epoch: 2, Training Loss: 2.6106326580047607\n",
            "Epoch: 2, Training Loss: 2.750535249710083\n",
            "Epoch: 2, Training Loss: 2.516383409500122\n",
            "Epoch: 2, Training Loss: 2.8889012336730957\n",
            "Epoch: 2, Training Loss: 2.846843957901001\n",
            "Epoch: 2, Training Loss: 2.651268482208252\n",
            "Epoch: 2, Training Loss: 2.8340742588043213\n",
            "Epoch: 2, Training Loss: 3.0329926013946533\n",
            "Epoch: 2, Training Loss: 2.938530206680298\n",
            "Epoch: 2, Training Loss: 2.582362413406372\n",
            "Epoch: 2, Training Loss: 2.5640509128570557\n",
            "Epoch: 2, Training Loss: 2.5352625846862793\n",
            "Epoch: 2, Training Loss: 2.7823891639709473\n",
            "Epoch: 2, Training Loss: 2.518280506134033\n",
            "Epoch: 2, Training Loss: 2.780170202255249\n",
            "Epoch: 2, Training Loss: 2.7487850189208984\n",
            "Epoch: 2, Training Loss: 2.7420976161956787\n",
            "Epoch: 2, Training Loss: 2.7634172439575195\n",
            "Epoch: 2, Training Loss: 2.667574644088745\n",
            "Epoch: 2, Training Loss: 2.7137279510498047\n",
            "Epoch: 2, Training Loss: 2.4809954166412354\n",
            "Epoch: 2, Training Loss: 2.523510694503784\n",
            "Epoch: 2, Training Loss: 2.511235237121582\n",
            "Epoch: 2, Training Loss: 2.6486470699310303\n",
            "Epoch: 2, Training Loss: 2.3714792728424072\n",
            "Epoch: 2, Training Loss: 2.750553607940674\n",
            "Epoch: 2, Training Loss: 2.5561039447784424\n",
            "Epoch: 2, Training Loss: 2.5764949321746826\n",
            "Epoch: 2, Training Loss: 2.5395543575286865\n",
            "Epoch: 2, Training Loss: 2.8653838634490967\n",
            "Epoch: 2, Training Loss: 2.617408037185669\n",
            "Epoch: 2, Training Loss: 2.5827038288116455\n",
            "Epoch: 2, Training Loss: 2.491499662399292\n",
            "Epoch: 2, Training Loss: 2.5545995235443115\n",
            "Epoch: 2, Training Loss: 2.37101674079895\n",
            "Epoch: 2, Training Loss: 2.730912923812866\n",
            "Epoch: 2, Training Loss: 2.3928322792053223\n",
            "Epoch: 2, Training Loss: 2.54805064201355\n",
            "Epoch: 2, Training Loss: 2.4599320888519287\n",
            "Epoch: 2, Training Loss: 2.5922586917877197\n",
            "Epoch: 2, Training Loss: 2.714080810546875\n",
            "Epoch: 2, Training Loss: 2.6217339038848877\n",
            "Epoch: 2, Training Loss: 2.4505398273468018\n",
            "Epoch: 2, Training Loss: 2.8644258975982666\n",
            "Epoch: 2, Training Loss: 2.6454646587371826\n",
            "Epoch: 2, Training Loss: 2.466464042663574\n",
            "Epoch: 2, Training Loss: 2.8680944442749023\n",
            "Epoch: 2, Training Loss: 2.455819606781006\n",
            "Epoch: 2, Training Loss: 2.5831212997436523\n",
            "Epoch: 2, Training Loss: 2.3195760250091553\n",
            "Epoch: 2, Training Loss: 2.6456127166748047\n",
            "Epoch: 2, Training Loss: 2.625741720199585\n",
            "Epoch: 2, Training Loss: 2.667177200317383\n",
            "Epoch: 2, Training Loss: 2.7165346145629883\n",
            "Epoch: 2, Training Loss: 2.427314043045044\n",
            "Epoch: 2, Training Loss: 2.415984630584717\n",
            "Epoch: 2, Training Loss: 2.968008518218994\n",
            "Epoch: 2, Training Loss: 2.5822865962982178\n",
            "Epoch: 2, Training Loss: 2.675536632537842\n",
            "Epoch: 2, Training Loss: 2.7122931480407715\n",
            "Epoch: 2, Training Loss: 2.778996229171753\n",
            "Epoch: 2, Training Loss: 2.62092924118042\n",
            "Epoch: 2, Training Loss: 2.9198131561279297\n",
            "Epoch: 2, Training Loss: 2.335704803466797\n",
            "Epoch: 2, Training Loss: 2.684779405593872\n",
            "Epoch: 2, Training Loss: 2.232858180999756\n",
            "Epoch: 2, Training Loss: 3.0691609382629395\n",
            "Epoch: 2, Training Loss: 2.729665517807007\n",
            "Epoch: 2, Training Loss: 2.7053234577178955\n",
            "Epoch: 2, Training Loss: 2.618098020553589\n",
            "Epoch: 2, Training Loss: 2.673769950866699\n",
            "Epoch: 2, Training Loss: 2.4590303897857666\n",
            "Epoch: 2, Training Loss: 2.7536990642547607\n",
            "Epoch: 2, Training Loss: 2.695466995239258\n",
            "Epoch: 2, Training Loss: 2.3282768726348877\n",
            "Epoch: 2, Training Loss: 2.312964677810669\n",
            "Epoch: 2, Training Loss: 2.418299674987793\n",
            "Epoch: 2, Training Loss: 2.4708869457244873\n",
            "Epoch: 2, Training Loss: 2.637200117111206\n",
            "Epoch: 2, Training Loss: 2.6488113403320312\n",
            "Epoch: 2, Training Loss: 2.85349178314209\n",
            "Epoch: 2, Training Loss: 2.7357826232910156\n",
            "Epoch: 2, Training Loss: 2.862666606903076\n",
            "Epoch: 2, Training Loss: 2.6378140449523926\n",
            "Epoch: 2, Training Loss: 2.6529552936553955\n",
            "Epoch: 2, Training Loss: 2.7716424465179443\n",
            "Epoch: 2, Training Loss: 2.707949161529541\n",
            "Epoch: 2, Training Loss: 2.622323751449585\n",
            "Epoch: 2, Training Loss: 2.524628162384033\n",
            "Epoch: 2, Training Loss: 2.3914401531219482\n",
            "Epoch: 2, Training Loss: 2.4973669052124023\n",
            "Epoch: 2, Training Loss: 2.711124897003174\n",
            "Epoch: 2, Training Loss: 2.4684975147247314\n",
            "Epoch: 2, Training Loss: 2.602768898010254\n",
            "Epoch: 2, Training Loss: 2.8221843242645264\n",
            "Epoch: 2, Training Loss: 2.6191372871398926\n",
            "Epoch: 2, Training Loss: 2.5257818698883057\n",
            "Epoch: 2, Training Loss: 2.9473581314086914\n",
            "Epoch: 2, Training Loss: 2.5125715732574463\n",
            "Epoch: 2, Training Loss: 2.4313855171203613\n",
            "Epoch: 2, Training Loss: 2.64306640625\n",
            "Epoch: 2, Training Loss: 2.7914929389953613\n",
            "Epoch: 2, Training Loss: 2.601851224899292\n",
            "Epoch: 2, Training Loss: 2.6105191707611084\n",
            "Epoch: 2, Training Loss: 2.763354778289795\n",
            "Epoch: 2, Training Loss: 2.795717477798462\n",
            "Epoch: 2, Training Loss: 2.565021514892578\n",
            "Epoch: 2, Training Loss: 2.5201170444488525\n",
            "Epoch: 2, Training Loss: 2.6792712211608887\n",
            "Epoch: 2, Training Loss: 2.3928442001342773\n",
            "Epoch: 2, Training Loss: 2.5226199626922607\n",
            "Epoch: 2, Training Loss: 2.5324597358703613\n",
            "Epoch: 2, Training Loss: 2.754528284072876\n",
            "Epoch: 2, Training Loss: 2.584287405014038\n",
            "Epoch: 2, Training Loss: 2.7000443935394287\n",
            "Epoch: 2, Training Loss: 2.5038278102874756\n",
            "Epoch: 2, Training Loss: 2.5337960720062256\n",
            "Epoch: 2, Training Loss: 2.7849831581115723\n",
            "Epoch: 2, Training Loss: 2.4945802688598633\n",
            "Epoch: 2, Training Loss: 2.5851683616638184\n",
            "Epoch: 2, Training Loss: 2.305414915084839\n",
            "Epoch: 2, Training Loss: 2.7899744510650635\n",
            "Epoch: 2, Training Loss: 2.3538169860839844\n",
            "Epoch: 2, Training Loss: 2.7188665866851807\n",
            "Epoch: 2, Validation Loss: 2.3718369007110596\n",
            "Epoch: 2, Validation Loss: 2.525240898132324\n",
            "Epoch: 2, Validation Loss: 2.1170692443847656\n",
            "Epoch: 2, Validation Loss: 2.354480504989624\n",
            "Epoch: 2, Validation Loss: 2.6650407314300537\n",
            "Epoch: 2, Validation Loss: 2.6654834747314453\n",
            "Epoch: 2, Validation Loss: 2.4784698486328125\n",
            "Epoch: 2, Validation Loss: 2.2902474403381348\n",
            "Epoch: 2, Validation Loss: 2.467790365219116\n",
            "Epoch: 2, Validation Loss: 2.475198984146118\n",
            "Epoch: 2, Validation Loss: 2.3850889205932617\n",
            "Epoch: 2, Validation Loss: 2.3484954833984375\n",
            "Epoch: 2, Validation Loss: 2.3944809436798096\n",
            "Epoch: 2, Validation Loss: 2.4965298175811768\n",
            "Epoch: 2, Validation Loss: 2.4478297233581543\n",
            "Epoch: 2, Validation Loss: 2.5751914978027344\n",
            "Epoch: 2, Validation Loss: 2.594214916229248\n",
            "Epoch: 2, Validation Loss: 2.3538601398468018\n",
            "Epoch: 2, Validation Loss: 2.42918062210083\n",
            "Epoch: 2, Validation Loss: 2.588270664215088\n",
            "Epoch: 2, Validation Loss: 2.5887911319732666\n",
            "Epoch: 2, Validation Loss: 2.695251703262329\n",
            "Epoch: 2, Validation Loss: 2.3925087451934814\n",
            "Epoch: 2, Validation Loss: 2.344083070755005\n",
            "Epoch: 2, Validation Loss: 2.402723550796509\n",
            "Epoch: 2, Validation Loss: 2.550248622894287\n",
            "Epoch: 2, Validation Loss: 2.686514139175415\n",
            "Epoch: 2, Validation Loss: 2.6857619285583496\n",
            "Epoch: 2, Validation Loss: 2.6361920833587646\n",
            "Epoch: 2, Validation Loss: 2.5537211894989014\n",
            "Epoch: 2, Validation Loss: 2.2651352882385254\n",
            "Epoch: 2, Validation Loss: 2.3712265491485596\n",
            "Epoch: 2, Validation Loss: 2.455634117126465\n",
            "Epoch: 2, Validation Loss: 2.3023881912231445\n",
            "Epoch: 2, Validation Loss: 2.295740842819214\n",
            "Epoch: 2, Validation Loss: 2.5143864154815674\n",
            "Epoch: 2, Validation Loss: 2.2148520946502686\n",
            "Epoch: 2, Validation Loss: 2.307579278945923\n",
            "Epoch: 2, Validation Loss: 2.633530855178833\n",
            "Epoch: 2, Validation Loss: 2.3596136569976807\n",
            "Epoch: 2, Validation Loss: 2.3549203872680664\n",
            "Epoch: 2, Validation Loss: 2.4973089694976807\n",
            "Epoch: 2, Validation Loss: 2.8058817386627197\n",
            "Epoch: 2, Validation Loss: 2.518249750137329\n",
            "Epoch: 2, Validation Loss: 2.381782293319702\n",
            "Epoch: 2, Validation Loss: 2.390101194381714\n",
            "Epoch: 2, Validation Loss: 2.1468589305877686\n",
            "Epoch: 2, Validation Loss: 2.3389151096343994\n",
            "Epoch: 2, Validation Loss: 2.680288076400757\n",
            "Epoch: 2, Validation Loss: 2.465013265609741\n",
            "Epoch: 2, Validation Loss: 2.1638476848602295\n",
            "Epoch: 2, Validation Loss: 2.4807615280151367\n",
            "Epoch: 2, Validation Loss: 2.2558469772338867\n",
            "Epoch: 2, Validation Loss: 2.5635035037994385\n",
            "Epoch: 2, Validation Loss: 2.343006134033203\n",
            "Epoch: 2, Validation Loss: 2.3860185146331787\n",
            "Epoch: 2, Validation Loss: 2.4860239028930664\n",
            "Epoch: 2, Validation Loss: 2.4704928398132324\n",
            "Epoch: 2, Validation Loss: 2.3518707752227783\n",
            "Epoch: 2, Validation Loss: 2.6100826263427734\n",
            "Epoch: 2, Validation Loss: 2.6478397846221924\n",
            "Epoch: 2, Validation Loss: 2.4130642414093018\n",
            "Epoch: 2, Validation Loss: 2.6411375999450684\n",
            "Epoch: 2, Validation Loss: 2.7223174571990967\n",
            "Epoch: 2, Validation Loss: 2.5803236961364746\n",
            "Epoch: 2, Validation Loss: 2.559321165084839\n",
            "Epoch: 2, Validation Loss: 2.336380958557129\n",
            "Epoch: 2, Validation Loss: 2.3348774909973145\n",
            "Epoch: 2, Validation Loss: 2.4586148262023926\n",
            "Epoch: 2, Validation Loss: 2.5178325176239014\n",
            "Epoch: 2, Validation Loss: 2.1644527912139893\n",
            "Epoch: 2, Validation Loss: 2.3141024112701416\n",
            "Epoch: 2, Validation Loss: 2.4157698154449463\n",
            "Epoch: 2, Validation Loss: 2.3944456577301025\n",
            "Epoch: 2, Validation Loss: 2.3811168670654297\n",
            "Epoch: 2, Validation Loss: 2.3712258338928223\n",
            "Epoch: 2, Validation Loss: 2.572423219680786\n",
            "Epoch: 2, Validation Loss: 2.520658016204834\n",
            "Epoch: 2, Validation Loss: 2.3263742923736572\n",
            "Epoch: 2, Validation Loss: 2.2006266117095947\n",
            "Epoch: 2, Validation Loss: 2.367720365524292\n",
            "Epoch: 2, Validation Loss: 2.569803476333618\n",
            "Epoch: 2, Validation Loss: 2.449266195297241\n",
            "Epoch: 2, Validation Loss: 2.497714042663574\n",
            "Epoch: 2, Validation Loss: 2.488377332687378\n",
            "Epoch: 2, Validation Loss: 2.3561501502990723\n",
            "Epoch: 2, Validation Loss: 2.593010663986206\n",
            "Epoch: 2, Validation Loss: 2.3267822265625\n",
            "Epoch: 2, Validation Loss: 2.653670310974121\n",
            "Epoch: 2, Validation Loss: 2.6639087200164795\n",
            "Epoch: 2, Validation Loss: 2.1513726711273193\n",
            "Epoch: 2, Validation Loss: 2.7110588550567627\n",
            "Epoch: 2, Validation Loss: 2.7136118412017822\n",
            "Epoch: 2, Validation Loss: 2.254809856414795\n",
            "Epoch: 2, Validation Loss: 2.514186143875122\n",
            "Epoch: 2, Validation Loss: 2.3005423545837402\n",
            "Epoch: 2, Validation Loss: 2.277719259262085\n",
            "Epoch: 2, Validation Loss: 2.637367010116577\n",
            "Epoch: 2, Validation Loss: 2.6743264198303223\n",
            "Epoch: 2, Validation Loss: 2.2264323234558105\n",
            "Epoch: 2, Validation Loss: 2.471477746963501\n",
            "Epoch: 2, Validation Loss: 2.3410582542419434\n",
            "Epoch: 2, Validation Loss: 2.434051513671875\n",
            "Epoch: 2, Validation Loss: 2.4808266162872314\n",
            "Epoch: 2, Validation Loss: 2.554720640182495\n",
            "Epoch: 2, Validation Loss: 2.381382465362549\n",
            "Epoch: 2, Validation Loss: 2.72157621383667\n",
            "Epoch: 2, Validation Loss: 2.426147699356079\n",
            "Epoch: 2, Validation Loss: 2.2367002964019775\n",
            "Epoch: 2, Validation Loss: 2.475766658782959\n",
            "Epoch: 2, Validation Loss: 2.578202486038208\n",
            "Epoch: 2, Validation Loss: 2.5211470127105713\n",
            "Epoch: 2, Validation Loss: 2.456437349319458\n",
            "Epoch: 2, Validation Loss: 2.511378765106201\n",
            "Epoch: 2, Validation Loss: 2.552175998687744\n",
            "Epoch: 2, Validation Loss: 2.237668514251709\n",
            "Epoch: 2, Validation Loss: 2.375047445297241\n",
            "Epoch: 2, Validation Loss: 2.6769378185272217\n",
            "Epoch: 2, Validation Loss: 2.4482717514038086\n",
            "Epoch: 2, Validation Loss: 2.529466152191162\n",
            "Epoch: 2, Validation Loss: 2.278634548187256\n",
            "Epoch: 2, Validation Loss: 2.376070499420166\n",
            "Epoch: 2, Validation Loss: 2.319227695465088\n",
            "Epoch: 2, Validation Loss: 2.3932688236236572\n",
            "Epoch: 2, Validation Loss: 2.608081102371216\n",
            "Epoch: 2, Validation Loss: 2.238490104675293\n",
            "Epoch: 2, Validation Loss: 2.7044100761413574\n",
            "Epoch: 2, Validation Loss: 2.8141016960144043\n",
            "Epoch: 2, Validation Loss: 2.361224412918091\n",
            "Epoch: 2, Validation Loss: 2.220069646835327\n",
            "Epoch: 2, Validation Loss: 2.3300838470458984\n",
            "Epoch: 2, Validation Loss: 2.244248628616333\n",
            "Epoch: 2, Validation Loss: 2.5497939586639404\n",
            "Epoch: 2, Validation Loss: 2.361309766769409\n",
            "Epoch: 2, Validation Loss: 2.3877811431884766\n",
            "Epoch: 2, Validation Loss: 2.5901849269866943\n",
            "Epoch: 2, Validation Loss: 2.443789005279541\n",
            "Epoch: 2, Validation Loss: 2.6896259784698486\n",
            "Epoch: 2, Validation Loss: 2.2969236373901367\n",
            "Epoch: 2, Validation Loss: 2.2075395584106445\n",
            "Epoch: 2, Validation Loss: 2.564983606338501\n",
            "Epoch: 2, Validation Loss: 2.34802508354187\n",
            "Epoch: 2, Validation Loss: 2.4987857341766357\n",
            "Epoch: 2, Validation Loss: 2.625661611557007\n",
            "Epoch: 2, Validation Loss: 2.724086284637451\n",
            "Epoch: 2, Validation Loss: 2.4190115928649902\n",
            "Epoch: 2, Validation Loss: 2.3443262577056885\n",
            "Epoch: 2, Validation Loss: 2.781493663787842\n",
            "Epoch: 2, Validation Loss: 2.451582193374634\n",
            "Epoch: 2, Validation Loss: 2.386432409286499\n",
            "Epoch: 2, Validation Loss: 2.496936798095703\n",
            "Epoch: 2, Validation Loss: 2.243776321411133\n",
            "Epoch: 2, Validation Loss: 2.3756520748138428\n",
            "Epoch: 2, Validation Loss: 2.6472835540771484\n",
            "Epoch: 2, Validation Loss: 2.331566333770752\n",
            "Epoch: 2, Validation Loss: 2.413048505783081\n",
            "Epoch: 2, Validation Loss: 2.6288602352142334\n",
            "Epoch: 2, Validation Loss: 2.6742820739746094\n",
            "Epoch: 2, Validation Loss: 2.597524881362915\n",
            "Epoch: 2, Validation Loss: 2.7464215755462646\n",
            "Epoch: 2, Validation Loss: 2.495791435241699\n",
            "Epoch: 2, Validation Loss: 2.4251089096069336\n",
            "Epoch: 2, Validation Loss: 2.344097375869751\n",
            "Epoch: 2, Validation Loss: 2.296684741973877\n",
            "Epoch: 2, Validation Loss: 2.395721197128296\n",
            "Epoch: 2, Validation Loss: 2.4928438663482666\n",
            "Epoch: 2, Validation Loss: 2.4611387252807617\n",
            "Epoch: 2, Validation Loss: 2.2546205520629883\n",
            "Epoch: 2, Validation Loss: 2.479506731033325\n",
            "Epoch: 2, Validation Loss: 2.201110363006592\n",
            "Epoch: 2, Validation Loss: 2.3066577911376953\n",
            "Epoch: 2, Validation Loss: 2.3942456245422363\n",
            "Epoch: 2, Validation Loss: 2.2679643630981445\n",
            "Epoch: 2, Validation Loss: 2.5599541664123535\n",
            "Epoch: 2, Validation Loss: 2.5013604164123535\n",
            "Epoch: 2, Validation Loss: 2.3176047801971436\n",
            "Epoch: 2, Validation Loss: 2.476790189743042\n",
            "Epoch: 2, Validation Loss: 2.3191592693328857\n",
            "Epoch: 2, Validation Loss: 2.384214162826538\n",
            "Epoch: 2, Validation Loss: 2.4221036434173584\n",
            "Epoch: 2, Validation Loss: 2.659212350845337\n",
            "Epoch: 2, Validation Loss: 2.2897324562072754\n",
            "Epoch: 2, Validation Loss: 2.7022159099578857\n",
            "Epoch: 2, Validation Loss: 2.3002004623413086\n",
            "Epoch: 2, Validation Loss: 2.5926337242126465\n",
            "Epoch: 2, Validation Loss: 2.3285858631134033\n",
            "Epoch: 2, Validation Loss: 2.5243911743164062\n",
            "Epoch: 2, Validation Loss: 2.506366014480591\n",
            "Epoch: 2, Validation Loss: 2.153991222381592\n",
            "Epoch: 2, Validation Loss: 2.4872710704803467\n",
            "Epoch: 2, Validation Loss: 2.436419725418091\n",
            "Epoch: 2, Validation Loss: 2.5184123516082764\n",
            "Epoch: 2, Validation Loss: 2.5351083278656006\n",
            "Epoch: 2, Validation Loss: 2.5741705894470215\n",
            "Epoch: 2, Validation Loss: 2.579904556274414\n",
            "Epoch: 2, Validation Loss: 2.5240023136138916\n",
            "Epoch: 2, Validation Loss: 2.3750948905944824\n",
            "Epoch: 2, Validation Loss: 2.5253002643585205\n",
            "Epoch: 2, Validation Loss: 2.410388231277466\n",
            "Epoch: 2, Validation Loss: 2.3367316722869873\n",
            "Epoch: 2, Validation Loss: 2.1974596977233887\n",
            "Epoch: 2, Validation Loss: 2.3895153999328613\n",
            "Epoch: 2, Validation Loss: 2.5339159965515137\n",
            "Epoch: 2, Validation Loss: 2.4297544956207275\n",
            "Epoch: 2, Validation Loss: 2.4308247566223145\n",
            "Epoch: 2, Validation Loss: 2.4123165607452393\n",
            "Epoch: 2, Validation Loss: 2.7181639671325684\n",
            "Epoch: 2, Validation Loss: 2.5611889362335205\n",
            "Epoch: 2, Validation Loss: 2.4612820148468018\n",
            "Epoch: 2, Validation Loss: 2.3975539207458496\n",
            "Epoch: 2, Validation Loss: 2.4119763374328613\n",
            "Epoch: 2, Validation Loss: 2.3678274154663086\n",
            "Epoch: 2, Validation Loss: 2.5568997859954834\n",
            "Epoch: 2, Validation Loss: 2.3290090560913086\n",
            "Epoch: 2, Validation Loss: 2.5554869174957275\n",
            "Epoch: 2, Validation Loss: 2.5532784461975098\n",
            "Epoch: 2, Validation Loss: 2.3696811199188232\n",
            "Epoch: 2, Validation Loss: 2.4663889408111572\n",
            "Epoch: 2, Validation Loss: 2.526723623275757\n",
            "Epoch: 2, Validation Loss: 2.649462938308716\n",
            "Epoch: 2, Validation Loss: 2.38940691947937\n",
            "Epoch: 2, Validation Loss: 2.3477108478546143\n",
            "Epoch: 2, Validation Loss: 2.2784385681152344\n",
            "Epoch: 2, Validation Loss: 2.495668649673462\n",
            "Epoch: 3\n",
            "------------------------------\n",
            "Epoch: 3, Training Loss: 2.535968542098999\n",
            "Epoch: 3, Training Loss: 2.478459119796753\n",
            "Epoch: 3, Training Loss: 2.5197460651397705\n",
            "Epoch: 3, Training Loss: 2.678622007369995\n",
            "Epoch: 3, Training Loss: 2.54775333404541\n",
            "Epoch: 3, Training Loss: 2.3788323402404785\n",
            "Epoch: 3, Training Loss: 2.5332140922546387\n",
            "Epoch: 3, Training Loss: 2.7801918983459473\n",
            "Epoch: 3, Training Loss: 2.6097800731658936\n",
            "Epoch: 3, Training Loss: 2.6127443313598633\n",
            "Epoch: 3, Training Loss: 2.546607255935669\n",
            "Epoch: 3, Training Loss: 2.5357658863067627\n",
            "Epoch: 3, Training Loss: 2.612149953842163\n",
            "Epoch: 3, Training Loss: 2.485579252243042\n",
            "Epoch: 3, Training Loss: 2.4395244121551514\n",
            "Epoch: 3, Training Loss: 2.5863258838653564\n",
            "Epoch: 3, Training Loss: 2.236785411834717\n",
            "Epoch: 3, Training Loss: 2.572927236557007\n",
            "Epoch: 3, Training Loss: 2.5193777084350586\n",
            "Epoch: 3, Training Loss: 2.4488656520843506\n",
            "Epoch: 3, Training Loss: 2.5851285457611084\n",
            "Epoch: 3, Training Loss: 2.9629781246185303\n",
            "Epoch: 3, Training Loss: 2.425210475921631\n",
            "Epoch: 3, Training Loss: 2.680908441543579\n",
            "Epoch: 3, Training Loss: 2.595968008041382\n",
            "Epoch: 3, Training Loss: 2.4598402976989746\n",
            "Epoch: 3, Training Loss: 2.5254764556884766\n",
            "Epoch: 3, Training Loss: 2.484827756881714\n",
            "Epoch: 3, Training Loss: 2.480332374572754\n",
            "Epoch: 3, Training Loss: 2.4215006828308105\n",
            "Epoch: 3, Training Loss: 2.399418830871582\n",
            "Epoch: 3, Training Loss: 2.4860782623291016\n",
            "Epoch: 3, Training Loss: 2.5134942531585693\n",
            "Epoch: 3, Training Loss: 2.5459766387939453\n",
            "Epoch: 3, Training Loss: 2.6008944511413574\n",
            "Epoch: 3, Training Loss: 2.4724531173706055\n",
            "Epoch: 3, Training Loss: 2.7340950965881348\n",
            "Epoch: 3, Training Loss: 2.5544424057006836\n",
            "Epoch: 3, Training Loss: 2.2688629627227783\n",
            "Epoch: 3, Training Loss: 2.6290721893310547\n",
            "Epoch: 3, Training Loss: 2.6010749340057373\n",
            "Epoch: 3, Training Loss: 2.418138265609741\n",
            "Epoch: 3, Training Loss: 2.4774370193481445\n",
            "Epoch: 3, Training Loss: 2.611658811569214\n",
            "Epoch: 3, Training Loss: 2.642019271850586\n",
            "Epoch: 3, Training Loss: 2.427170991897583\n",
            "Epoch: 3, Training Loss: 2.828272581100464\n",
            "Epoch: 3, Training Loss: 2.8169093132019043\n",
            "Epoch: 3, Training Loss: 2.621368885040283\n",
            "Epoch: 3, Training Loss: 2.3189408779144287\n",
            "Epoch: 3, Training Loss: 2.58746337890625\n",
            "Epoch: 3, Training Loss: 2.5246031284332275\n",
            "Epoch: 3, Training Loss: 2.774207353591919\n",
            "Epoch: 3, Training Loss: 2.54550838470459\n",
            "Epoch: 3, Training Loss: 2.6662445068359375\n",
            "Epoch: 3, Training Loss: 2.5190346240997314\n",
            "Epoch: 3, Training Loss: 2.520204782485962\n",
            "Epoch: 3, Training Loss: 2.446058988571167\n",
            "Epoch: 3, Training Loss: 2.771345853805542\n",
            "Epoch: 3, Training Loss: 2.538952589035034\n",
            "Epoch: 3, Training Loss: 2.778650999069214\n",
            "Epoch: 3, Training Loss: 2.776331663131714\n",
            "Epoch: 3, Training Loss: 2.6525864601135254\n",
            "Epoch: 3, Training Loss: 2.516561269760132\n",
            "Epoch: 3, Training Loss: 2.4443202018737793\n",
            "Epoch: 3, Training Loss: 2.572725296020508\n",
            "Epoch: 3, Training Loss: 2.545687437057495\n",
            "Epoch: 3, Training Loss: 2.781466484069824\n",
            "Epoch: 3, Training Loss: 2.4883272647857666\n",
            "Epoch: 3, Training Loss: 2.5303146839141846\n",
            "Epoch: 3, Training Loss: 2.4375882148742676\n",
            "Epoch: 3, Training Loss: 2.7870638370513916\n",
            "Epoch: 3, Training Loss: 2.2338333129882812\n",
            "Epoch: 3, Training Loss: 2.6265320777893066\n",
            "Epoch: 3, Training Loss: 2.707956075668335\n",
            "Epoch: 3, Training Loss: 2.388742208480835\n",
            "Epoch: 3, Training Loss: 2.515038251876831\n",
            "Epoch: 3, Training Loss: 2.3400638103485107\n",
            "Epoch: 3, Training Loss: 2.7383649349212646\n",
            "Epoch: 3, Training Loss: 2.50146746635437\n",
            "Epoch: 3, Training Loss: 2.2467570304870605\n",
            "Epoch: 3, Training Loss: 2.4342119693756104\n",
            "Epoch: 3, Training Loss: 2.619554281234741\n",
            "Epoch: 3, Training Loss: 2.5101239681243896\n",
            "Epoch: 3, Training Loss: 2.8799424171447754\n",
            "Epoch: 3, Training Loss: 2.648235321044922\n",
            "Epoch: 3, Training Loss: 2.0571095943450928\n",
            "Epoch: 3, Training Loss: 2.506382465362549\n",
            "Epoch: 3, Training Loss: 2.499492645263672\n",
            "Epoch: 3, Training Loss: 2.253753900527954\n",
            "Epoch: 3, Training Loss: 2.5819599628448486\n",
            "Epoch: 3, Training Loss: 2.5039145946502686\n",
            "Epoch: 3, Training Loss: 2.6642019748687744\n",
            "Epoch: 3, Training Loss: 2.6882143020629883\n",
            "Epoch: 3, Training Loss: 2.6034419536590576\n",
            "Epoch: 3, Training Loss: 2.397301197052002\n",
            "Epoch: 3, Training Loss: 2.63027024269104\n",
            "Epoch: 3, Training Loss: 2.635934829711914\n",
            "Epoch: 3, Training Loss: 2.535965919494629\n",
            "Epoch: 3, Training Loss: 2.424448013305664\n",
            "Epoch: 3, Training Loss: 2.422421932220459\n",
            "Epoch: 3, Training Loss: 2.4668052196502686\n",
            "Epoch: 3, Training Loss: 2.4441914558410645\n",
            "Epoch: 3, Training Loss: 2.702505111694336\n",
            "Epoch: 3, Training Loss: 2.469050645828247\n",
            "Epoch: 3, Training Loss: 2.204331636428833\n",
            "Epoch: 3, Training Loss: 2.592222213745117\n",
            "Epoch: 3, Training Loss: 2.4584271907806396\n",
            "Epoch: 3, Training Loss: 2.6105384826660156\n",
            "Epoch: 3, Training Loss: 2.4599690437316895\n",
            "Epoch: 3, Training Loss: 2.31319260597229\n",
            "Epoch: 3, Training Loss: 2.5776846408843994\n",
            "Epoch: 3, Training Loss: 2.393338441848755\n",
            "Epoch: 3, Training Loss: 2.6674492359161377\n",
            "Epoch: 3, Training Loss: 2.4206161499023438\n",
            "Epoch: 3, Training Loss: 2.4251174926757812\n",
            "Epoch: 3, Training Loss: 2.6286208629608154\n",
            "Epoch: 3, Training Loss: 2.414026975631714\n",
            "Epoch: 3, Training Loss: 2.6694130897521973\n",
            "Epoch: 3, Training Loss: 2.624668836593628\n",
            "Epoch: 3, Training Loss: 2.7193214893341064\n",
            "Epoch: 3, Training Loss: 2.4289791584014893\n",
            "Epoch: 3, Training Loss: 2.3414361476898193\n",
            "Epoch: 3, Training Loss: 2.5479576587677\n",
            "Epoch: 3, Training Loss: 2.6241092681884766\n",
            "Epoch: 3, Training Loss: 2.505997896194458\n",
            "Epoch: 3, Training Loss: 2.539560079574585\n",
            "Epoch: 3, Training Loss: 2.6273248195648193\n",
            "Epoch: 3, Training Loss: 2.426433563232422\n",
            "Epoch: 3, Training Loss: 2.6074512004852295\n",
            "Epoch: 3, Training Loss: 2.5871851444244385\n",
            "Epoch: 3, Training Loss: 2.603327751159668\n",
            "Epoch: 3, Training Loss: 2.2330026626586914\n",
            "Epoch: 3, Training Loss: 2.4548568725585938\n",
            "Epoch: 3, Training Loss: 2.467775344848633\n",
            "Epoch: 3, Training Loss: 2.434756278991699\n",
            "Epoch: 3, Training Loss: 2.1666808128356934\n",
            "Epoch: 3, Training Loss: 2.4965455532073975\n",
            "Epoch: 3, Training Loss: 2.424861431121826\n",
            "Epoch: 3, Training Loss: 2.508317708969116\n",
            "Epoch: 3, Training Loss: 2.365309238433838\n",
            "Epoch: 3, Training Loss: 2.4839861392974854\n",
            "Epoch: 3, Training Loss: 2.4426305294036865\n",
            "Epoch: 3, Training Loss: 2.505004405975342\n",
            "Epoch: 3, Training Loss: 2.4236607551574707\n",
            "Epoch: 3, Training Loss: 2.493443250656128\n",
            "Epoch: 3, Training Loss: 2.419039726257324\n",
            "Epoch: 3, Training Loss: 2.434256076812744\n",
            "Epoch: 3, Training Loss: 2.365208864212036\n",
            "Epoch: 3, Training Loss: 2.5876846313476562\n",
            "Epoch: 3, Training Loss: 2.1381008625030518\n",
            "Epoch: 3, Training Loss: 2.615962028503418\n",
            "Epoch: 3, Training Loss: 2.6087467670440674\n",
            "Epoch: 3, Training Loss: 2.5970747470855713\n",
            "Epoch: 3, Training Loss: 2.4353621006011963\n",
            "Epoch: 3, Training Loss: 2.641836643218994\n",
            "Epoch: 3, Training Loss: 2.641448497772217\n",
            "Epoch: 3, Training Loss: 2.3586719036102295\n",
            "Epoch: 3, Training Loss: 2.4368433952331543\n",
            "Epoch: 3, Training Loss: 2.8107831478118896\n",
            "Epoch: 3, Training Loss: 2.2881546020507812\n",
            "Epoch: 3, Training Loss: 2.3014678955078125\n",
            "Epoch: 3, Training Loss: 2.5482497215270996\n",
            "Epoch: 3, Training Loss: 2.5162813663482666\n",
            "Epoch: 3, Training Loss: 2.2444169521331787\n",
            "Epoch: 3, Training Loss: 2.2524800300598145\n",
            "Epoch: 3, Training Loss: 2.4468600749969482\n",
            "Epoch: 3, Training Loss: 2.662693500518799\n",
            "Epoch: 3, Training Loss: 2.4259042739868164\n",
            "Epoch: 3, Training Loss: 2.6493072509765625\n",
            "Epoch: 3, Training Loss: 2.5348682403564453\n",
            "Epoch: 3, Training Loss: 2.3415277004241943\n",
            "Epoch: 3, Training Loss: 2.3106563091278076\n",
            "Epoch: 3, Training Loss: 2.424283266067505\n",
            "Epoch: 3, Training Loss: 2.382754325866699\n",
            "Epoch: 3, Training Loss: 2.7201309204101562\n",
            "Epoch: 3, Training Loss: 2.4555084705352783\n",
            "Epoch: 3, Training Loss: 2.3013246059417725\n",
            "Epoch: 3, Training Loss: 2.57187557220459\n",
            "Epoch: 3, Training Loss: 2.3963117599487305\n",
            "Epoch: 3, Training Loss: 2.2654612064361572\n",
            "Epoch: 3, Training Loss: 2.528935670852661\n",
            "Epoch: 3, Training Loss: 2.496685028076172\n",
            "Epoch: 3, Training Loss: 2.4358861446380615\n",
            "Epoch: 3, Training Loss: 2.559844493865967\n",
            "Epoch: 3, Training Loss: 2.631988525390625\n",
            "Epoch: 3, Training Loss: 2.6858530044555664\n",
            "Epoch: 3, Training Loss: 2.493896007537842\n",
            "Epoch: 3, Training Loss: 2.630167245864868\n",
            "Epoch: 3, Training Loss: 2.8347420692443848\n",
            "Epoch: 3, Training Loss: 2.5619287490844727\n",
            "Epoch: 3, Training Loss: 2.5169434547424316\n",
            "Epoch: 3, Training Loss: 2.6726839542388916\n",
            "Epoch: 3, Training Loss: 2.520266056060791\n",
            "Epoch: 3, Training Loss: 2.6838839054107666\n",
            "Epoch: 3, Training Loss: 2.6722776889801025\n",
            "Epoch: 3, Training Loss: 2.286041021347046\n",
            "Epoch: 3, Training Loss: 2.4822540283203125\n",
            "Epoch: 3, Training Loss: 2.5449459552764893\n",
            "Epoch: 3, Training Loss: 2.4169669151306152\n",
            "Epoch: 3, Training Loss: 2.563415050506592\n",
            "Epoch: 3, Training Loss: 2.6558494567871094\n",
            "Epoch: 3, Training Loss: 2.6427478790283203\n",
            "Epoch: 3, Training Loss: 2.4808616638183594\n",
            "Epoch: 3, Training Loss: 2.44744873046875\n",
            "Epoch: 3, Training Loss: 2.577573537826538\n",
            "Epoch: 3, Training Loss: 2.59344744682312\n",
            "Epoch: 3, Training Loss: 2.6000325679779053\n",
            "Epoch: 3, Training Loss: 2.524087429046631\n",
            "Epoch: 3, Training Loss: 2.4523422718048096\n",
            "Epoch: 3, Training Loss: 2.595233678817749\n",
            "Epoch: 3, Training Loss: 2.386569023132324\n",
            "Epoch: 3, Training Loss: 2.5124447345733643\n",
            "Epoch: 3, Training Loss: 2.5754401683807373\n",
            "Epoch: 3, Training Loss: 2.6006667613983154\n",
            "Epoch: 3, Training Loss: 2.4656646251678467\n",
            "Epoch: 3, Training Loss: 2.342373847961426\n",
            "Epoch: 3, Training Loss: 2.297184467315674\n",
            "Epoch: 3, Training Loss: 2.4618210792541504\n",
            "Epoch: 3, Training Loss: 2.4716949462890625\n",
            "Epoch: 3, Training Loss: 2.7705166339874268\n",
            "Epoch: 3, Training Loss: 2.611717462539673\n",
            "Epoch: 3, Training Loss: 2.369842052459717\n",
            "Epoch: 3, Training Loss: 2.550645589828491\n",
            "Epoch: 3, Training Loss: 2.8974905014038086\n",
            "Epoch: 3, Training Loss: 2.6683132648468018\n",
            "Epoch: 3, Training Loss: 2.675847053527832\n",
            "Epoch: 3, Training Loss: 2.4504637718200684\n",
            "Epoch: 3, Training Loss: 2.4183177947998047\n",
            "Epoch: 3, Training Loss: 2.634474039077759\n",
            "Epoch: 3, Training Loss: 2.3283400535583496\n",
            "Epoch: 3, Training Loss: 2.64898419380188\n",
            "Epoch: 3, Training Loss: 2.598930835723877\n",
            "Epoch: 3, Training Loss: 2.605138063430786\n",
            "Epoch: 3, Training Loss: 2.7360281944274902\n",
            "Epoch: 3, Training Loss: 2.3985846042633057\n",
            "Epoch: 3, Training Loss: 2.5920846462249756\n",
            "Epoch: 3, Training Loss: 2.3597190380096436\n",
            "Epoch: 3, Training Loss: 2.385650396347046\n",
            "Epoch: 3, Training Loss: 2.5060203075408936\n",
            "Epoch: 3, Training Loss: 2.6731889247894287\n",
            "Epoch: 3, Training Loss: 2.133106231689453\n",
            "Epoch: 3, Training Loss: 2.4509811401367188\n",
            "Epoch: 3, Training Loss: 2.6164307594299316\n",
            "Epoch: 3, Training Loss: 2.499807357788086\n",
            "Epoch: 3, Training Loss: 2.4546713829040527\n",
            "Epoch: 3, Training Loss: 2.3573596477508545\n",
            "Epoch: 3, Training Loss: 2.290550947189331\n",
            "Epoch: 3, Training Loss: 2.511767864227295\n",
            "Epoch: 3, Training Loss: 2.5860700607299805\n",
            "Epoch: 3, Training Loss: 2.1796622276306152\n",
            "Epoch: 3, Training Loss: 2.4947593212127686\n",
            "Epoch: 3, Training Loss: 2.555671215057373\n",
            "Epoch: 3, Training Loss: 2.4219937324523926\n",
            "Epoch: 3, Training Loss: 2.6007070541381836\n",
            "Epoch: 3, Training Loss: 2.757014513015747\n",
            "Epoch: 3, Training Loss: 2.357983112335205\n",
            "Epoch: 3, Training Loss: 2.3919150829315186\n",
            "Epoch: 3, Training Loss: 2.570932626724243\n",
            "Epoch: 3, Training Loss: 2.320972204208374\n",
            "Epoch: 3, Training Loss: 2.4123752117156982\n",
            "Epoch: 3, Training Loss: 2.5670602321624756\n",
            "Epoch: 3, Training Loss: 2.575909376144409\n",
            "Epoch: 3, Training Loss: 2.4567511081695557\n",
            "Epoch: 3, Training Loss: 2.393850326538086\n",
            "Epoch: 3, Training Loss: 2.4547486305236816\n",
            "Epoch: 3, Training Loss: 2.5269997119903564\n",
            "Epoch: 3, Training Loss: 2.2917332649230957\n",
            "Epoch: 3, Training Loss: 2.4439010620117188\n",
            "Epoch: 3, Training Loss: 2.544915199279785\n",
            "Epoch: 3, Training Loss: 2.4749670028686523\n",
            "Epoch: 3, Training Loss: 2.1895251274108887\n",
            "Epoch: 3, Training Loss: 2.6218369007110596\n",
            "Epoch: 3, Training Loss: 2.5897130966186523\n",
            "Epoch: 3, Training Loss: 2.490443706512451\n",
            "Epoch: 3, Training Loss: 2.5413806438446045\n",
            "Epoch: 3, Training Loss: 2.4025189876556396\n",
            "Epoch: 3, Training Loss: 2.6657466888427734\n",
            "Epoch: 3, Training Loss: 2.6416006088256836\n",
            "Epoch: 3, Training Loss: 2.4722976684570312\n",
            "Epoch: 3, Training Loss: 2.519735336303711\n",
            "Epoch: 3, Training Loss: 2.3518664836883545\n",
            "Epoch: 3, Training Loss: 2.4021847248077393\n",
            "Epoch: 3, Training Loss: 2.251967191696167\n",
            "Epoch: 3, Training Loss: 2.445770740509033\n",
            "Epoch: 3, Training Loss: 2.316338062286377\n",
            "Epoch: 3, Training Loss: 2.326998472213745\n",
            "Epoch: 3, Training Loss: 2.464932918548584\n",
            "Epoch: 3, Training Loss: 2.5250654220581055\n",
            "Epoch: 3, Training Loss: 2.555655002593994\n",
            "Epoch: 3, Training Loss: 2.6734893321990967\n",
            "Epoch: 3, Training Loss: 2.552145004272461\n",
            "Epoch: 3, Training Loss: 2.539179801940918\n",
            "Epoch: 3, Training Loss: 2.5702931880950928\n",
            "Epoch: 3, Training Loss: 2.3163931369781494\n",
            "Epoch: 3, Training Loss: 2.3095734119415283\n",
            "Epoch: 3, Training Loss: 2.386918783187866\n",
            "Epoch: 3, Training Loss: 2.5709328651428223\n",
            "Epoch: 3, Training Loss: 2.4438836574554443\n",
            "Epoch: 3, Training Loss: 2.609898328781128\n",
            "Epoch: 3, Training Loss: 2.3055763244628906\n",
            "Epoch: 3, Training Loss: 2.431612014770508\n",
            "Epoch: 3, Training Loss: 2.427323341369629\n",
            "Epoch: 3, Training Loss: 2.3027048110961914\n",
            "Epoch: 3, Training Loss: 2.5597851276397705\n",
            "Epoch: 3, Training Loss: 2.340933084487915\n",
            "Epoch: 3, Training Loss: 2.375598669052124\n",
            "Epoch: 3, Training Loss: 2.530834436416626\n",
            "Epoch: 3, Training Loss: 2.4294960498809814\n",
            "Epoch: 3, Training Loss: 2.5337047576904297\n",
            "Epoch: 3, Training Loss: 2.4664297103881836\n",
            "Epoch: 3, Training Loss: 2.5948736667633057\n",
            "Epoch: 3, Training Loss: 2.410205841064453\n",
            "Epoch: 3, Training Loss: 2.435288190841675\n",
            "Epoch: 3, Training Loss: 2.6177055835723877\n",
            "Epoch: 3, Training Loss: 2.204674005508423\n",
            "Epoch: 3, Training Loss: 2.2624127864837646\n",
            "Epoch: 3, Training Loss: 2.396042585372925\n",
            "Epoch: 3, Training Loss: 2.2751245498657227\n",
            "Epoch: 3, Training Loss: 2.2554314136505127\n",
            "Epoch: 3, Training Loss: 2.4994187355041504\n",
            "Epoch: 3, Training Loss: 2.403733491897583\n",
            "Epoch: 3, Training Loss: 2.298380136489868\n",
            "Epoch: 3, Training Loss: 2.3407645225524902\n",
            "Epoch: 3, Training Loss: 2.384413719177246\n",
            "Epoch: 3, Training Loss: 2.5558063983917236\n",
            "Epoch: 3, Training Loss: 2.429917573928833\n",
            "Epoch: 3, Training Loss: 2.602163076400757\n",
            "Epoch: 3, Training Loss: 2.5918681621551514\n",
            "Epoch: 3, Training Loss: 2.537832498550415\n",
            "Epoch: 3, Training Loss: 2.6020500659942627\n",
            "Epoch: 3, Training Loss: 2.3101918697357178\n",
            "Epoch: 3, Training Loss: 2.4532852172851562\n",
            "Epoch: 3, Training Loss: 2.6239659786224365\n",
            "Epoch: 3, Training Loss: 2.518651008605957\n",
            "Epoch: 3, Training Loss: 2.4265286922454834\n",
            "Epoch: 3, Training Loss: 2.672240972518921\n",
            "Epoch: 3, Training Loss: 2.4593048095703125\n",
            "Epoch: 3, Training Loss: 2.639716148376465\n",
            "Epoch: 3, Training Loss: 2.5274736881256104\n",
            "Epoch: 3, Training Loss: 2.4470901489257812\n",
            "Epoch: 3, Training Loss: 2.6744213104248047\n",
            "Epoch: 3, Training Loss: 2.3270230293273926\n",
            "Epoch: 3, Training Loss: 2.35093092918396\n",
            "Epoch: 3, Training Loss: 2.401015043258667\n",
            "Epoch: 3, Training Loss: 2.252136468887329\n",
            "Epoch: 3, Training Loss: 2.4182136058807373\n",
            "Epoch: 3, Training Loss: 2.2650327682495117\n",
            "Epoch: 3, Training Loss: 2.6993813514709473\n",
            "Epoch: 3, Training Loss: 2.6899490356445312\n",
            "Epoch: 3, Training Loss: 2.549708604812622\n",
            "Epoch: 3, Training Loss: 2.749525785446167\n",
            "Epoch: 3, Training Loss: 2.266667366027832\n",
            "Epoch: 3, Training Loss: 2.411813259124756\n",
            "Epoch: 3, Training Loss: 2.3650012016296387\n",
            "Epoch: 3, Training Loss: 2.7285544872283936\n",
            "Epoch: 3, Training Loss: 2.4697020053863525\n",
            "Epoch: 3, Training Loss: 2.616102933883667\n",
            "Epoch: 3, Training Loss: 2.205747365951538\n",
            "Epoch: 3, Training Loss: 2.6339075565338135\n",
            "Epoch: 3, Training Loss: 2.3376121520996094\n",
            "Epoch: 3, Training Loss: 2.3092315196990967\n",
            "Epoch: 3, Training Loss: 2.4267163276672363\n",
            "Epoch: 3, Training Loss: 2.1390068531036377\n",
            "Epoch: 3, Training Loss: 2.4511375427246094\n",
            "Epoch: 3, Training Loss: 2.443002462387085\n",
            "Epoch: 3, Training Loss: 2.5449557304382324\n",
            "Epoch: 3, Training Loss: 2.55439829826355\n",
            "Epoch: 3, Training Loss: 2.5738327503204346\n",
            "Epoch: 3, Training Loss: 2.490973711013794\n",
            "Epoch: 3, Training Loss: 2.4969642162323\n",
            "Epoch: 3, Training Loss: 2.3539791107177734\n",
            "Epoch: 3, Training Loss: 2.434079647064209\n",
            "Epoch: 3, Training Loss: 2.2038142681121826\n",
            "Epoch: 3, Training Loss: 2.528428792953491\n",
            "Epoch: 3, Training Loss: 2.1949872970581055\n",
            "Epoch: 3, Training Loss: 2.4588065147399902\n",
            "Epoch: 3, Training Loss: 2.4155313968658447\n",
            "Epoch: 3, Training Loss: 2.364973783493042\n",
            "Epoch: 3, Training Loss: 2.1216635704040527\n",
            "Epoch: 3, Training Loss: 2.671189546585083\n",
            "Epoch: 3, Training Loss: 2.485044002532959\n",
            "Epoch: 3, Training Loss: 2.5310025215148926\n",
            "Epoch: 3, Training Loss: 2.2738566398620605\n",
            "Epoch: 3, Training Loss: 2.5537829399108887\n",
            "Epoch: 3, Training Loss: 2.3056538105010986\n",
            "Epoch: 3, Training Loss: 2.3171722888946533\n",
            "Epoch: 3, Training Loss: 2.1626482009887695\n",
            "Epoch: 3, Training Loss: 2.4470300674438477\n",
            "Epoch: 3, Training Loss: 2.3426902294158936\n",
            "Epoch: 3, Training Loss: 2.2595434188842773\n",
            "Epoch: 3, Training Loss: 2.464909553527832\n",
            "Epoch: 3, Training Loss: 2.7113640308380127\n",
            "Epoch: 3, Training Loss: 2.6185834407806396\n",
            "Epoch: 3, Training Loss: 2.5536949634552\n",
            "Epoch: 3, Training Loss: 2.378434658050537\n",
            "Epoch: 3, Training Loss: 2.239575147628784\n",
            "Epoch: 3, Training Loss: 2.6789088249206543\n",
            "Epoch: 3, Training Loss: 2.1229088306427\n",
            "Epoch: 3, Training Loss: 2.2989449501037598\n",
            "Epoch: 3, Training Loss: 2.500715970993042\n",
            "Epoch: 3, Training Loss: 2.613046884536743\n",
            "Epoch: 3, Training Loss: 2.6577517986297607\n",
            "Epoch: 3, Training Loss: 2.3838329315185547\n",
            "Epoch: 3, Training Loss: 2.396043300628662\n",
            "Epoch: 3, Training Loss: 2.2353413105010986\n",
            "Epoch: 3, Training Loss: 2.4981319904327393\n",
            "Epoch: 3, Training Loss: 2.3093721866607666\n",
            "Epoch: 3, Training Loss: 2.333394765853882\n",
            "Epoch: 3, Training Loss: 2.6350648403167725\n",
            "Epoch: 3, Training Loss: 2.8691155910491943\n",
            "Epoch: 3, Training Loss: 2.3520667552948\n",
            "Epoch: 3, Training Loss: 2.425900459289551\n",
            "Epoch: 3, Training Loss: 2.260103702545166\n",
            "Epoch: 3, Training Loss: 2.437788486480713\n",
            "Epoch: 3, Training Loss: 2.4216558933258057\n",
            "Epoch: 3, Training Loss: 2.5382351875305176\n",
            "Epoch: 3, Training Loss: 2.4353413581848145\n",
            "Epoch: 3, Training Loss: 2.3927900791168213\n",
            "Epoch: 3, Training Loss: 2.4260571002960205\n",
            "Epoch: 3, Training Loss: 2.3474881649017334\n",
            "Epoch: 3, Training Loss: 2.3851165771484375\n",
            "Epoch: 3, Training Loss: 2.670098304748535\n",
            "Epoch: 3, Training Loss: 2.76094388961792\n",
            "Epoch: 3, Training Loss: 2.317640542984009\n",
            "Epoch: 3, Training Loss: 2.550855875015259\n",
            "Epoch: 3, Training Loss: 2.670694351196289\n",
            "Epoch: 3, Training Loss: 2.6227195262908936\n",
            "Epoch: 3, Training Loss: 2.495316982269287\n",
            "Epoch: 3, Training Loss: 2.3072543144226074\n",
            "Epoch: 3, Training Loss: 2.499739170074463\n",
            "Epoch: 3, Training Loss: 2.6721975803375244\n",
            "Epoch: 3, Training Loss: 2.445627212524414\n",
            "Epoch: 3, Training Loss: 2.1177866458892822\n",
            "Epoch: 3, Training Loss: 2.3304617404937744\n",
            "Epoch: 3, Training Loss: 2.533390998840332\n",
            "Epoch: 3, Training Loss: 2.482017755508423\n",
            "Epoch: 3, Training Loss: 2.5493593215942383\n",
            "Epoch: 3, Training Loss: 2.4487359523773193\n",
            "Epoch: 3, Training Loss: 2.3090949058532715\n",
            "Epoch: 3, Training Loss: 2.459289789199829\n",
            "Epoch: 3, Training Loss: 2.4708621501922607\n",
            "Epoch: 3, Training Loss: 2.4163174629211426\n",
            "Epoch: 3, Training Loss: 2.3112709522247314\n",
            "Epoch: 3, Training Loss: 2.538240432739258\n",
            "Epoch: 3, Training Loss: 2.2955236434936523\n",
            "Epoch: 3, Training Loss: 2.5580897331237793\n",
            "Epoch: 3, Training Loss: 2.5681533813476562\n",
            "Epoch: 3, Training Loss: 2.631392002105713\n",
            "Epoch: 3, Training Loss: 2.534959554672241\n",
            "Epoch: 3, Training Loss: 2.2067010402679443\n",
            "Epoch: 3, Training Loss: 2.2862393856048584\n",
            "Epoch: 3, Training Loss: 2.5656752586364746\n",
            "Epoch: 3, Training Loss: 2.656764507293701\n",
            "Epoch: 3, Training Loss: 2.4417972564697266\n",
            "Epoch: 3, Training Loss: 2.65423846244812\n",
            "Epoch: 3, Training Loss: 2.484800100326538\n",
            "Epoch: 3, Training Loss: 2.548305034637451\n",
            "Epoch: 3, Training Loss: 2.5356285572052\n",
            "Epoch: 3, Training Loss: 2.7479279041290283\n",
            "Epoch: 3, Training Loss: 2.3301775455474854\n",
            "Epoch: 3, Training Loss: 2.581209182739258\n",
            "Epoch: 3, Training Loss: 2.226606607437134\n",
            "Epoch: 3, Training Loss: 2.345459222793579\n",
            "Epoch: 3, Training Loss: 2.6063501834869385\n",
            "Epoch: 3, Training Loss: 2.6431870460510254\n",
            "Epoch: 3, Training Loss: 2.4734585285186768\n",
            "Epoch: 3, Training Loss: 2.4888861179351807\n",
            "Epoch: 3, Training Loss: 2.5552361011505127\n",
            "Epoch: 3, Training Loss: 2.5630197525024414\n",
            "Epoch: 3, Training Loss: 2.702538013458252\n",
            "Epoch: 3, Training Loss: 2.2200260162353516\n",
            "Epoch: 3, Training Loss: 2.4163169860839844\n",
            "Epoch: 3, Training Loss: 2.5788891315460205\n",
            "Epoch: 3, Training Loss: 2.4756853580474854\n",
            "Epoch: 3, Training Loss: 2.603034019470215\n",
            "Epoch: 3, Training Loss: 2.6296565532684326\n",
            "Epoch: 3, Training Loss: 2.613168954849243\n",
            "Epoch: 3, Training Loss: 2.3785128593444824\n",
            "Epoch: 3, Training Loss: 2.588041067123413\n",
            "Epoch: 3, Training Loss: 2.327794075012207\n",
            "Epoch: 3, Training Loss: 2.5876076221466064\n",
            "Epoch: 3, Training Loss: 2.292576789855957\n",
            "Epoch: 3, Training Loss: 2.5491647720336914\n",
            "Epoch: 3, Training Loss: 2.4428868293762207\n",
            "Epoch: 3, Training Loss: 2.3822555541992188\n",
            "Epoch: 3, Training Loss: 2.316863775253296\n",
            "Epoch: 3, Training Loss: 2.4906365871429443\n",
            "Epoch: 3, Training Loss: 2.827230215072632\n",
            "Epoch: 3, Training Loss: 2.5740180015563965\n",
            "Epoch: 3, Training Loss: 2.3665528297424316\n",
            "Epoch: 3, Training Loss: 2.6204562187194824\n",
            "Epoch: 3, Training Loss: 2.1612584590911865\n",
            "Epoch: 3, Training Loss: 2.3667690753936768\n",
            "Epoch: 3, Training Loss: 2.541360378265381\n",
            "Epoch: 3, Training Loss: 2.4707508087158203\n",
            "Epoch: 3, Training Loss: 2.317075729370117\n",
            "Epoch: 3, Training Loss: 2.4079010486602783\n",
            "Epoch: 3, Training Loss: 2.5254170894622803\n",
            "Epoch: 3, Training Loss: 2.537897825241089\n",
            "Epoch: 3, Training Loss: 2.5218167304992676\n",
            "Epoch: 3, Training Loss: 2.6890785694122314\n",
            "Epoch: 3, Training Loss: 2.5077714920043945\n",
            "Epoch: 3, Training Loss: 2.36491060256958\n",
            "Epoch: 3, Training Loss: 2.365123987197876\n",
            "Epoch: 3, Training Loss: 2.521047830581665\n",
            "Epoch: 3, Training Loss: 2.281536817550659\n",
            "Epoch: 3, Training Loss: 2.508718252182007\n",
            "Epoch: 3, Training Loss: 2.374494791030884\n",
            "Epoch: 3, Training Loss: 2.628615617752075\n",
            "Epoch: 3, Training Loss: 2.2340173721313477\n",
            "Epoch: 3, Training Loss: 2.1850481033325195\n",
            "Epoch: 3, Training Loss: 2.4564805030822754\n",
            "Epoch: 3, Training Loss: 2.3827173709869385\n",
            "Epoch: 3, Training Loss: 2.8294577598571777\n",
            "Epoch: 3, Training Loss: 2.55946683883667\n",
            "Epoch: 3, Training Loss: 2.290789842605591\n",
            "Epoch: 3, Training Loss: 2.442960500717163\n",
            "Epoch: 3, Training Loss: 2.5509252548217773\n",
            "Epoch: 3, Training Loss: 2.409667491912842\n",
            "Epoch: 3, Training Loss: 2.9045889377593994\n",
            "Epoch: 3, Training Loss: 2.5737154483795166\n",
            "Epoch: 3, Training Loss: 2.3256382942199707\n",
            "Epoch: 3, Training Loss: 2.2809128761291504\n",
            "Epoch: 3, Training Loss: 2.2498443126678467\n",
            "Epoch: 3, Training Loss: 2.4565234184265137\n",
            "Epoch: 3, Training Loss: 2.2767646312713623\n",
            "Epoch: 3, Training Loss: 2.637101173400879\n",
            "Epoch: 3, Training Loss: 2.5556235313415527\n",
            "Epoch: 3, Training Loss: 2.270703077316284\n",
            "Epoch: 3, Training Loss: 2.385040283203125\n",
            "Epoch: 3, Training Loss: 2.391740322113037\n",
            "Epoch: 3, Training Loss: 2.3936524391174316\n",
            "Epoch: 3, Training Loss: 2.1951093673706055\n",
            "Epoch: 3, Training Loss: 2.1912918090820312\n",
            "Epoch: 3, Training Loss: 2.5052809715270996\n",
            "Epoch: 3, Training Loss: 2.2563440799713135\n",
            "Epoch: 3, Training Loss: 2.6932759284973145\n",
            "Epoch: 3, Training Loss: 2.538867950439453\n",
            "Epoch: 3, Training Loss: 2.296030282974243\n",
            "Epoch: 3, Training Loss: 2.198986768722534\n",
            "Epoch: 3, Training Loss: 2.2151577472686768\n",
            "Epoch: 3, Training Loss: 2.0607705116271973\n",
            "Epoch: 3, Training Loss: 2.363178014755249\n",
            "Epoch: 3, Training Loss: 2.616014242172241\n",
            "Epoch: 3, Training Loss: 2.745971918106079\n",
            "Epoch: 3, Training Loss: 2.746793270111084\n",
            "Epoch: 3, Training Loss: 2.29522442817688\n",
            "Epoch: 3, Training Loss: 2.8007619380950928\n",
            "Epoch: 3, Training Loss: 2.457141876220703\n",
            "Epoch: 3, Training Loss: 2.409660816192627\n",
            "Epoch: 3, Training Loss: 2.213512659072876\n",
            "Epoch: 3, Training Loss: 2.598605155944824\n",
            "Epoch: 3, Training Loss: 2.2985124588012695\n",
            "Epoch: 3, Training Loss: 2.4660916328430176\n",
            "Epoch: 3, Training Loss: 2.6834893226623535\n",
            "Epoch: 3, Training Loss: 2.356688976287842\n",
            "Epoch: 3, Training Loss: 2.590607166290283\n",
            "Epoch: 3, Training Loss: 2.1851353645324707\n",
            "Epoch: 3, Training Loss: 2.519498348236084\n",
            "Epoch: 3, Training Loss: 2.321861505508423\n",
            "Epoch: 3, Training Loss: 2.318746328353882\n",
            "Epoch: 3, Training Loss: 2.7381503582000732\n",
            "Epoch: 3, Training Loss: 2.551250457763672\n",
            "Epoch: 3, Training Loss: 2.2960283756256104\n",
            "Epoch: 3, Training Loss: 2.2803874015808105\n",
            "Epoch: 3, Training Loss: 2.70420503616333\n",
            "Epoch: 3, Training Loss: 2.56085467338562\n",
            "Epoch: 3, Training Loss: 2.5659239292144775\n",
            "Epoch: 3, Training Loss: 2.3076765537261963\n",
            "Epoch: 3, Training Loss: 2.178271770477295\n",
            "Epoch: 3, Training Loss: 2.625706911087036\n",
            "Epoch: 3, Training Loss: 2.4179513454437256\n",
            "Epoch: 3, Training Loss: 2.691531181335449\n",
            "Epoch: 3, Training Loss: 2.755568265914917\n",
            "Epoch: 3, Training Loss: 2.4690656661987305\n",
            "Epoch: 3, Training Loss: 2.5176753997802734\n",
            "Epoch: 3, Training Loss: 2.3505985736846924\n",
            "Epoch: 3, Training Loss: 2.6565184593200684\n",
            "Epoch: 3, Training Loss: 2.1168086528778076\n",
            "Epoch: 3, Training Loss: 2.6872689723968506\n",
            "Epoch: 3, Training Loss: 2.451747417449951\n",
            "Epoch: 3, Training Loss: 2.254786729812622\n",
            "Epoch: 3, Training Loss: 2.1336894035339355\n",
            "Epoch: 3, Training Loss: 2.67763614654541\n",
            "Epoch: 3, Training Loss: 2.561596393585205\n",
            "Epoch: 3, Training Loss: 2.236849069595337\n",
            "Epoch: 3, Training Loss: 2.2631611824035645\n",
            "Epoch: 3, Training Loss: 2.545431137084961\n",
            "Epoch: 3, Training Loss: 2.564810276031494\n",
            "Epoch: 3, Training Loss: 2.3768203258514404\n",
            "Epoch: 3, Training Loss: 2.4114935398101807\n",
            "Epoch: 3, Training Loss: 2.3719112873077393\n",
            "Epoch: 3, Training Loss: 2.5546200275421143\n",
            "Epoch: 3, Training Loss: 2.408975839614868\n",
            "Epoch: 3, Training Loss: 2.3470876216888428\n",
            "Epoch: 3, Training Loss: 2.5565996170043945\n",
            "Epoch: 3, Training Loss: 2.558293104171753\n",
            "Epoch: 3, Training Loss: 2.497643232345581\n",
            "Epoch: 3, Training Loss: 2.1602907180786133\n",
            "Epoch: 3, Training Loss: 2.101287841796875\n",
            "Epoch: 3, Training Loss: 2.556304454803467\n",
            "Epoch: 3, Training Loss: 2.3104894161224365\n",
            "Epoch: 3, Training Loss: 2.525428295135498\n",
            "Epoch: 3, Training Loss: 2.3417251110076904\n",
            "Epoch: 3, Training Loss: 2.4953768253326416\n",
            "Epoch: 3, Training Loss: 2.5283117294311523\n",
            "Epoch: 3, Training Loss: 2.381765842437744\n",
            "Epoch: 3, Training Loss: 2.124605655670166\n",
            "Epoch: 3, Training Loss: 2.5630455017089844\n",
            "Epoch: 3, Training Loss: 2.6064441204071045\n",
            "Epoch: 3, Training Loss: 2.5080461502075195\n",
            "Epoch: 3, Training Loss: 2.517148017883301\n",
            "Epoch: 3, Training Loss: 2.4133646488189697\n",
            "Epoch: 3, Training Loss: 2.5581154823303223\n",
            "Epoch: 3, Training Loss: 2.5269570350646973\n",
            "Epoch: 3, Training Loss: 2.265665292739868\n",
            "Epoch: 3, Training Loss: 2.4895408153533936\n",
            "Epoch: 3, Training Loss: 2.2042829990386963\n",
            "Epoch: 3, Training Loss: 2.510063409805298\n",
            "Epoch: 3, Training Loss: 2.2415223121643066\n",
            "Epoch: 3, Training Loss: 2.671459674835205\n",
            "Epoch: 3, Training Loss: 2.383854866027832\n",
            "Epoch: 3, Training Loss: 2.470508575439453\n",
            "Epoch: 3, Training Loss: 2.2744531631469727\n",
            "Epoch: 3, Training Loss: 2.4173779487609863\n",
            "Epoch: 3, Training Loss: 2.2627315521240234\n",
            "Epoch: 3, Training Loss: 2.2976458072662354\n",
            "Epoch: 3, Training Loss: 2.3245127201080322\n",
            "Epoch: 3, Training Loss: 2.527763605117798\n",
            "Epoch: 3, Training Loss: 2.3928205966949463\n",
            "Epoch: 3, Training Loss: 2.271419048309326\n",
            "Epoch: 3, Training Loss: 2.348675012588501\n",
            "Epoch: 3, Training Loss: 2.8813886642456055\n",
            "Epoch: 3, Training Loss: 2.4789836406707764\n",
            "Epoch: 3, Training Loss: 2.469351053237915\n",
            "Epoch: 3, Training Loss: 2.7041361331939697\n",
            "Epoch: 3, Training Loss: 2.709123373031616\n",
            "Epoch: 3, Training Loss: 2.569547176361084\n",
            "Epoch: 3, Training Loss: 2.1579678058624268\n",
            "Epoch: 3, Training Loss: 2.4659483432769775\n",
            "Epoch: 3, Training Loss: 2.230548143386841\n",
            "Epoch: 3, Training Loss: 2.4632227420806885\n",
            "Epoch: 3, Training Loss: 2.385378360748291\n",
            "Epoch: 3, Training Loss: 2.399507761001587\n",
            "Epoch: 3, Training Loss: 2.29618239402771\n",
            "Epoch: 3, Training Loss: 2.669461488723755\n",
            "Epoch: 3, Training Loss: 2.327622175216675\n",
            "Epoch: 3, Training Loss: 2.4313182830810547\n",
            "Epoch: 3, Training Loss: 2.3939249515533447\n",
            "Epoch: 3, Training Loss: 2.6110000610351562\n",
            "Epoch: 3, Training Loss: 2.7115418910980225\n",
            "Epoch: 3, Training Loss: 2.302022695541382\n",
            "Epoch: 3, Training Loss: 2.339160919189453\n",
            "Epoch: 3, Training Loss: 2.35896635055542\n",
            "Epoch: 3, Training Loss: 2.6583330631256104\n",
            "Epoch: 3, Training Loss: 2.2956032752990723\n",
            "Epoch: 3, Training Loss: 2.206989049911499\n",
            "Epoch: 3, Training Loss: 2.3534369468688965\n",
            "Epoch: 3, Training Loss: 2.424513101577759\n",
            "Epoch: 3, Training Loss: 2.25612211227417\n",
            "Epoch: 3, Training Loss: 2.4814298152923584\n",
            "Epoch: 3, Training Loss: 2.5933003425598145\n",
            "Epoch: 3, Training Loss: 2.493986129760742\n",
            "Epoch: 3, Training Loss: 2.358729600906372\n",
            "Epoch: 3, Training Loss: 2.3029820919036865\n",
            "Epoch: 3, Training Loss: 2.550051689147949\n",
            "Epoch: 3, Training Loss: 2.4870028495788574\n",
            "Epoch: 3, Training Loss: 2.391446113586426\n",
            "Epoch: 3, Training Loss: 2.324110507965088\n",
            "Epoch: 3, Training Loss: 2.354490280151367\n",
            "Epoch: 3, Training Loss: 2.280837297439575\n",
            "Epoch: 3, Training Loss: 2.6037094593048096\n",
            "Epoch: 3, Training Loss: 2.405588150024414\n",
            "Epoch: 3, Training Loss: 2.392751932144165\n",
            "Epoch: 3, Training Loss: 2.0910727977752686\n",
            "Epoch: 3, Training Loss: 2.2929911613464355\n",
            "Epoch: 3, Training Loss: 2.339515209197998\n",
            "Epoch: 3, Training Loss: 2.4235124588012695\n",
            "Epoch: 3, Training Loss: 2.5469024181365967\n",
            "Epoch: 3, Training Loss: 2.3534562587738037\n",
            "Epoch: 3, Training Loss: 2.3546488285064697\n",
            "Epoch: 3, Training Loss: 2.4098217487335205\n",
            "Epoch: 3, Training Loss: 2.5706050395965576\n",
            "Epoch: 3, Training Loss: 2.427560329437256\n",
            "Epoch: 3, Training Loss: 2.37975811958313\n",
            "Epoch: 3, Training Loss: 2.614046335220337\n",
            "Epoch: 3, Training Loss: 2.4587504863739014\n",
            "Epoch: 3, Training Loss: 2.2488203048706055\n",
            "Epoch: 3, Training Loss: 2.4276888370513916\n",
            "Epoch: 3, Training Loss: 2.3067989349365234\n",
            "Epoch: 3, Training Loss: 2.6650307178497314\n",
            "Epoch: 3, Training Loss: 2.4991235733032227\n",
            "Epoch: 3, Training Loss: 2.68704891204834\n",
            "Epoch: 3, Training Loss: 2.7306628227233887\n",
            "Epoch: 3, Training Loss: 2.3044276237487793\n",
            "Epoch: 3, Training Loss: 2.63763689994812\n",
            "Epoch: 3, Training Loss: 2.3389265537261963\n",
            "Epoch: 3, Training Loss: 2.604768991470337\n",
            "Epoch: 3, Training Loss: 2.5480754375457764\n",
            "Epoch: 3, Training Loss: 2.588850259780884\n",
            "Epoch: 3, Training Loss: 2.3585448265075684\n",
            "Epoch: 3, Training Loss: 2.5670268535614014\n",
            "Epoch: 3, Training Loss: 2.3991196155548096\n",
            "Epoch: 3, Training Loss: 2.3069989681243896\n",
            "Epoch: 3, Training Loss: 2.606365442276001\n",
            "Epoch: 3, Training Loss: 2.2641537189483643\n",
            "Epoch: 3, Training Loss: 2.6024205684661865\n",
            "Epoch: 3, Training Loss: 2.6081955432891846\n",
            "Epoch: 3, Training Loss: 2.316899299621582\n",
            "Epoch: 3, Training Loss: 2.536322593688965\n",
            "Epoch: 3, Training Loss: 2.391010046005249\n",
            "Epoch: 3, Training Loss: 2.1483750343322754\n",
            "Epoch: 3, Training Loss: 2.058218240737915\n",
            "Epoch: 3, Training Loss: 2.613598108291626\n",
            "Epoch: 3, Training Loss: 2.2014713287353516\n",
            "Epoch: 3, Training Loss: 2.7176737785339355\n",
            "Epoch: 3, Training Loss: 2.3509716987609863\n",
            "Epoch: 3, Training Loss: 2.377959728240967\n",
            "Epoch: 3, Training Loss: 2.1346049308776855\n",
            "Epoch: 3, Training Loss: 2.1573033332824707\n",
            "Epoch: 3, Training Loss: 2.217763662338257\n",
            "Epoch: 3, Training Loss: 2.5646116733551025\n",
            "Epoch: 3, Training Loss: 2.5223114490509033\n",
            "Epoch: 3, Training Loss: 2.5471105575561523\n",
            "Epoch: 3, Training Loss: 2.5575623512268066\n",
            "Epoch: 3, Training Loss: 2.760232448577881\n",
            "Epoch: 3, Training Loss: 2.3806440830230713\n",
            "Epoch: 3, Training Loss: 2.3395330905914307\n",
            "Epoch: 3, Training Loss: 2.046970844268799\n",
            "Epoch: 3, Training Loss: 2.370901346206665\n",
            "Epoch: 3, Training Loss: 2.2261013984680176\n",
            "Epoch: 3, Training Loss: 2.5078535079956055\n",
            "Epoch: 3, Training Loss: 2.6304492950439453\n",
            "Epoch: 3, Training Loss: 2.3903512954711914\n",
            "Epoch: 3, Training Loss: 2.5633740425109863\n",
            "Epoch: 3, Training Loss: 2.521660804748535\n",
            "Epoch: 3, Training Loss: 2.361366033554077\n",
            "Epoch: 3, Training Loss: 2.26242995262146\n",
            "Epoch: 3, Training Loss: 2.3825385570526123\n",
            "Epoch: 3, Training Loss: 2.253783702850342\n",
            "Epoch: 3, Training Loss: 2.373652696609497\n",
            "Epoch: 3, Training Loss: 2.4126312732696533\n",
            "Epoch: 3, Training Loss: 2.3762147426605225\n",
            "Epoch: 3, Training Loss: 2.339395046234131\n",
            "Epoch: 3, Training Loss: 2.3202133178710938\n",
            "Epoch: 3, Training Loss: 2.351149082183838\n",
            "Epoch: 3, Training Loss: 2.5108654499053955\n",
            "Epoch: 3, Training Loss: 2.362286329269409\n",
            "Epoch: 3, Training Loss: 2.279784917831421\n",
            "Epoch: 3, Training Loss: 2.3319849967956543\n",
            "Epoch: 3, Training Loss: 2.36845326423645\n",
            "Epoch: 3, Training Loss: 2.336113929748535\n",
            "Epoch: 3, Training Loss: 2.5545663833618164\n",
            "Epoch: 3, Training Loss: 2.5157082080841064\n",
            "Epoch: 3, Training Loss: 2.2006728649139404\n",
            "Epoch: 3, Training Loss: 2.427647590637207\n",
            "Epoch: 3, Training Loss: 2.2147092819213867\n",
            "Epoch: 3, Training Loss: 2.2637882232666016\n",
            "Epoch: 3, Training Loss: 2.3662843704223633\n",
            "Epoch: 3, Training Loss: 2.2842369079589844\n",
            "Epoch: 3, Training Loss: 2.5263543128967285\n",
            "Epoch: 3, Training Loss: 2.271589517593384\n",
            "Epoch: 3, Training Loss: 2.338142156600952\n",
            "Epoch: 3, Training Loss: 2.6452012062072754\n",
            "Epoch: 3, Training Loss: 2.359299421310425\n",
            "Epoch: 3, Training Loss: 2.6261162757873535\n",
            "Epoch: 3, Training Loss: 2.2618422508239746\n",
            "Epoch: 3, Training Loss: 2.274200677871704\n",
            "Epoch: 3, Training Loss: 2.2917394638061523\n",
            "Epoch: 3, Training Loss: 2.7331526279449463\n",
            "Epoch: 3, Training Loss: 2.2555394172668457\n",
            "Epoch: 3, Training Loss: 2.6118319034576416\n",
            "Epoch: 3, Training Loss: 2.379795551300049\n",
            "Epoch: 3, Training Loss: 2.5000081062316895\n",
            "Epoch: 3, Training Loss: 2.5865492820739746\n",
            "Epoch: 3, Training Loss: 2.240100145339966\n",
            "Epoch: 3, Training Loss: 2.577436923980713\n",
            "Epoch: 3, Training Loss: 2.597085952758789\n",
            "Epoch: 3, Training Loss: 2.4379994869232178\n",
            "Epoch: 3, Training Loss: 2.6549506187438965\n",
            "Epoch: 3, Training Loss: 2.5358500480651855\n",
            "Epoch: 3, Training Loss: 2.4458606243133545\n",
            "Epoch: 3, Training Loss: 2.520498275756836\n",
            "Epoch: 3, Training Loss: 2.4426653385162354\n",
            "Epoch: 3, Training Loss: 2.5046401023864746\n",
            "Epoch: 3, Training Loss: 2.423196792602539\n",
            "Epoch: 3, Training Loss: 2.3330864906311035\n",
            "Epoch: 3, Training Loss: 2.3590383529663086\n",
            "Epoch: 3, Training Loss: 2.4555256366729736\n",
            "Epoch: 3, Training Loss: 2.3466432094573975\n",
            "Epoch: 3, Training Loss: 2.505582332611084\n",
            "Epoch: 3, Training Loss: 2.3867342472076416\n",
            "Epoch: 3, Training Loss: 2.2890260219573975\n",
            "Epoch: 3, Training Loss: 2.3129191398620605\n",
            "Epoch: 3, Training Loss: 2.199188232421875\n",
            "Epoch: 3, Training Loss: 2.1909725666046143\n",
            "Epoch: 3, Training Loss: 2.5764665603637695\n",
            "Epoch: 3, Training Loss: 2.34102201461792\n",
            "Epoch: 3, Training Loss: 2.3199987411499023\n",
            "Epoch: 3, Training Loss: 2.5848114490509033\n",
            "Epoch: 3, Training Loss: 2.365166425704956\n",
            "Epoch: 3, Training Loss: 2.5997159481048584\n",
            "Epoch: 3, Training Loss: 2.597374439239502\n",
            "Epoch: 3, Training Loss: 2.1560239791870117\n",
            "Epoch: 3, Training Loss: 2.453644275665283\n",
            "Epoch: 3, Training Loss: 2.364441394805908\n",
            "Epoch: 3, Training Loss: 2.1667675971984863\n",
            "Epoch: 3, Training Loss: 2.4750096797943115\n",
            "Epoch: 3, Training Loss: 2.33160400390625\n",
            "Epoch: 3, Training Loss: 2.355029582977295\n",
            "Epoch: 3, Training Loss: 2.3874082565307617\n",
            "Epoch: 3, Training Loss: 2.757298231124878\n",
            "Epoch: 3, Training Loss: 2.3884003162384033\n",
            "Epoch: 3, Training Loss: 2.687711477279663\n",
            "Epoch: 3, Training Loss: 2.3461153507232666\n",
            "Epoch: 3, Training Loss: 2.440704584121704\n",
            "Epoch: 3, Training Loss: 2.545177459716797\n",
            "Epoch: 3, Training Loss: 2.3770482540130615\n",
            "Epoch: 3, Training Loss: 2.4541752338409424\n",
            "Epoch: 3, Training Loss: 2.164409875869751\n",
            "Epoch: 3, Training Loss: 2.3550777435302734\n",
            "Epoch: 3, Training Loss: 2.4630751609802246\n",
            "Epoch: 3, Training Loss: 2.3358845710754395\n",
            "Epoch: 3, Training Loss: 2.232553720474243\n",
            "Epoch: 3, Training Loss: 2.8071353435516357\n",
            "Epoch: 3, Training Loss: 2.492194414138794\n",
            "Epoch: 3, Training Loss: 2.719674825668335\n",
            "Epoch: 3, Training Loss: 2.3975367546081543\n",
            "Epoch: 3, Training Loss: 2.3841631412506104\n",
            "Epoch: 3, Training Loss: 2.2029500007629395\n",
            "Epoch: 3, Training Loss: 2.4149720668792725\n",
            "Epoch: 3, Training Loss: 2.3664653301239014\n",
            "Epoch: 3, Training Loss: 2.428532838821411\n",
            "Epoch: 3, Training Loss: 2.1046535968780518\n",
            "Epoch: 3, Training Loss: 2.3301503658294678\n",
            "Epoch: 3, Training Loss: 2.4952099323272705\n",
            "Epoch: 3, Training Loss: 2.7231571674346924\n",
            "Epoch: 3, Training Loss: 2.5673067569732666\n",
            "Epoch: 3, Training Loss: 2.355114698410034\n",
            "Epoch: 3, Training Loss: 2.3435728549957275\n",
            "Epoch: 3, Training Loss: 2.29384446144104\n",
            "Epoch: 3, Training Loss: 2.305901050567627\n",
            "Epoch: 3, Training Loss: 2.272590160369873\n",
            "Epoch: 3, Training Loss: 2.4056787490844727\n",
            "Epoch: 3, Training Loss: 2.5210325717926025\n",
            "Epoch: 3, Training Loss: 2.4925448894500732\n",
            "Epoch: 3, Training Loss: 2.37341046333313\n",
            "Epoch: 3, Training Loss: 2.584425926208496\n",
            "Epoch: 3, Training Loss: 2.542874574661255\n",
            "Epoch: 3, Training Loss: 2.180323600769043\n",
            "Epoch: 3, Training Loss: 2.273275375366211\n",
            "Epoch: 3, Training Loss: 2.4620726108551025\n",
            "Epoch: 3, Training Loss: 2.536118984222412\n",
            "Epoch: 3, Training Loss: 2.3485403060913086\n",
            "Epoch: 3, Training Loss: 2.6080214977264404\n",
            "Epoch: 3, Training Loss: 2.6393392086029053\n",
            "Epoch: 3, Training Loss: 2.356811285018921\n",
            "Epoch: 3, Training Loss: 2.345196485519409\n",
            "Epoch: 3, Training Loss: 2.4021260738372803\n",
            "Epoch: 3, Training Loss: 2.3393702507019043\n",
            "Epoch: 3, Training Loss: 2.3522963523864746\n",
            "Epoch: 3, Training Loss: 2.1181118488311768\n",
            "Epoch: 3, Training Loss: 2.510941743850708\n",
            "Epoch: 3, Training Loss: 2.3034775257110596\n",
            "Epoch: 3, Training Loss: 2.2550244331359863\n",
            "Epoch: 3, Training Loss: 2.4707868099212646\n",
            "Epoch: 3, Training Loss: 2.260120391845703\n",
            "Epoch: 3, Training Loss: 2.174771785736084\n",
            "Epoch: 3, Training Loss: 2.350234031677246\n",
            "Epoch: 3, Training Loss: 2.4712460041046143\n",
            "Epoch: 3, Training Loss: 2.2331457138061523\n",
            "Epoch: 3, Training Loss: 2.406304121017456\n",
            "Epoch: 3, Training Loss: 2.4905693531036377\n",
            "Epoch: 3, Training Loss: 2.4459550380706787\n",
            "Epoch: 3, Training Loss: 2.3683595657348633\n",
            "Epoch: 3, Training Loss: 2.5806381702423096\n",
            "Epoch: 3, Training Loss: 2.5687835216522217\n",
            "Epoch: 3, Training Loss: 2.479830741882324\n",
            "Epoch: 3, Training Loss: 2.456568479537964\n",
            "Epoch: 3, Training Loss: 2.3117241859436035\n",
            "Epoch: 3, Training Loss: 2.2747466564178467\n",
            "Epoch: 3, Training Loss: 2.796639919281006\n",
            "Epoch: 3, Training Loss: 2.5050885677337646\n",
            "Epoch: 3, Training Loss: 2.3638763427734375\n",
            "Epoch: 3, Training Loss: 2.5383667945861816\n",
            "Epoch: 3, Training Loss: 2.631648063659668\n",
            "Epoch: 3, Training Loss: 2.462928295135498\n",
            "Epoch: 3, Training Loss: 2.131286382675171\n",
            "Epoch: 3, Training Loss: 2.2231462001800537\n",
            "Epoch: 3, Training Loss: 2.316408157348633\n",
            "Epoch: 3, Training Loss: 2.358147382736206\n",
            "Epoch: 3, Training Loss: 2.698091983795166\n",
            "Epoch: 3, Training Loss: 2.6161625385284424\n",
            "Epoch: 3, Training Loss: 2.5359082221984863\n",
            "Epoch: 3, Training Loss: 2.425575017929077\n",
            "Epoch: 3, Training Loss: 2.1189072132110596\n",
            "Epoch: 3, Training Loss: 2.320737600326538\n",
            "Epoch: 3, Training Loss: 2.4725446701049805\n",
            "Epoch: 3, Training Loss: 2.362283706665039\n",
            "Epoch: 3, Training Loss: 2.5342483520507812\n",
            "Epoch: 3, Training Loss: 2.5985257625579834\n",
            "Epoch: 3, Training Loss: 2.230806827545166\n",
            "Epoch: 3, Training Loss: 2.3952109813690186\n",
            "Epoch: 3, Training Loss: 2.4168407917022705\n",
            "Epoch: 3, Training Loss: 2.561351776123047\n",
            "Epoch: 3, Training Loss: 2.364366054534912\n",
            "Epoch: 3, Training Loss: 2.331005811691284\n",
            "Epoch: 3, Training Loss: 2.328779935836792\n",
            "Epoch: 3, Training Loss: 2.4621474742889404\n",
            "Epoch: 3, Training Loss: 2.3946032524108887\n",
            "Epoch: 3, Training Loss: 2.4187171459198\n",
            "Epoch: 3, Training Loss: 2.456409454345703\n",
            "Epoch: 3, Training Loss: 2.3251383304595947\n",
            "Epoch: 3, Training Loss: 2.227893590927124\n",
            "Epoch: 3, Training Loss: 2.528735399246216\n",
            "Epoch: 3, Training Loss: 2.1691370010375977\n",
            "Epoch: 3, Training Loss: 2.3169174194335938\n",
            "Epoch: 3, Training Loss: 2.5396084785461426\n",
            "Epoch: 3, Training Loss: 2.690849542617798\n",
            "Epoch: 3, Training Loss: 2.701215982437134\n",
            "Epoch: 3, Training Loss: 2.200178623199463\n",
            "Epoch: 3, Training Loss: 2.488743305206299\n",
            "Epoch: 3, Training Loss: 2.306380271911621\n",
            "Epoch: 3, Training Loss: 2.5153391361236572\n",
            "Epoch: 3, Training Loss: 2.242683172225952\n",
            "Epoch: 3, Training Loss: 2.312635660171509\n",
            "Epoch: 3, Training Loss: 2.4090774059295654\n",
            "Epoch: 3, Training Loss: 2.4625070095062256\n",
            "Epoch: 3, Training Loss: 2.4977123737335205\n",
            "Epoch: 3, Training Loss: 2.663357734680176\n",
            "Epoch: 3, Training Loss: 2.2685182094573975\n",
            "Epoch: 3, Training Loss: 2.4096415042877197\n",
            "Epoch: 3, Training Loss: 2.2887721061706543\n",
            "Epoch: 3, Training Loss: 2.4186785221099854\n",
            "Epoch: 3, Training Loss: 2.522385597229004\n",
            "Epoch: 3, Training Loss: 2.431431531906128\n",
            "Epoch: 3, Training Loss: 2.627049446105957\n",
            "Epoch: 3, Training Loss: 2.431706190109253\n",
            "Epoch: 3, Training Loss: 2.23909854888916\n",
            "Epoch: 3, Training Loss: 2.3328182697296143\n",
            "Epoch: 3, Training Loss: 2.3979787826538086\n",
            "Epoch: 3, Training Loss: 2.6168973445892334\n",
            "Epoch: 3, Training Loss: 2.4182474613189697\n",
            "Epoch: 3, Training Loss: 2.3828277587890625\n",
            "Epoch: 3, Training Loss: 2.3306589126586914\n",
            "Epoch: 3, Training Loss: 2.433253049850464\n",
            "Epoch: 3, Training Loss: 2.188035726547241\n",
            "Epoch: 3, Training Loss: 2.367955207824707\n",
            "Epoch: 3, Training Loss: 2.432255506515503\n",
            "Epoch: 3, Training Loss: 2.3949546813964844\n",
            "Epoch: 3, Training Loss: 2.165928602218628\n",
            "Epoch: 3, Training Loss: 2.1683735847473145\n",
            "Epoch: 3, Training Loss: 2.336162805557251\n",
            "Epoch: 3, Training Loss: 2.604515552520752\n",
            "Epoch: 3, Training Loss: 2.608781337738037\n",
            "Epoch: 3, Training Loss: 2.207707166671753\n",
            "Epoch: 3, Training Loss: 2.435824394226074\n",
            "Epoch: 3, Training Loss: 2.4094464778900146\n",
            "Epoch: 3, Training Loss: 2.544381618499756\n",
            "Epoch: 3, Training Loss: 2.503434419631958\n",
            "Epoch: 3, Training Loss: 2.253769874572754\n",
            "Epoch: 3, Training Loss: 2.231654167175293\n",
            "Epoch: 3, Training Loss: 2.432546854019165\n",
            "Epoch: 3, Training Loss: 2.3625051975250244\n",
            "Epoch: 3, Training Loss: 2.367997407913208\n",
            "Epoch: 3, Training Loss: 2.2117390632629395\n",
            "Epoch: 3, Training Loss: 2.2169055938720703\n",
            "Epoch: 3, Training Loss: 2.4579570293426514\n",
            "Epoch: 3, Training Loss: 2.188646078109741\n",
            "Epoch: 3, Training Loss: 2.3914592266082764\n",
            "Epoch: 3, Training Loss: 2.350292682647705\n",
            "Epoch: 3, Training Loss: 2.4249777793884277\n",
            "Epoch: 3, Training Loss: 2.6353797912597656\n",
            "Epoch: 3, Training Loss: 1.9844229221343994\n",
            "Epoch: 3, Training Loss: 2.5137877464294434\n",
            "Epoch: 3, Training Loss: 2.2076704502105713\n",
            "Epoch: 3, Training Loss: 2.424859046936035\n",
            "Epoch: 3, Training Loss: 2.4821085929870605\n",
            "Epoch: 3, Training Loss: 2.274388074874878\n",
            "Epoch: 3, Training Loss: 2.4534640312194824\n",
            "Epoch: 3, Training Loss: 2.302608013153076\n",
            "Epoch: 3, Training Loss: 1.9690372943878174\n",
            "Epoch: 3, Training Loss: 2.11145281791687\n",
            "Epoch: 3, Training Loss: 2.680529832839966\n",
            "Epoch: 3, Training Loss: 2.3073689937591553\n",
            "Epoch: 3, Training Loss: 2.502991199493408\n",
            "Epoch: 3, Training Loss: 2.5648510456085205\n",
            "Epoch: 3, Training Loss: 2.281524896621704\n",
            "Epoch: 3, Training Loss: 2.313725709915161\n",
            "Epoch: 3, Training Loss: 2.517652988433838\n",
            "Epoch: 3, Training Loss: 2.420259475708008\n",
            "Epoch: 3, Training Loss: 2.4877095222473145\n",
            "Epoch: 3, Training Loss: 2.4117894172668457\n",
            "Epoch: 3, Training Loss: 2.3272130489349365\n",
            "Epoch: 3, Training Loss: 2.411881685256958\n",
            "Epoch: 3, Training Loss: 2.3340859413146973\n",
            "Epoch: 3, Training Loss: 2.480389356613159\n",
            "Epoch: 3, Training Loss: 2.204930305480957\n",
            "Epoch: 3, Training Loss: 2.172150135040283\n",
            "Epoch: 3, Training Loss: 2.4102635383605957\n",
            "Epoch: 3, Training Loss: 2.5211033821105957\n",
            "Epoch: 3, Training Loss: 2.4442944526672363\n",
            "Epoch: 3, Training Loss: 2.4121408462524414\n",
            "Epoch: 3, Training Loss: 2.502561569213867\n",
            "Epoch: 3, Training Loss: 2.2514984607696533\n",
            "Epoch: 3, Training Loss: 2.4402990341186523\n",
            "Epoch: 3, Training Loss: 2.5860157012939453\n",
            "Epoch: 3, Training Loss: 2.3815999031066895\n",
            "Epoch: 3, Training Loss: 2.293156147003174\n",
            "Epoch: 3, Training Loss: 2.1661622524261475\n",
            "Epoch: 3, Training Loss: 2.3839986324310303\n",
            "Epoch: 3, Training Loss: 2.294241428375244\n",
            "Epoch: 3, Training Loss: 2.3477272987365723\n",
            "Epoch: 3, Training Loss: 2.4816501140594482\n",
            "Epoch: 3, Training Loss: 2.2544467449188232\n",
            "Epoch: 3, Training Loss: 2.2271971702575684\n",
            "Epoch: 3, Training Loss: 2.503582715988159\n",
            "Epoch: 3, Training Loss: 2.270825147628784\n",
            "Epoch: 3, Training Loss: 2.350572109222412\n",
            "Epoch: 3, Training Loss: 2.462031126022339\n",
            "Epoch: 3, Training Loss: 2.168954849243164\n",
            "Epoch: 3, Training Loss: 2.4162161350250244\n",
            "Epoch: 3, Training Loss: 2.441847801208496\n",
            "Epoch: 3, Training Loss: 2.232250690460205\n",
            "Epoch: 3, Training Loss: 2.0494792461395264\n",
            "Epoch: 3, Training Loss: 2.4094295501708984\n",
            "Epoch: 3, Training Loss: 2.5653088092803955\n",
            "Epoch: 3, Training Loss: 2.3878164291381836\n",
            "Epoch: 3, Training Loss: 2.1052379608154297\n",
            "Epoch: 3, Training Loss: 2.558326244354248\n",
            "Epoch: 3, Training Loss: 2.5293002128601074\n",
            "Epoch: 3, Training Loss: 2.5074989795684814\n",
            "Epoch: 3, Training Loss: 2.3312795162200928\n",
            "Epoch: 3, Training Loss: 2.557311534881592\n",
            "Epoch: 3, Training Loss: 2.435948371887207\n",
            "Epoch: 3, Training Loss: 2.142245054244995\n",
            "Epoch: 3, Training Loss: 2.3821444511413574\n",
            "Epoch: 3, Training Loss: 2.266331434249878\n",
            "Epoch: 3, Training Loss: 2.3500609397888184\n",
            "Epoch: 3, Training Loss: 2.476166009902954\n",
            "Epoch: 3, Training Loss: 2.4450690746307373\n",
            "Epoch: 3, Training Loss: 2.436455011367798\n",
            "Epoch: 3, Training Loss: 2.288508415222168\n",
            "Epoch: 3, Training Loss: 2.4297451972961426\n",
            "Epoch: 3, Training Loss: 2.3994715213775635\n",
            "Epoch: 3, Training Loss: 2.4203484058380127\n",
            "Epoch: 3, Training Loss: 2.293576717376709\n",
            "Epoch: 3, Training Loss: 2.516871929168701\n",
            "Epoch: 3, Training Loss: 2.5511960983276367\n",
            "Epoch: 3, Training Loss: 2.555757522583008\n",
            "Epoch: 3, Training Loss: 2.2701995372772217\n",
            "Epoch: 3, Training Loss: 2.300943613052368\n",
            "Epoch: 3, Training Loss: 2.0921175479888916\n",
            "Epoch: 3, Training Loss: 2.2582294940948486\n",
            "Epoch: 3, Training Loss: 2.4292566776275635\n",
            "Epoch: 3, Training Loss: 2.597532033920288\n",
            "Epoch: 3, Training Loss: 2.2573792934417725\n",
            "Epoch: 3, Training Loss: 2.558751344680786\n",
            "Epoch: 3, Training Loss: 2.4902632236480713\n",
            "Epoch: 3, Training Loss: 2.4548301696777344\n",
            "Epoch: 3, Training Loss: 2.4802231788635254\n",
            "Epoch: 3, Training Loss: 2.4575564861297607\n",
            "Epoch: 3, Training Loss: 2.3934175968170166\n",
            "Epoch: 3, Training Loss: 2.3596885204315186\n",
            "Epoch: 3, Training Loss: 2.300851821899414\n",
            "Epoch: 3, Training Loss: 2.0703558921813965\n",
            "Epoch: 3, Training Loss: 2.404021978378296\n",
            "Epoch: 3, Training Loss: 2.3999927043914795\n",
            "Epoch: 3, Training Loss: 2.2646195888519287\n",
            "Epoch: 3, Training Loss: 2.252568244934082\n",
            "Epoch: 3, Training Loss: 2.1176724433898926\n",
            "Epoch: 3, Training Loss: 2.0199735164642334\n",
            "Epoch: 3, Training Loss: 2.1303911209106445\n",
            "Epoch: 3, Training Loss: 2.2861688137054443\n",
            "Epoch: 3, Training Loss: 2.2959015369415283\n",
            "Epoch: 3, Training Loss: 2.554964065551758\n",
            "Epoch: 3, Training Loss: 2.2345783710479736\n",
            "Epoch: 3, Training Loss: 2.516998291015625\n",
            "Epoch: 3, Training Loss: 2.24080491065979\n",
            "Epoch: 3, Training Loss: 2.2954001426696777\n",
            "Epoch: 3, Training Loss: 2.4443864822387695\n",
            "Epoch: 3, Training Loss: 2.8052146434783936\n",
            "Epoch: 3, Training Loss: 2.112273693084717\n",
            "Epoch: 3, Training Loss: 2.3192858695983887\n",
            "Epoch: 3, Training Loss: 2.258511781692505\n",
            "Epoch: 3, Training Loss: 2.0473721027374268\n",
            "Epoch: 3, Training Loss: 2.1271612644195557\n",
            "Epoch: 3, Training Loss: 2.4312713146209717\n",
            "Epoch: 3, Training Loss: 2.3994243144989014\n",
            "Epoch: 3, Training Loss: 2.2621874809265137\n",
            "Epoch: 3, Training Loss: 2.4841363430023193\n",
            "Epoch: 3, Training Loss: 2.4413161277770996\n",
            "Epoch: 3, Training Loss: 2.1798272132873535\n",
            "Epoch: 3, Training Loss: 2.082759141921997\n",
            "Epoch: 3, Training Loss: 2.449674129486084\n",
            "Epoch: 3, Training Loss: 2.51568865776062\n",
            "Epoch: 3, Training Loss: 1.9536164999008179\n",
            "Epoch: 3, Training Loss: 2.429084062576294\n",
            "Epoch: 3, Training Loss: 2.384519100189209\n",
            "Epoch: 3, Training Loss: 2.325144052505493\n",
            "Epoch: 3, Training Loss: 2.1108202934265137\n",
            "Epoch: 3, Training Loss: 2.296076536178589\n",
            "Epoch: 3, Training Loss: 2.309744119644165\n",
            "Epoch: 3, Training Loss: 2.2156026363372803\n",
            "Epoch: 3, Training Loss: 2.3249430656433105\n",
            "Epoch: 3, Training Loss: 2.5112392902374268\n",
            "Epoch: 3, Training Loss: 2.0714187622070312\n",
            "Epoch: 3, Training Loss: 2.497797966003418\n",
            "Epoch: 3, Training Loss: 2.365588426589966\n",
            "Epoch: 3, Training Loss: 2.4651706218719482\n",
            "Epoch: 3, Training Loss: 2.4736380577087402\n",
            "Epoch: 3, Training Loss: 2.462238311767578\n",
            "Epoch: 3, Training Loss: 2.322636365890503\n",
            "Epoch: 3, Training Loss: 2.6714091300964355\n",
            "Epoch: 3, Training Loss: 2.529283046722412\n",
            "Epoch: 3, Training Loss: 2.321470260620117\n",
            "Epoch: 3, Training Loss: 2.19064998626709\n",
            "Epoch: 3, Training Loss: 2.3930623531341553\n",
            "Epoch: 3, Training Loss: 2.2652618885040283\n",
            "Epoch: 3, Training Loss: 2.1917765140533447\n",
            "Epoch: 3, Training Loss: 2.2730846405029297\n",
            "Epoch: 3, Training Loss: 2.2506017684936523\n",
            "Epoch: 3, Training Loss: 2.2875595092773438\n",
            "Epoch: 3, Training Loss: 2.384523391723633\n",
            "Epoch: 3, Training Loss: 2.4521515369415283\n",
            "Epoch: 3, Training Loss: 2.4102182388305664\n",
            "Epoch: 3, Training Loss: 2.6667327880859375\n",
            "Epoch: 3, Training Loss: 2.4863407611846924\n",
            "Epoch: 3, Training Loss: 2.424267292022705\n",
            "Epoch: 3, Training Loss: 2.29357647895813\n",
            "Epoch: 3, Training Loss: 2.5450692176818848\n",
            "Epoch: 3, Training Loss: 2.4538512229919434\n",
            "Epoch: 3, Training Loss: 2.3950083255767822\n",
            "Epoch: 3, Training Loss: 2.530062437057495\n",
            "Epoch: 3, Training Loss: 2.3447608947753906\n",
            "Epoch: 3, Training Loss: 2.265237331390381\n",
            "Epoch: 3, Training Loss: 2.5858943462371826\n",
            "Epoch: 3, Training Loss: 2.198345422744751\n",
            "Epoch: 3, Training Loss: 2.1954996585845947\n",
            "Epoch: 3, Training Loss: 2.2937865257263184\n",
            "Epoch: 3, Training Loss: 2.2088658809661865\n",
            "Epoch: 3, Training Loss: 2.4002463817596436\n",
            "Epoch: 3, Training Loss: 2.4364664554595947\n",
            "Epoch: 3, Training Loss: 2.4589104652404785\n",
            "Epoch: 3, Training Loss: 2.348567247390747\n",
            "Epoch: 3, Training Loss: 2.3272459506988525\n",
            "Epoch: 3, Training Loss: 2.2197229862213135\n",
            "Epoch: 3, Training Loss: 2.494269609451294\n",
            "Epoch: 3, Training Loss: 2.2728607654571533\n",
            "Epoch: 3, Training Loss: 2.50972318649292\n",
            "Epoch: 3, Training Loss: 2.1833252906799316\n",
            "Epoch: 3, Training Loss: 2.2425055503845215\n",
            "Epoch: 3, Training Loss: 2.473687171936035\n",
            "Epoch: 3, Training Loss: 2.33607816696167\n",
            "Epoch: 3, Training Loss: 2.37420392036438\n",
            "Epoch: 3, Training Loss: 2.5964791774749756\n",
            "Epoch: 3, Training Loss: 2.5804340839385986\n",
            "Epoch: 3, Training Loss: 2.426781177520752\n",
            "Epoch: 3, Training Loss: 2.740586996078491\n",
            "Epoch: 3, Training Loss: 2.2793755531311035\n",
            "Epoch: 3, Training Loss: 2.2801687717437744\n",
            "Epoch: 3, Training Loss: 2.3226828575134277\n",
            "Epoch: 3, Training Loss: 2.2067291736602783\n",
            "Epoch: 3, Training Loss: 2.307655096054077\n",
            "Epoch: 3, Training Loss: 2.218902349472046\n",
            "Epoch: 3, Training Loss: 2.2683193683624268\n",
            "Epoch: 3, Training Loss: 2.297959327697754\n",
            "Epoch: 3, Training Loss: 2.3654050827026367\n",
            "Epoch: 3, Training Loss: 2.4262497425079346\n",
            "Epoch: 3, Training Loss: 2.4046168327331543\n",
            "Epoch: 3, Training Loss: 2.18523907661438\n",
            "Epoch: 3, Training Loss: 2.375234842300415\n",
            "Epoch: 3, Training Loss: 2.3481669425964355\n",
            "Epoch: 3, Training Loss: 2.4426767826080322\n",
            "Epoch: 3, Training Loss: 2.2766354084014893\n",
            "Epoch: 3, Training Loss: 2.3872692584991455\n",
            "Epoch: 3, Training Loss: 2.584329128265381\n",
            "Epoch: 3, Training Loss: 2.4277584552764893\n",
            "Epoch: 3, Training Loss: 1.841450572013855\n",
            "Epoch: 3, Training Loss: 2.0363125801086426\n",
            "Epoch: 3, Training Loss: 2.729684352874756\n",
            "Epoch: 3, Training Loss: 2.3522520065307617\n",
            "Epoch: 3, Training Loss: 2.158905506134033\n",
            "Epoch: 3, Training Loss: 2.229423999786377\n",
            "Epoch: 3, Training Loss: 2.2569918632507324\n",
            "Epoch: 3, Training Loss: 2.3288164138793945\n",
            "Epoch: 3, Training Loss: 2.10611629486084\n",
            "Epoch: 3, Training Loss: 2.230804443359375\n",
            "Epoch: 3, Training Loss: 2.500728130340576\n",
            "Epoch: 3, Training Loss: 2.454650402069092\n",
            "Epoch: 3, Training Loss: 2.4789469242095947\n",
            "Epoch: 3, Training Loss: 2.1776676177978516\n",
            "Epoch: 3, Training Loss: 2.074219226837158\n",
            "Epoch: 3, Training Loss: 2.282196044921875\n",
            "Epoch: 3, Training Loss: 2.267296314239502\n",
            "Epoch: 3, Training Loss: 2.383176326751709\n",
            "Epoch: 3, Training Loss: 2.045011281967163\n",
            "Epoch: 3, Training Loss: 2.199167013168335\n",
            "Epoch: 3, Training Loss: 2.203579902648926\n",
            "Epoch: 3, Training Loss: 2.265176773071289\n",
            "Epoch: 3, Training Loss: 2.3175971508026123\n",
            "Epoch: 3, Training Loss: 2.2433762550354004\n",
            "Epoch: 3, Training Loss: 2.5898823738098145\n",
            "Epoch: 3, Training Loss: 2.3204195499420166\n",
            "Epoch: 3, Training Loss: 2.3153722286224365\n",
            "Epoch: 3, Training Loss: 2.2345733642578125\n",
            "Epoch: 3, Training Loss: 2.9260053634643555\n",
            "Epoch: 3, Training Loss: 2.478721857070923\n",
            "Epoch: 3, Training Loss: 2.3795676231384277\n",
            "Epoch: 3, Training Loss: 2.6599628925323486\n",
            "Epoch: 3, Training Loss: 2.521768569946289\n",
            "Epoch: 3, Training Loss: 2.3811569213867188\n",
            "Epoch: 3, Training Loss: 2.5277271270751953\n",
            "Epoch: 3, Training Loss: 2.4242589473724365\n",
            "Epoch: 3, Training Loss: 1.9680074453353882\n",
            "Epoch: 3, Training Loss: 2.268843412399292\n",
            "Epoch: 3, Training Loss: 2.2659082412719727\n",
            "Epoch: 3, Training Loss: 2.131225109100342\n",
            "Epoch: 3, Training Loss: 2.516099214553833\n",
            "Epoch: 3, Training Loss: 2.307023048400879\n",
            "Epoch: 3, Training Loss: 2.453169107437134\n",
            "Epoch: 3, Training Loss: 2.4505748748779297\n",
            "Epoch: 3, Training Loss: 2.3377251625061035\n",
            "Epoch: 3, Training Loss: 2.43865966796875\n",
            "Epoch: 3, Training Loss: 2.251216411590576\n",
            "Epoch: 3, Training Loss: 2.489891290664673\n",
            "Epoch: 3, Training Loss: 2.4811136722564697\n",
            "Epoch: 3, Training Loss: 2.520502805709839\n",
            "Epoch: 3, Training Loss: 2.3299639225006104\n",
            "Epoch: 3, Training Loss: 2.095658779144287\n",
            "Epoch: 3, Training Loss: 2.799767255783081\n",
            "Epoch: 3, Training Loss: 2.217349052429199\n",
            "Epoch: 3, Training Loss: 2.2923829555511475\n",
            "Epoch: 3, Training Loss: 2.3087821006774902\n",
            "Epoch: 3, Training Loss: 2.4783856868743896\n",
            "Epoch: 3, Training Loss: 2.4587697982788086\n",
            "Epoch: 3, Training Loss: 2.511091470718384\n",
            "Epoch: 3, Training Loss: 2.252577066421509\n",
            "Epoch: 3, Training Loss: 2.475954294204712\n",
            "Epoch: 3, Training Loss: 2.3737478256225586\n",
            "Epoch: 3, Training Loss: 2.3475468158721924\n",
            "Epoch: 3, Training Loss: 2.004376173019409\n",
            "Epoch: 3, Training Loss: 1.9665660858154297\n",
            "Epoch: 3, Training Loss: 2.1465606689453125\n",
            "Epoch: 3, Training Loss: 2.2380423545837402\n",
            "Epoch: 3, Training Loss: 2.2982189655303955\n",
            "Epoch: 3, Training Loss: 2.035423994064331\n",
            "Epoch: 3, Training Loss: 2.41180157661438\n",
            "Epoch: 3, Training Loss: 2.278843879699707\n",
            "Epoch: 3, Training Loss: 2.028041362762451\n",
            "Epoch: 3, Training Loss: 2.103679656982422\n",
            "Epoch: 3, Training Loss: 2.3324098587036133\n",
            "Epoch: 3, Training Loss: 2.0483508110046387\n",
            "Epoch: 3, Training Loss: 1.9786406755447388\n",
            "Epoch: 3, Training Loss: 2.4436538219451904\n",
            "Epoch: 3, Training Loss: 2.3432884216308594\n",
            "Epoch: 3, Training Loss: 2.288320779800415\n",
            "Epoch: 3, Training Loss: 2.2755818367004395\n",
            "Epoch: 3, Training Loss: 2.345823049545288\n",
            "Epoch: 3, Training Loss: 2.4117019176483154\n",
            "Epoch: 3, Training Loss: 2.288032293319702\n",
            "Epoch: 3, Training Loss: 2.3723604679107666\n",
            "Epoch: 3, Training Loss: 2.568342924118042\n",
            "Epoch: 3, Training Loss: 2.2967429161071777\n",
            "Epoch: 3, Training Loss: 2.075111150741577\n",
            "Epoch: 3, Training Loss: 2.3259661197662354\n",
            "Epoch: 3, Training Loss: 2.3267126083374023\n",
            "Epoch: 3, Training Loss: 2.2422800064086914\n",
            "Epoch: 3, Training Loss: 2.381617307662964\n",
            "Epoch: 3, Training Loss: 2.0202412605285645\n",
            "Epoch: 3, Training Loss: 2.590813636779785\n",
            "Epoch: 3, Training Loss: 2.229426622390747\n",
            "Epoch: 3, Training Loss: 2.2340729236602783\n",
            "Epoch: 3, Training Loss: 2.3803868293762207\n",
            "Epoch: 3, Training Loss: 2.4290759563446045\n",
            "Epoch: 3, Training Loss: 2.3527612686157227\n",
            "Epoch: 3, Training Loss: 2.1899514198303223\n",
            "Epoch: 3, Training Loss: 2.2381863594055176\n",
            "Epoch: 3, Training Loss: 2.3858730792999268\n",
            "Epoch: 3, Training Loss: 2.3423712253570557\n",
            "Epoch: 3, Training Loss: 2.26261043548584\n",
            "Epoch: 3, Training Loss: 2.0518531799316406\n",
            "Epoch: 3, Training Loss: 2.1317622661590576\n",
            "Epoch: 3, Training Loss: 2.280318021774292\n",
            "Epoch: 3, Training Loss: 2.3668367862701416\n",
            "Epoch: 3, Training Loss: 2.0805423259735107\n",
            "Epoch: 3, Training Loss: 2.0590755939483643\n",
            "Epoch: 3, Training Loss: 2.2009994983673096\n",
            "Epoch: 3, Training Loss: 2.3739137649536133\n",
            "Epoch: 3, Training Loss: 2.4122138023376465\n",
            "Epoch: 3, Training Loss: 2.2102506160736084\n",
            "Epoch: 3, Training Loss: 1.9585837125778198\n",
            "Epoch: 3, Training Loss: 2.365574598312378\n",
            "Epoch: 3, Training Loss: 2.439998149871826\n",
            "Epoch: 3, Training Loss: 2.173845052719116\n",
            "Epoch: 3, Training Loss: 2.39630126953125\n",
            "Epoch: 3, Training Loss: 2.3607845306396484\n",
            "Epoch: 3, Training Loss: 2.3594095706939697\n",
            "Epoch: 3, Training Loss: 2.1427736282348633\n",
            "Epoch: 3, Training Loss: 2.3266425132751465\n",
            "Epoch: 3, Training Loss: 2.3738906383514404\n",
            "Epoch: 3, Training Loss: 2.239692449569702\n",
            "Epoch: 3, Training Loss: 2.6713836193084717\n",
            "Epoch: 3, Training Loss: 2.424872398376465\n",
            "Epoch: 3, Training Loss: 2.526557207107544\n",
            "Epoch: 3, Training Loss: 2.4074506759643555\n",
            "Epoch: 3, Training Loss: 2.6038787364959717\n",
            "Epoch: 3, Training Loss: 2.3316922187805176\n",
            "Epoch: 3, Training Loss: 2.6365206241607666\n",
            "Epoch: 3, Training Loss: 2.5926618576049805\n",
            "Epoch: 3, Training Loss: 2.2163500785827637\n",
            "Epoch: 3, Training Loss: 2.35886287689209\n",
            "Epoch: 3, Training Loss: 2.6601154804229736\n",
            "Epoch: 3, Training Loss: 2.344529151916504\n",
            "Epoch: 3, Training Loss: 2.079852342605591\n",
            "Epoch: 3, Training Loss: 2.3554532527923584\n",
            "Epoch: 3, Training Loss: 2.060741662979126\n",
            "Epoch: 3, Training Loss: 2.571676254272461\n",
            "Epoch: 3, Training Loss: 2.2088065147399902\n",
            "Epoch: 3, Training Loss: 2.2473647594451904\n",
            "Epoch: 3, Training Loss: 2.4133968353271484\n",
            "Epoch: 3, Training Loss: 2.3489561080932617\n",
            "Epoch: 3, Training Loss: 2.0005056858062744\n",
            "Epoch: 3, Training Loss: 2.153407573699951\n",
            "Epoch: 3, Training Loss: 2.4214234352111816\n",
            "Epoch: 3, Training Loss: 2.449965000152588\n",
            "Epoch: 3, Training Loss: 2.290426254272461\n",
            "Epoch: 3, Training Loss: 2.2079017162323\n",
            "Epoch: 3, Training Loss: 2.6691126823425293\n",
            "Epoch: 3, Training Loss: 2.585115909576416\n",
            "Epoch: 3, Training Loss: 2.1350953578948975\n",
            "Epoch: 3, Training Loss: 2.565875768661499\n",
            "Epoch: 3, Training Loss: 2.320582628250122\n",
            "Epoch: 3, Training Loss: 2.1699321269989014\n",
            "Epoch: 3, Training Loss: 2.4060218334198\n",
            "Epoch: 3, Training Loss: 2.239020586013794\n",
            "Epoch: 3, Training Loss: 2.184152364730835\n",
            "Epoch: 3, Training Loss: 2.257005214691162\n",
            "Epoch: 3, Training Loss: 2.5415797233581543\n",
            "Epoch: 3, Training Loss: 2.575171947479248\n",
            "Epoch: 3, Training Loss: 2.353996515274048\n",
            "Epoch: 3, Training Loss: 2.3289787769317627\n",
            "Epoch: 3, Training Loss: 2.1855947971343994\n",
            "Epoch: 3, Training Loss: 2.1615078449249268\n",
            "Epoch: 3, Training Loss: 2.167196035385132\n",
            "Epoch: 3, Training Loss: 2.3342580795288086\n",
            "Epoch: 3, Training Loss: 2.4323208332061768\n",
            "Epoch: 3, Training Loss: 2.4854893684387207\n",
            "Epoch: 3, Training Loss: 2.0320682525634766\n",
            "Epoch: 3, Training Loss: 2.3982656002044678\n",
            "Epoch: 3, Training Loss: 2.4130098819732666\n",
            "Epoch: 3, Training Loss: 2.24021053314209\n",
            "Epoch: 3, Training Loss: 2.444288730621338\n",
            "Epoch: 3, Training Loss: 2.3247623443603516\n",
            "Epoch: 3, Training Loss: 2.1063625812530518\n",
            "Epoch: 3, Training Loss: 2.3161962032318115\n",
            "Epoch: 3, Training Loss: 2.216900587081909\n",
            "Epoch: 3, Training Loss: 2.1528923511505127\n",
            "Epoch: 3, Training Loss: 2.263488292694092\n",
            "Epoch: 3, Training Loss: 2.4285387992858887\n",
            "Epoch: 3, Training Loss: 2.1158313751220703\n",
            "Epoch: 3, Training Loss: 2.2900710105895996\n",
            "Epoch: 3, Training Loss: 2.5967748165130615\n",
            "Epoch: 3, Training Loss: 2.430591106414795\n",
            "Epoch: 3, Training Loss: 2.55206561088562\n",
            "Epoch: 3, Training Loss: 1.9427289962768555\n",
            "Epoch: 3, Training Loss: 2.2790019512176514\n",
            "Epoch: 3, Training Loss: 2.0778968334198\n",
            "Epoch: 3, Training Loss: 2.259819507598877\n",
            "Epoch: 3, Training Loss: 2.1002585887908936\n",
            "Epoch: 3, Training Loss: 2.0135843753814697\n",
            "Epoch: 3, Training Loss: 2.184518337249756\n",
            "Epoch: 3, Training Loss: 2.5354950428009033\n",
            "Epoch: 3, Training Loss: 2.2128732204437256\n",
            "Epoch: 3, Training Loss: 2.44142746925354\n",
            "Epoch: 3, Training Loss: 2.55251407623291\n",
            "Epoch: 3, Training Loss: 2.367877960205078\n",
            "Epoch: 3, Training Loss: 2.227916955947876\n",
            "Epoch: 3, Training Loss: 2.402299404144287\n",
            "Epoch: 3, Training Loss: 2.1492700576782227\n",
            "Epoch: 3, Training Loss: 2.270226001739502\n",
            "Epoch: 3, Training Loss: 2.5997753143310547\n",
            "Epoch: 3, Training Loss: 2.400733470916748\n",
            "Epoch: 3, Training Loss: 2.296705722808838\n",
            "Epoch: 3, Training Loss: 2.216850996017456\n",
            "Epoch: 3, Training Loss: 2.530311346054077\n",
            "Epoch: 3, Training Loss: 2.5007426738739014\n",
            "Epoch: 3, Training Loss: 2.369849443435669\n",
            "Epoch: 3, Training Loss: 2.2629544734954834\n",
            "Epoch: 3, Training Loss: 2.312105178833008\n",
            "Epoch: 3, Training Loss: 2.2797772884368896\n",
            "Epoch: 3, Training Loss: 2.282780408859253\n",
            "Epoch: 3, Training Loss: 2.189612627029419\n",
            "Epoch: 3, Training Loss: 2.1344010829925537\n",
            "Epoch: 3, Training Loss: 2.3794243335723877\n",
            "Epoch: 3, Training Loss: 2.3014535903930664\n",
            "Epoch: 3, Training Loss: 2.4533042907714844\n",
            "Epoch: 3, Training Loss: 2.2738492488861084\n",
            "Epoch: 3, Training Loss: 2.6255722045898438\n",
            "Epoch: 3, Training Loss: 2.233217239379883\n",
            "Epoch: 3, Training Loss: 2.085179328918457\n",
            "Epoch: 3, Training Loss: 2.2900829315185547\n",
            "Epoch: 3, Training Loss: 2.1438701152801514\n",
            "Epoch: 3, Training Loss: 2.319054126739502\n",
            "Epoch: 3, Training Loss: 2.3931519985198975\n",
            "Epoch: 3, Training Loss: 2.165015459060669\n",
            "Epoch: 3, Training Loss: 2.3223249912261963\n",
            "Epoch: 3, Training Loss: 2.266122579574585\n",
            "Epoch: 3, Training Loss: 2.1751675605773926\n",
            "Epoch: 3, Training Loss: 2.339811325073242\n",
            "Epoch: 3, Training Loss: 2.3742194175720215\n",
            "Epoch: 3, Training Loss: 2.4485116004943848\n",
            "Epoch: 3, Training Loss: 2.5313165187835693\n",
            "Epoch: 3, Training Loss: 2.3317110538482666\n",
            "Epoch: 3, Training Loss: 2.3782236576080322\n",
            "Epoch: 3, Training Loss: 2.2844467163085938\n",
            "Epoch: 3, Training Loss: 2.154675006866455\n",
            "Epoch: 3, Training Loss: 2.4403491020202637\n",
            "Epoch: 3, Training Loss: 2.1181018352508545\n",
            "Epoch: 3, Training Loss: 2.4118094444274902\n",
            "Epoch: 3, Training Loss: 2.5931217670440674\n",
            "Epoch: 3, Training Loss: 2.449201822280884\n",
            "Epoch: 3, Training Loss: 2.3583414554595947\n",
            "Epoch: 3, Training Loss: 2.48652720451355\n",
            "Epoch: 3, Training Loss: 2.2519242763519287\n",
            "Epoch: 3, Training Loss: 2.292177438735962\n",
            "Epoch: 3, Training Loss: 2.3833112716674805\n",
            "Epoch: 3, Training Loss: 2.31223201751709\n",
            "Epoch: 3, Training Loss: 2.1943249702453613\n",
            "Epoch: 3, Training Loss: 2.2140185832977295\n",
            "Epoch: 3, Training Loss: 2.2053349018096924\n",
            "Epoch: 3, Training Loss: 2.2861433029174805\n",
            "Epoch: 3, Training Loss: 2.3894128799438477\n",
            "Epoch: 3, Training Loss: 2.2456271648406982\n",
            "Epoch: 3, Training Loss: 2.23110294342041\n",
            "Epoch: 3, Training Loss: 2.3783066272735596\n",
            "Epoch: 3, Training Loss: 2.324814796447754\n",
            "Epoch: 3, Training Loss: 2.168036460876465\n",
            "Epoch: 3, Training Loss: 2.2756404876708984\n",
            "Epoch: 3, Training Loss: 2.2687435150146484\n",
            "Epoch: 3, Training Loss: 2.4306397438049316\n",
            "Epoch: 3, Training Loss: 2.1288115978240967\n",
            "Epoch: 3, Training Loss: 2.3383708000183105\n",
            "Epoch: 3, Training Loss: 2.2694056034088135\n",
            "Epoch: 3, Training Loss: 2.2807843685150146\n",
            "Epoch: 3, Training Loss: 2.0328402519226074\n",
            "Epoch: 3, Training Loss: 2.148831367492676\n",
            "Epoch: 3, Training Loss: 2.2490601539611816\n",
            "Epoch: 3, Training Loss: 2.4224512577056885\n",
            "Epoch: 3, Training Loss: 2.4948678016662598\n",
            "Epoch: 3, Training Loss: 2.348332405090332\n",
            "Epoch: 3, Training Loss: 2.4722368717193604\n",
            "Epoch: 3, Training Loss: 1.976660132408142\n",
            "Epoch: 3, Training Loss: 2.2050423622131348\n",
            "Epoch: 3, Training Loss: 2.486365556716919\n",
            "Epoch: 3, Training Loss: 2.3757166862487793\n",
            "Epoch: 3, Training Loss: 2.4370992183685303\n",
            "Epoch: 3, Training Loss: 2.2578542232513428\n",
            "Epoch: 3, Training Loss: 2.175062656402588\n",
            "Epoch: 3, Training Loss: 2.2075371742248535\n",
            "Epoch: 3, Training Loss: 2.1719040870666504\n",
            "Epoch: 3, Training Loss: 2.266871690750122\n",
            "Epoch: 3, Training Loss: 2.272918701171875\n",
            "Epoch: 3, Training Loss: 2.2719602584838867\n",
            "Epoch: 3, Training Loss: 2.5194294452667236\n",
            "Epoch: 3, Training Loss: 2.121992349624634\n",
            "Epoch: 3, Training Loss: 2.112192153930664\n",
            "Epoch: 3, Training Loss: 2.419677972793579\n",
            "Epoch: 3, Training Loss: 2.0731937885284424\n",
            "Epoch: 3, Training Loss: 2.2814276218414307\n",
            "Epoch: 3, Training Loss: 2.480799913406372\n",
            "Epoch: 3, Training Loss: 1.8774738311767578\n",
            "Epoch: 3, Training Loss: 2.5805177688598633\n",
            "Epoch: 3, Training Loss: 2.359546661376953\n",
            "Epoch: 3, Training Loss: 2.5360705852508545\n",
            "Epoch: 3, Training Loss: 2.2152774333953857\n",
            "Epoch: 3, Training Loss: 2.2814600467681885\n",
            "Epoch: 3, Training Loss: 2.1791882514953613\n",
            "Epoch: 3, Training Loss: 2.1967666149139404\n",
            "Epoch: 3, Training Loss: 2.353027582168579\n",
            "Epoch: 3, Training Loss: 2.1836626529693604\n",
            "Epoch: 3, Training Loss: 2.4044790267944336\n",
            "Epoch: 3, Training Loss: 2.2681314945220947\n",
            "Epoch: 3, Training Loss: 2.1862142086029053\n",
            "Epoch: 3, Training Loss: 2.033177614212036\n",
            "Epoch: 3, Training Loss: 2.2474429607391357\n",
            "Epoch: 3, Training Loss: 2.3746743202209473\n",
            "Epoch: 3, Training Loss: 2.3164618015289307\n",
            "Epoch: 3, Training Loss: 1.916136622428894\n",
            "Epoch: 3, Training Loss: 2.201822280883789\n",
            "Epoch: 3, Training Loss: 2.3363168239593506\n",
            "Epoch: 3, Training Loss: 2.5362329483032227\n",
            "Epoch: 3, Training Loss: 2.275564670562744\n",
            "Epoch: 3, Training Loss: 2.6540639400482178\n",
            "Epoch: 3, Training Loss: 2.108492612838745\n",
            "Epoch: 3, Training Loss: 2.79240083694458\n",
            "Epoch: 3, Training Loss: 2.3319716453552246\n",
            "Epoch: 3, Training Loss: 2.515735149383545\n",
            "Epoch: 3, Training Loss: 2.3986446857452393\n",
            "Epoch: 3, Training Loss: 2.2382960319519043\n",
            "Epoch: 3, Training Loss: 2.199389696121216\n",
            "Epoch: 3, Training Loss: 2.340076208114624\n",
            "Epoch: 3, Training Loss: 1.8946431875228882\n",
            "Epoch: 3, Training Loss: 2.4331791400909424\n",
            "Epoch: 3, Training Loss: 2.3806207180023193\n",
            "Epoch: 3, Training Loss: 2.659475564956665\n",
            "Epoch: 3, Training Loss: 2.320590019226074\n",
            "Epoch: 3, Training Loss: 2.3590073585510254\n",
            "Epoch: 3, Training Loss: 1.883588433265686\n",
            "Epoch: 3, Training Loss: 2.409383535385132\n",
            "Epoch: 3, Training Loss: 2.1500701904296875\n",
            "Epoch: 3, Training Loss: 2.1813886165618896\n",
            "Epoch: 3, Training Loss: 2.317289352416992\n",
            "Epoch: 3, Training Loss: 2.087186574935913\n",
            "Epoch: 3, Training Loss: 2.3372385501861572\n",
            "Epoch: 3, Training Loss: 2.481755256652832\n",
            "Epoch: 3, Training Loss: 2.3451755046844482\n",
            "Epoch: 3, Training Loss: 2.1831042766571045\n",
            "Epoch: 3, Training Loss: 2.217702865600586\n",
            "Epoch: 3, Training Loss: 2.3309555053710938\n",
            "Epoch: 3, Training Loss: 2.2532894611358643\n",
            "Epoch: 3, Training Loss: 2.448089838027954\n",
            "Epoch: 3, Training Loss: 2.4642367362976074\n",
            "Epoch: 3, Training Loss: 2.3231372833251953\n",
            "Epoch: 3, Training Loss: 2.1464192867279053\n",
            "Epoch: 3, Training Loss: 2.279484748840332\n",
            "Epoch: 3, Training Loss: 2.040215492248535\n",
            "Epoch: 3, Training Loss: 2.535209894180298\n",
            "Epoch: 3, Training Loss: 2.2727439403533936\n",
            "Epoch: 3, Training Loss: 2.4571986198425293\n",
            "Epoch: 3, Training Loss: 2.1980085372924805\n",
            "Epoch: 3, Training Loss: 2.2126810550689697\n",
            "Epoch: 3, Training Loss: 2.3265159130096436\n",
            "Epoch: 3, Training Loss: 2.354071617126465\n",
            "Epoch: 3, Training Loss: 2.2500481605529785\n",
            "Epoch: 3, Training Loss: 2.035468101501465\n",
            "Epoch: 3, Training Loss: 2.1992526054382324\n",
            "Epoch: 3, Training Loss: 2.4498934745788574\n",
            "Epoch: 3, Training Loss: 2.0987117290496826\n",
            "Epoch: 3, Training Loss: 2.0404787063598633\n",
            "Epoch: 3, Training Loss: 2.388732433319092\n",
            "Epoch: 3, Training Loss: 2.4510300159454346\n",
            "Epoch: 3, Training Loss: 2.198007345199585\n",
            "Epoch: 3, Training Loss: 2.440615177154541\n",
            "Epoch: 3, Training Loss: 2.2337236404418945\n",
            "Epoch: 3, Training Loss: 2.0175490379333496\n",
            "Epoch: 3, Training Loss: 2.344024419784546\n",
            "Epoch: 3, Training Loss: 2.537431478500366\n",
            "Epoch: 3, Training Loss: 2.2207870483398438\n",
            "Epoch: 3, Training Loss: 2.1740260124206543\n",
            "Epoch: 3, Training Loss: 2.3726141452789307\n",
            "Epoch: 3, Training Loss: 2.2270703315734863\n",
            "Epoch: 3, Training Loss: 2.1500086784362793\n",
            "Epoch: 3, Training Loss: 2.445474147796631\n",
            "Epoch: 3, Training Loss: 2.032512903213501\n",
            "Epoch: 3, Training Loss: 2.0897579193115234\n",
            "Epoch: 3, Training Loss: 2.2272090911865234\n",
            "Epoch: 3, Training Loss: 2.4203367233276367\n",
            "Epoch: 3, Training Loss: 2.1948070526123047\n",
            "Epoch: 3, Training Loss: 2.51719331741333\n",
            "Epoch: 3, Training Loss: 2.409346103668213\n",
            "Epoch: 3, Training Loss: 2.236781597137451\n",
            "Epoch: 3, Training Loss: 1.973894476890564\n",
            "Epoch: 3, Training Loss: 2.4580092430114746\n",
            "Epoch: 3, Training Loss: 2.342808961868286\n",
            "Epoch: 3, Training Loss: 2.180504322052002\n",
            "Epoch: 3, Training Loss: 2.475600004196167\n",
            "Epoch: 3, Training Loss: 2.6586670875549316\n",
            "Epoch: 3, Training Loss: 2.3291242122650146\n",
            "Epoch: 3, Training Loss: 2.2096757888793945\n",
            "Epoch: 3, Training Loss: 2.2211124897003174\n",
            "Epoch: 3, Training Loss: 2.4242303371429443\n",
            "Epoch: 3, Training Loss: 2.4332165718078613\n",
            "Epoch: 3, Training Loss: 2.180954694747925\n",
            "Epoch: 3, Training Loss: 2.310835838317871\n",
            "Epoch: 3, Training Loss: 2.2301127910614014\n",
            "Epoch: 3, Training Loss: 2.515015125274658\n",
            "Epoch: 3, Training Loss: 2.225552558898926\n",
            "Epoch: 3, Training Loss: 2.2041170597076416\n",
            "Epoch: 3, Training Loss: 2.106602907180786\n",
            "Epoch: 3, Training Loss: 2.223135232925415\n",
            "Epoch: 3, Training Loss: 2.459059953689575\n",
            "Epoch: 3, Training Loss: 2.3423988819122314\n",
            "Epoch: 3, Training Loss: 2.2930080890655518\n",
            "Epoch: 3, Training Loss: 2.3539581298828125\n",
            "Epoch: 3, Training Loss: 2.337904930114746\n",
            "Epoch: 3, Training Loss: 2.53494930267334\n",
            "Epoch: 3, Training Loss: 2.315134286880493\n",
            "Epoch: 3, Training Loss: 2.6135201454162598\n",
            "Epoch: 3, Training Loss: 2.2439448833465576\n",
            "Epoch: 3, Training Loss: 2.209500551223755\n",
            "Epoch: 3, Training Loss: 2.1909093856811523\n",
            "Epoch: 3, Training Loss: 2.418576240539551\n",
            "Epoch: 3, Training Loss: 2.2147514820098877\n",
            "Epoch: 3, Training Loss: 2.426520824432373\n",
            "Epoch: 3, Training Loss: 2.166513442993164\n",
            "Epoch: 3, Training Loss: 2.219036817550659\n",
            "Epoch: 3, Training Loss: 2.2063040733337402\n",
            "Epoch: 3, Training Loss: 2.2371768951416016\n",
            "Epoch: 3, Training Loss: 2.1446127891540527\n",
            "Epoch: 3, Training Loss: 2.3662593364715576\n",
            "Epoch: 3, Training Loss: 2.203139305114746\n",
            "Epoch: 3, Training Loss: 2.4260025024414062\n",
            "Epoch: 3, Training Loss: 2.0559144020080566\n",
            "Epoch: 3, Training Loss: 2.354343891143799\n",
            "Epoch: 3, Training Loss: 2.244899034500122\n",
            "Epoch: 3, Training Loss: 2.3313546180725098\n",
            "Epoch: 3, Training Loss: 2.2025742530822754\n",
            "Epoch: 3, Training Loss: 2.139328718185425\n",
            "Epoch: 3, Training Loss: 2.44334077835083\n",
            "Epoch: 3, Training Loss: 2.157731771469116\n",
            "Epoch: 3, Training Loss: 2.084393262863159\n",
            "Epoch: 3, Training Loss: 2.6163008213043213\n",
            "Epoch: 3, Training Loss: 2.336843490600586\n",
            "Epoch: 3, Training Loss: 2.491176128387451\n",
            "Epoch: 3, Training Loss: 2.2362773418426514\n",
            "Epoch: 3, Training Loss: 2.201197385787964\n",
            "Epoch: 3, Training Loss: 2.2159740924835205\n",
            "Epoch: 3, Training Loss: 2.148719072341919\n",
            "Epoch: 3, Training Loss: 2.2104783058166504\n",
            "Epoch: 3, Training Loss: 2.46964955329895\n",
            "Epoch: 3, Training Loss: 2.4514331817626953\n",
            "Epoch: 3, Training Loss: 2.330644369125366\n",
            "Epoch: 3, Training Loss: 2.1915266513824463\n",
            "Epoch: 3, Training Loss: 2.2070796489715576\n",
            "Epoch: 3, Training Loss: 2.2585418224334717\n",
            "Epoch: 3, Training Loss: 2.234473705291748\n",
            "Epoch: 3, Training Loss: 2.288348436355591\n",
            "Epoch: 3, Training Loss: 2.3495187759399414\n",
            "Epoch: 3, Training Loss: 2.2547714710235596\n",
            "Epoch: 3, Training Loss: 2.3170242309570312\n",
            "Epoch: 3, Training Loss: 2.0241172313690186\n",
            "Epoch: 3, Training Loss: 2.3762729167938232\n",
            "Epoch: 3, Training Loss: 2.245905876159668\n",
            "Epoch: 3, Training Loss: 2.2854578495025635\n",
            "Epoch: 3, Training Loss: 2.3373796939849854\n",
            "Epoch: 3, Training Loss: 2.2991597652435303\n",
            "Epoch: 3, Training Loss: 2.309614419937134\n",
            "Epoch: 3, Training Loss: 1.9588539600372314\n",
            "Epoch: 3, Training Loss: 2.0185389518737793\n",
            "Epoch: 3, Training Loss: 2.2571794986724854\n",
            "Epoch: 3, Training Loss: 2.3655214309692383\n",
            "Epoch: 3, Training Loss: 2.0031144618988037\n",
            "Epoch: 3, Training Loss: 2.288384437561035\n",
            "Epoch: 3, Training Loss: 2.0991389751434326\n",
            "Epoch: 3, Training Loss: 2.0534026622772217\n",
            "Epoch: 3, Training Loss: 2.0839309692382812\n",
            "Epoch: 3, Training Loss: 2.1054320335388184\n",
            "Epoch: 3, Training Loss: 2.3521432876586914\n",
            "Epoch: 3, Training Loss: 1.9659461975097656\n",
            "Epoch: 3, Training Loss: 2.1335031986236572\n",
            "Epoch: 3, Training Loss: 2.2921524047851562\n",
            "Epoch: 3, Training Loss: 2.185694694519043\n",
            "Epoch: 3, Training Loss: 2.4877071380615234\n",
            "Epoch: 3, Training Loss: 2.2574379444122314\n",
            "Epoch: 3, Training Loss: 2.372236728668213\n",
            "Epoch: 3, Training Loss: 2.613151788711548\n",
            "Epoch: 3, Training Loss: 2.277773857116699\n",
            "Epoch: 3, Training Loss: 2.133946657180786\n",
            "Epoch: 3, Training Loss: 2.427532196044922\n",
            "Epoch: 3, Training Loss: 2.2899160385131836\n",
            "Epoch: 3, Training Loss: 2.0633738040924072\n",
            "Epoch: 3, Training Loss: 2.320030927658081\n",
            "Epoch: 3, Training Loss: 2.418174982070923\n",
            "Epoch: 3, Training Loss: 2.3655691146850586\n",
            "Epoch: 3, Training Loss: 2.1904823780059814\n",
            "Epoch: 3, Training Loss: 2.443403720855713\n",
            "Epoch: 3, Training Loss: 2.1569840908050537\n",
            "Epoch: 3, Training Loss: 2.1549322605133057\n",
            "Epoch: 3, Training Loss: 2.4758644104003906\n",
            "Epoch: 3, Training Loss: 2.4145913124084473\n",
            "Epoch: 3, Training Loss: 2.1614301204681396\n",
            "Epoch: 3, Training Loss: 2.6636736392974854\n",
            "Epoch: 3, Training Loss: 2.1112306118011475\n",
            "Epoch: 3, Training Loss: 2.324201822280884\n",
            "Epoch: 3, Training Loss: 1.9489506483078003\n",
            "Epoch: 3, Training Loss: 2.0393552780151367\n",
            "Epoch: 3, Training Loss: 2.3102967739105225\n",
            "Epoch: 3, Training Loss: 2.2032413482666016\n",
            "Epoch: 3, Training Loss: 2.246192216873169\n",
            "Epoch: 3, Training Loss: 2.1076323986053467\n",
            "Epoch: 3, Training Loss: 2.3210389614105225\n",
            "Epoch: 3, Training Loss: 1.9966636896133423\n",
            "Epoch: 3, Training Loss: 2.40010929107666\n",
            "Epoch: 3, Training Loss: 2.1644318103790283\n",
            "Epoch: 3, Training Loss: 2.1379897594451904\n",
            "Epoch: 3, Training Loss: 2.4548442363739014\n",
            "Epoch: 3, Training Loss: 2.2711775302886963\n",
            "Epoch: 3, Training Loss: 2.4330637454986572\n",
            "Epoch: 3, Training Loss: 2.386460065841675\n",
            "Epoch: 3, Training Loss: 2.3903629779815674\n",
            "Epoch: 3, Training Loss: 2.3638646602630615\n",
            "Epoch: 3, Training Loss: 2.447340488433838\n",
            "Epoch: 3, Training Loss: 2.2697038650512695\n",
            "Epoch: 3, Training Loss: 2.39573073387146\n",
            "Epoch: 3, Training Loss: 2.1877377033233643\n",
            "Epoch: 3, Training Loss: 2.2136476039886475\n",
            "Epoch: 3, Training Loss: 2.509829044342041\n",
            "Epoch: 3, Training Loss: 2.0978662967681885\n",
            "Epoch: 3, Training Loss: 2.5707101821899414\n",
            "Epoch: 3, Training Loss: 2.0194578170776367\n",
            "Epoch: 3, Training Loss: 2.0804800987243652\n",
            "Epoch: 3, Training Loss: 2.2688148021698\n",
            "Epoch: 3, Training Loss: 2.289451837539673\n",
            "Epoch: 3, Training Loss: 2.3318190574645996\n",
            "Epoch: 3, Training Loss: 2.3359529972076416\n",
            "Epoch: 3, Training Loss: 2.411207914352417\n",
            "Epoch: 3, Training Loss: 2.461945056915283\n",
            "Epoch: 3, Training Loss: 2.1780173778533936\n",
            "Epoch: 3, Training Loss: 2.160588502883911\n",
            "Epoch: 3, Training Loss: 2.1278762817382812\n",
            "Epoch: 3, Training Loss: 2.6112165451049805\n",
            "Epoch: 3, Training Loss: 2.4518165588378906\n",
            "Epoch: 3, Training Loss: 2.0279290676116943\n",
            "Epoch: 3, Training Loss: 2.1313316822052\n",
            "Epoch: 3, Training Loss: 2.353905439376831\n",
            "Epoch: 3, Training Loss: 2.1756632328033447\n",
            "Epoch: 3, Training Loss: 2.159916877746582\n",
            "Epoch: 3, Training Loss: 2.3997802734375\n",
            "Epoch: 3, Training Loss: 2.2962920665740967\n",
            "Epoch: 3, Training Loss: 2.252614736557007\n",
            "Epoch: 3, Training Loss: 2.2810957431793213\n",
            "Epoch: 3, Training Loss: 2.6169705390930176\n",
            "Epoch: 3, Training Loss: 2.1560542583465576\n",
            "Epoch: 3, Training Loss: 1.9686946868896484\n",
            "Epoch: 3, Training Loss: 2.365678071975708\n",
            "Epoch: 3, Training Loss: 2.223464012145996\n",
            "Epoch: 3, Training Loss: 2.3184702396392822\n",
            "Epoch: 3, Training Loss: 2.3967556953430176\n",
            "Epoch: 3, Training Loss: 2.4805359840393066\n",
            "Epoch: 3, Training Loss: 2.3004989624023438\n",
            "Epoch: 3, Training Loss: 2.2754087448120117\n",
            "Epoch: 3, Training Loss: 2.43268084526062\n",
            "Epoch: 3, Training Loss: 2.284064769744873\n",
            "Epoch: 3, Training Loss: 2.2701761722564697\n",
            "Epoch: 3, Training Loss: 2.230390787124634\n",
            "Epoch: 3, Training Loss: 2.3519392013549805\n",
            "Epoch: 3, Training Loss: 2.214689016342163\n",
            "Epoch: 3, Training Loss: 2.2681496143341064\n",
            "Epoch: 3, Training Loss: 2.1052300930023193\n",
            "Epoch: 3, Training Loss: 2.334754705429077\n",
            "Epoch: 3, Training Loss: 2.1725292205810547\n",
            "Epoch: 3, Training Loss: 2.211205244064331\n",
            "Epoch: 3, Training Loss: 2.371554136276245\n",
            "Epoch: 3, Training Loss: 2.645714282989502\n",
            "Epoch: 3, Training Loss: 2.4413304328918457\n",
            "Epoch: 3, Training Loss: 2.3125603199005127\n",
            "Epoch: 3, Training Loss: 2.0398495197296143\n",
            "Epoch: 3, Training Loss: 2.142155647277832\n",
            "Epoch: 3, Training Loss: 2.191964626312256\n",
            "Epoch: 3, Training Loss: 2.5008697509765625\n",
            "Epoch: 3, Training Loss: 2.21073317527771\n",
            "Epoch: 3, Training Loss: 2.41312575340271\n",
            "Epoch: 3, Training Loss: 2.1229333877563477\n",
            "Epoch: 3, Training Loss: 2.1637394428253174\n",
            "Epoch: 3, Training Loss: 2.355846881866455\n",
            "Epoch: 3, Training Loss: 2.427342176437378\n",
            "Epoch: 3, Training Loss: 2.188408374786377\n",
            "Epoch: 3, Training Loss: 2.722390651702881\n",
            "Epoch: 3, Training Loss: 2.2902109622955322\n",
            "Epoch: 3, Training Loss: 2.154106616973877\n",
            "Epoch: 3, Training Loss: 2.4927144050598145\n",
            "Epoch: 3, Training Loss: 2.3128366470336914\n",
            "Epoch: 3, Training Loss: 2.357372522354126\n",
            "Epoch: 3, Training Loss: 2.44513201713562\n",
            "Epoch: 3, Training Loss: 2.301440715789795\n",
            "Epoch: 3, Training Loss: 2.1700124740600586\n",
            "Epoch: 3, Training Loss: 2.02419114112854\n",
            "Epoch: 3, Training Loss: 2.506629705429077\n",
            "Epoch: 3, Training Loss: 2.4909605979919434\n",
            "Epoch: 3, Training Loss: 2.6151232719421387\n",
            "Epoch: 3, Training Loss: 2.4343669414520264\n",
            "Epoch: 3, Training Loss: 2.2611567974090576\n",
            "Epoch: 3, Training Loss: 2.512744903564453\n",
            "Epoch: 3, Training Loss: 1.8798294067382812\n",
            "Epoch: 3, Training Loss: 2.6956589221954346\n",
            "Epoch: 3, Training Loss: 2.314145565032959\n",
            "Epoch: 3, Training Loss: 2.34248423576355\n",
            "Epoch: 3, Training Loss: 2.547941207885742\n",
            "Epoch: 3, Training Loss: 2.148468255996704\n",
            "Epoch: 3, Training Loss: 2.1721441745758057\n",
            "Epoch: 3, Training Loss: 2.3908474445343018\n",
            "Epoch: 3, Training Loss: 2.165355682373047\n",
            "Epoch: 3, Training Loss: 2.330268383026123\n",
            "Epoch: 3, Training Loss: 2.3759372234344482\n",
            "Epoch: 3, Training Loss: 2.193312644958496\n",
            "Epoch: 3, Validation Loss: 2.0839648246765137\n",
            "Epoch: 3, Validation Loss: 2.1038291454315186\n",
            "Epoch: 3, Validation Loss: 2.241130828857422\n",
            "Epoch: 3, Validation Loss: 2.190617322921753\n",
            "Epoch: 3, Validation Loss: 1.7649784088134766\n",
            "Epoch: 3, Validation Loss: 2.0444488525390625\n",
            "Epoch: 3, Validation Loss: 2.2437305450439453\n",
            "Epoch: 3, Validation Loss: 1.9267568588256836\n",
            "Epoch: 3, Validation Loss: 2.1157846450805664\n",
            "Epoch: 3, Validation Loss: 2.1254687309265137\n",
            "Epoch: 3, Validation Loss: 2.144932508468628\n",
            "Epoch: 3, Validation Loss: 2.0020840167999268\n",
            "Epoch: 3, Validation Loss: 2.4128737449645996\n",
            "Epoch: 3, Validation Loss: 2.3444039821624756\n",
            "Epoch: 3, Validation Loss: 2.250494956970215\n",
            "Epoch: 3, Validation Loss: 2.425093412399292\n",
            "Epoch: 3, Validation Loss: 2.1076807975769043\n",
            "Epoch: 3, Validation Loss: 2.0278544425964355\n",
            "Epoch: 3, Validation Loss: 2.22424578666687\n",
            "Epoch: 3, Validation Loss: 2.318862199783325\n",
            "Epoch: 3, Validation Loss: 2.211911201477051\n",
            "Epoch: 3, Validation Loss: 1.9892609119415283\n",
            "Epoch: 3, Validation Loss: 2.242213726043701\n",
            "Epoch: 3, Validation Loss: 2.2182273864746094\n",
            "Epoch: 3, Validation Loss: 2.091196298599243\n",
            "Epoch: 3, Validation Loss: 2.0698187351226807\n",
            "Epoch: 3, Validation Loss: 1.8643923997879028\n",
            "Epoch: 3, Validation Loss: 2.163095712661743\n",
            "Epoch: 3, Validation Loss: 1.8765499591827393\n",
            "Epoch: 3, Validation Loss: 2.0908896923065186\n",
            "Epoch: 3, Validation Loss: 2.0793328285217285\n",
            "Epoch: 3, Validation Loss: 2.072085380554199\n",
            "Epoch: 3, Validation Loss: 2.2083804607391357\n",
            "Epoch: 3, Validation Loss: 2.434221029281616\n",
            "Epoch: 3, Validation Loss: 2.2783164978027344\n",
            "Epoch: 3, Validation Loss: 2.117220401763916\n",
            "Epoch: 3, Validation Loss: 1.6935938596725464\n",
            "Epoch: 3, Validation Loss: 2.102766275405884\n",
            "Epoch: 3, Validation Loss: 2.121448516845703\n",
            "Epoch: 3, Validation Loss: 2.225515842437744\n",
            "Epoch: 3, Validation Loss: 1.9516987800598145\n",
            "Epoch: 3, Validation Loss: 2.2336432933807373\n",
            "Epoch: 3, Validation Loss: 2.0661213397979736\n",
            "Epoch: 3, Validation Loss: 2.0861306190490723\n",
            "Epoch: 3, Validation Loss: 2.2357373237609863\n",
            "Epoch: 3, Validation Loss: 2.009409189224243\n",
            "Epoch: 3, Validation Loss: 2.302302837371826\n",
            "Epoch: 3, Validation Loss: 2.255147933959961\n",
            "Epoch: 3, Validation Loss: 2.1181390285491943\n",
            "Epoch: 3, Validation Loss: 2.2472448348999023\n",
            "Epoch: 3, Validation Loss: 1.965653896331787\n",
            "Epoch: 3, Validation Loss: 2.1250343322753906\n",
            "Epoch: 3, Validation Loss: 2.4836606979370117\n",
            "Epoch: 3, Validation Loss: 2.423384189605713\n",
            "Epoch: 3, Validation Loss: 2.276524066925049\n",
            "Epoch: 3, Validation Loss: 2.2261905670166016\n",
            "Epoch: 3, Validation Loss: 2.4709270000457764\n",
            "Epoch: 3, Validation Loss: 1.941109538078308\n",
            "Epoch: 3, Validation Loss: 2.00437068939209\n",
            "Epoch: 3, Validation Loss: 2.3406617641448975\n",
            "Epoch: 3, Validation Loss: 1.939782738685608\n",
            "Epoch: 3, Validation Loss: 2.3179051876068115\n",
            "Epoch: 3, Validation Loss: 2.108062505722046\n",
            "Epoch: 3, Validation Loss: 2.104170083999634\n",
            "Epoch: 3, Validation Loss: 2.060894727706909\n",
            "Epoch: 3, Validation Loss: 2.2878029346466064\n",
            "Epoch: 3, Validation Loss: 2.072766065597534\n",
            "Epoch: 3, Validation Loss: 2.0309972763061523\n",
            "Epoch: 3, Validation Loss: 2.1559696197509766\n",
            "Epoch: 3, Validation Loss: 2.0323235988616943\n",
            "Epoch: 3, Validation Loss: 2.0209779739379883\n",
            "Epoch: 3, Validation Loss: 2.147188901901245\n",
            "Epoch: 3, Validation Loss: 2.4235482215881348\n",
            "Epoch: 3, Validation Loss: 2.0199480056762695\n",
            "Epoch: 3, Validation Loss: 1.9781709909439087\n",
            "Epoch: 3, Validation Loss: 1.9813340902328491\n",
            "Epoch: 3, Validation Loss: 2.175551652908325\n",
            "Epoch: 3, Validation Loss: 2.1456003189086914\n",
            "Epoch: 3, Validation Loss: 2.2667882442474365\n",
            "Epoch: 3, Validation Loss: 2.2836453914642334\n",
            "Epoch: 3, Validation Loss: 2.117546796798706\n",
            "Epoch: 3, Validation Loss: 2.076420783996582\n",
            "Epoch: 3, Validation Loss: 2.1109743118286133\n",
            "Epoch: 3, Validation Loss: 2.3174684047698975\n",
            "Epoch: 3, Validation Loss: 2.0125527381896973\n",
            "Epoch: 3, Validation Loss: 2.0460240840911865\n",
            "Epoch: 3, Validation Loss: 2.230973482131958\n",
            "Epoch: 3, Validation Loss: 2.3259010314941406\n",
            "Epoch: 3, Validation Loss: 2.174927234649658\n",
            "Epoch: 3, Validation Loss: 2.2143359184265137\n",
            "Epoch: 3, Validation Loss: 2.2885100841522217\n",
            "Epoch: 3, Validation Loss: 1.9858874082565308\n",
            "Epoch: 3, Validation Loss: 1.9727404117584229\n",
            "Epoch: 3, Validation Loss: 2.135802745819092\n",
            "Epoch: 3, Validation Loss: 2.0208017826080322\n",
            "Epoch: 3, Validation Loss: 2.101609706878662\n",
            "Epoch: 3, Validation Loss: 2.0264768600463867\n",
            "Epoch: 3, Validation Loss: 1.9886375665664673\n",
            "Epoch: 3, Validation Loss: 2.070164442062378\n",
            "Epoch: 3, Validation Loss: 2.0222702026367188\n",
            "Epoch: 3, Validation Loss: 2.064884662628174\n",
            "Epoch: 3, Validation Loss: 2.0716145038604736\n",
            "Epoch: 3, Validation Loss: 2.0063774585723877\n",
            "Epoch: 3, Validation Loss: 1.7836265563964844\n",
            "Epoch: 3, Validation Loss: 2.004507541656494\n",
            "Epoch: 3, Validation Loss: 2.1298444271087646\n",
            "Epoch: 3, Validation Loss: 2.1116514205932617\n",
            "Epoch: 3, Validation Loss: 2.3151628971099854\n",
            "Epoch: 3, Validation Loss: 2.3996427059173584\n",
            "Epoch: 3, Validation Loss: 2.2475757598876953\n",
            "Epoch: 3, Validation Loss: 2.126504898071289\n",
            "Epoch: 3, Validation Loss: 2.0425851345062256\n",
            "Epoch: 3, Validation Loss: 2.380194664001465\n",
            "Epoch: 3, Validation Loss: 2.078345537185669\n",
            "Epoch: 3, Validation Loss: 2.045215129852295\n",
            "Epoch: 3, Validation Loss: 1.9302012920379639\n",
            "Epoch: 3, Validation Loss: 2.1652562618255615\n",
            "Epoch: 3, Validation Loss: 1.981029987335205\n",
            "Epoch: 3, Validation Loss: 2.032418727874756\n",
            "Epoch: 3, Validation Loss: 2.2721445560455322\n",
            "Epoch: 3, Validation Loss: 1.91793954372406\n",
            "Epoch: 3, Validation Loss: 2.070439577102661\n",
            "Epoch: 3, Validation Loss: 2.0462987422943115\n",
            "Epoch: 3, Validation Loss: 1.9514483213424683\n",
            "Epoch: 3, Validation Loss: 1.9179298877716064\n",
            "Epoch: 3, Validation Loss: 1.9756563901901245\n",
            "Epoch: 3, Validation Loss: 2.4856128692626953\n",
            "Epoch: 3, Validation Loss: 2.0405080318450928\n",
            "Epoch: 3, Validation Loss: 1.8996012210845947\n",
            "Epoch: 3, Validation Loss: 2.182798385620117\n",
            "Epoch: 3, Validation Loss: 2.0401902198791504\n",
            "Epoch: 3, Validation Loss: 2.388935089111328\n",
            "Epoch: 3, Validation Loss: 2.262070417404175\n",
            "Epoch: 3, Validation Loss: 2.1522862911224365\n",
            "Epoch: 3, Validation Loss: 2.2079973220825195\n",
            "Epoch: 3, Validation Loss: 2.0536468029022217\n",
            "Epoch: 3, Validation Loss: 2.1692099571228027\n",
            "Epoch: 3, Validation Loss: 2.198263168334961\n",
            "Epoch: 3, Validation Loss: 2.186887264251709\n",
            "Epoch: 3, Validation Loss: 2.0635502338409424\n",
            "Epoch: 3, Validation Loss: 1.9371947050094604\n",
            "Epoch: 3, Validation Loss: 2.294969320297241\n",
            "Epoch: 3, Validation Loss: 2.2241573333740234\n",
            "Epoch: 3, Validation Loss: 2.051828145980835\n",
            "Epoch: 3, Validation Loss: 1.9895519018173218\n",
            "Epoch: 3, Validation Loss: 2.357501983642578\n",
            "Epoch: 3, Validation Loss: 2.198633909225464\n",
            "Epoch: 3, Validation Loss: 2.1869139671325684\n",
            "Epoch: 3, Validation Loss: 2.128513813018799\n",
            "Epoch: 3, Validation Loss: 2.2454869747161865\n",
            "Epoch: 3, Validation Loss: 2.2091455459594727\n",
            "Epoch: 3, Validation Loss: 2.4367594718933105\n",
            "Epoch: 3, Validation Loss: 2.1843247413635254\n",
            "Epoch: 3, Validation Loss: 2.078152656555176\n",
            "Epoch: 3, Validation Loss: 2.374363660812378\n",
            "Epoch: 3, Validation Loss: 2.111290216445923\n",
            "Epoch: 3, Validation Loss: 2.1067938804626465\n",
            "Epoch: 3, Validation Loss: 2.0827248096466064\n",
            "Epoch: 3, Validation Loss: 2.120985507965088\n",
            "Epoch: 3, Validation Loss: 2.0399930477142334\n",
            "Epoch: 3, Validation Loss: 2.1843602657318115\n",
            "Epoch: 3, Validation Loss: 2.1426851749420166\n",
            "Epoch: 3, Validation Loss: 2.1915059089660645\n",
            "Epoch: 3, Validation Loss: 2.083138942718506\n",
            "Epoch: 3, Validation Loss: 1.9997704029083252\n",
            "Epoch: 3, Validation Loss: 2.1711485385894775\n",
            "Epoch: 3, Validation Loss: 2.1424107551574707\n",
            "Epoch: 3, Validation Loss: 2.1260769367218018\n",
            "Epoch: 3, Validation Loss: 2.1757190227508545\n",
            "Epoch: 3, Validation Loss: 2.028212070465088\n",
            "Epoch: 3, Validation Loss: 1.999715805053711\n",
            "Epoch: 3, Validation Loss: 2.11271595954895\n",
            "Epoch: 3, Validation Loss: 2.445863962173462\n",
            "Epoch: 3, Validation Loss: 1.9892164468765259\n",
            "Epoch: 3, Validation Loss: 1.9830288887023926\n",
            "Epoch: 3, Validation Loss: 2.2888829708099365\n",
            "Epoch: 3, Validation Loss: 2.28446102142334\n",
            "Epoch: 3, Validation Loss: 1.9736378192901611\n",
            "Epoch: 3, Validation Loss: 2.2859997749328613\n",
            "Epoch: 3, Validation Loss: 1.950740933418274\n",
            "Epoch: 3, Validation Loss: 2.060213088989258\n",
            "Epoch: 3, Validation Loss: 2.1514437198638916\n",
            "Epoch: 3, Validation Loss: 2.012585163116455\n",
            "Epoch: 3, Validation Loss: 2.14176869392395\n",
            "Epoch: 3, Validation Loss: 2.2242133617401123\n",
            "Epoch: 3, Validation Loss: 2.100487470626831\n",
            "Epoch: 3, Validation Loss: 2.0632729530334473\n",
            "Epoch: 3, Validation Loss: 1.975937843322754\n",
            "Epoch: 3, Validation Loss: 1.9834843873977661\n",
            "Epoch: 3, Validation Loss: 1.9210885763168335\n",
            "Epoch: 3, Validation Loss: 1.9884886741638184\n",
            "Epoch: 3, Validation Loss: 2.208765745162964\n",
            "Epoch: 3, Validation Loss: 2.1692771911621094\n",
            "Epoch: 3, Validation Loss: 2.1123645305633545\n",
            "Epoch: 3, Validation Loss: 2.2586774826049805\n",
            "Epoch: 3, Validation Loss: 2.1523468494415283\n",
            "Epoch: 3, Validation Loss: 1.9401911497116089\n",
            "Epoch: 3, Validation Loss: 2.0074872970581055\n",
            "Epoch: 3, Validation Loss: 1.8668259382247925\n",
            "Epoch: 3, Validation Loss: 2.227729558944702\n",
            "Epoch: 3, Validation Loss: 2.165681838989258\n",
            "Epoch: 3, Validation Loss: 2.052360773086548\n",
            "Epoch: 3, Validation Loss: 2.021395206451416\n",
            "Epoch: 3, Validation Loss: 2.023000478744507\n",
            "Epoch: 3, Validation Loss: 2.1864240169525146\n",
            "Epoch: 3, Validation Loss: 2.192150592803955\n",
            "Epoch: 3, Validation Loss: 2.2484872341156006\n",
            "Epoch: 3, Validation Loss: 2.3199572563171387\n",
            "Epoch: 3, Validation Loss: 2.291727066040039\n",
            "Epoch: 3, Validation Loss: 1.951062560081482\n",
            "Epoch: 3, Validation Loss: 2.1882100105285645\n",
            "Epoch: 3, Validation Loss: 2.365272283554077\n",
            "Epoch: 3, Validation Loss: 2.3414952754974365\n",
            "Epoch: 3, Validation Loss: 2.264439821243286\n",
            "Epoch: 3, Validation Loss: 2.189591407775879\n",
            "Epoch: 3, Validation Loss: 1.8954081535339355\n",
            "Epoch: 3, Validation Loss: 1.9480245113372803\n",
            "Epoch: 3, Validation Loss: 2.2221779823303223\n",
            "Epoch: 3, Validation Loss: 2.271627187728882\n",
            "Epoch: 3, Validation Loss: 2.0362210273742676\n",
            "Epoch: 3, Validation Loss: 2.2133166790008545\n",
            "Epoch: 3, Validation Loss: 2.00388503074646\n",
            "Epoch: 3, Validation Loss: 2.054767608642578\n",
            "Epoch: 3, Validation Loss: 1.8375035524368286\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 256\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "d_ff = 512\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 3\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------------------\")\n",
        "    transformer.train()\n",
        "    for data in train_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    transformer.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in val_dataloader:\n",
        "            src_data, tgt_data = data\n",
        "            output = transformer(src_data, tgt_data[:, :-1])\n",
        "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "            print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
        "\n",
        "    torch.save(transformer.state_dict(), f'./transformer_state_dict_epoch_{epoch+1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLQfPbuV_MW4",
        "outputId": "afb704c1-4e7e-4786-bfb0-165f70ddd8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.9633657932281494\n",
            "Test Loss: 2.136871337890625\n",
            "Test Loss: 2.209472417831421\n",
            "Test Loss: 2.5739591121673584\n",
            "Test Loss: 1.8896713256835938\n",
            "Test Loss: 2.049983501434326\n",
            "Test Loss: 1.9989005327224731\n",
            "Test Loss: 1.9682408571243286\n",
            "Test Loss: 2.2493555545806885\n",
            "Test Loss: 1.7985692024230957\n",
            "Test Loss: 1.91669762134552\n",
            "Test Loss: 2.1654419898986816\n",
            "Test Loss: 1.9810129404067993\n",
            "Test Loss: 2.275704860687256\n",
            "Test Loss: 1.9295533895492554\n",
            "Test Loss: 2.298708915710449\n",
            "Test Loss: 2.0952014923095703\n",
            "Test Loss: 2.018160820007324\n",
            "Test Loss: 2.206990957260132\n",
            "Test Loss: 2.076070547103882\n",
            "Test Loss: 2.231081485748291\n",
            "Test Loss: 2.1140525341033936\n",
            "Test Loss: 2.2065725326538086\n",
            "Test Loss: 2.185159921646118\n",
            "Test Loss: 1.9285701513290405\n",
            "Test Loss: 2.2580907344818115\n",
            "Test Loss: 2.3661727905273438\n",
            "Test Loss: 2.2734856605529785\n",
            "Test Loss: 2.06719708442688\n",
            "Test Loss: 2.192049741744995\n",
            "Test Loss: 2.1346540451049805\n",
            "Test Loss: 2.2178053855895996\n",
            "Test Loss: 1.7609453201293945\n",
            "Test Loss: 1.9672911167144775\n",
            "Test Loss: 1.9419969320297241\n",
            "Test Loss: 2.1898422241210938\n",
            "Test Loss: 2.07623028755188\n",
            "Test Loss: 2.127284049987793\n",
            "Test Loss: 2.188077449798584\n",
            "Test Loss: 1.9905451536178589\n",
            "Test Loss: 2.134265184402466\n",
            "Test Loss: 2.031985282897949\n",
            "Test Loss: 2.3294546604156494\n",
            "Test Loss: 1.9484108686447144\n",
            "Test Loss: 1.9758719205856323\n",
            "Test Loss: 1.9097212553024292\n",
            "Test Loss: 2.3552968502044678\n",
            "Test Loss: 2.1251931190490723\n",
            "Test Loss: 2.3484716415405273\n",
            "Test Loss: 1.7791306972503662\n",
            "Test Loss: 2.114715814590454\n",
            "Test Loss: 1.9030766487121582\n",
            "Test Loss: 2.0497219562530518\n",
            "Test Loss: 2.336275100708008\n",
            "Test Loss: 2.1621761322021484\n",
            "Test Loss: 2.100626230239868\n",
            "Test Loss: 2.0370852947235107\n",
            "Test Loss: 1.9720948934555054\n",
            "Test Loss: 2.083487033843994\n",
            "Test Loss: 2.1426286697387695\n",
            "Test Loss: 2.070518970489502\n",
            "Test Loss: 2.004427194595337\n",
            "Test Loss: 2.022265672683716\n",
            "Test Loss: 2.1570775508880615\n",
            "Test Loss: 2.287264823913574\n",
            "Test Loss: 2.3375449180603027\n",
            "Test Loss: 2.0966241359710693\n",
            "Test Loss: 2.012444019317627\n",
            "Test Loss: 2.2195518016815186\n",
            "Test Loss: 2.257711887359619\n",
            "Test Loss: 1.8489155769348145\n",
            "Test Loss: 2.0365524291992188\n",
            "Test Loss: 2.1229851245880127\n",
            "Test Loss: 1.7670925855636597\n",
            "Test Loss: 2.3229029178619385\n",
            "Test Loss: 1.9156711101531982\n",
            "Test Loss: 2.0704779624938965\n",
            "Test Loss: 2.3666720390319824\n",
            "Test Loss: 2.266622543334961\n",
            "Test Loss: 1.9166274070739746\n",
            "Test Loss: 1.8578717708587646\n",
            "Test Loss: 2.1044423580169678\n",
            "Test Loss: 2.215533971786499\n",
            "Test Loss: 2.05965518951416\n",
            "Test Loss: 2.2997028827667236\n",
            "Test Loss: 2.0478267669677734\n",
            "Test Loss: 2.2011804580688477\n",
            "Test Loss: 2.2196383476257324\n",
            "Test Loss: 2.213940143585205\n",
            "Test Loss: 1.9955885410308838\n",
            "Test Loss: 2.04634952545166\n",
            "Test Loss: 1.9690008163452148\n",
            "Test Loss: 2.0491974353790283\n",
            "Test Loss: 2.1953163146972656\n",
            "Test Loss: 2.198160409927368\n",
            "Test Loss: 2.267124891281128\n",
            "Test Loss: 2.380831003189087\n",
            "Test Loss: 1.9702500104904175\n",
            "Test Loss: 1.8401262760162354\n",
            "Test Loss: 2.105180025100708\n",
            "Test Loss: 1.915518879890442\n",
            "Test Loss: 2.2526097297668457\n",
            "Test Loss: 2.1631100177764893\n",
            "Test Loss: 2.270273208618164\n",
            "Test Loss: 2.4124999046325684\n",
            "Test Loss: 2.208042860031128\n",
            "Test Loss: 2.246570348739624\n",
            "Test Loss: 2.321249008178711\n",
            "Test Loss: 2.5668182373046875\n",
            "Test Loss: 2.267925500869751\n",
            "Test Loss: 2.1117236614227295\n",
            "Test Loss: 2.1391305923461914\n",
            "Test Loss: 2.0509650707244873\n",
            "Test Loss: 2.0419998168945312\n",
            "Test Loss: 2.023893117904663\n",
            "Test Loss: 2.011568784713745\n",
            "Test Loss: 1.9412901401519775\n",
            "Test Loss: 2.216886043548584\n",
            "Test Loss: 2.194594621658325\n",
            "Test Loss: 1.886146903038025\n",
            "Test Loss: 2.2005670070648193\n",
            "Test Loss: 2.1507067680358887\n",
            "Test Loss: 2.0694615840911865\n",
            "Test Loss: 2.1748554706573486\n",
            "Test Loss: 2.2927229404449463\n",
            "Test Loss: 2.1689162254333496\n",
            "Test Loss: 2.175884962081909\n",
            "Test Loss: 2.3630053997039795\n",
            "Test Loss: 2.3462131023406982\n",
            "Test Loss: 1.7616339921951294\n",
            "Test Loss: 1.9347331523895264\n",
            "Test Loss: 2.0276057720184326\n",
            "Test Loss: 2.113670587539673\n",
            "Test Loss: 2.005546808242798\n",
            "Test Loss: 2.188974618911743\n",
            "Test Loss: 2.0962936878204346\n",
            "Test Loss: 2.0459234714508057\n",
            "Test Loss: 1.9501584768295288\n",
            "Test Loss: 1.824094295501709\n",
            "Test Loss: 2.092817544937134\n",
            "Test Loss: 2.299710750579834\n",
            "Test Loss: 2.1802868843078613\n",
            "Test Loss: 2.1052820682525635\n",
            "Test Loss: 2.202113389968872\n",
            "Test Loss: 1.9762539863586426\n",
            "Test Loss: 1.7743107080459595\n",
            "Test Loss: 2.426818370819092\n",
            "Test Loss: 2.3083674907684326\n",
            "Test Loss: 1.8659651279449463\n",
            "Test Loss: 1.906535029411316\n",
            "Test Loss: 1.7585870027542114\n",
            "Test Loss: 2.211531639099121\n",
            "Test Loss: 2.1414389610290527\n",
            "Test Loss: 2.195850133895874\n",
            "Test Loss: 1.9434895515441895\n",
            "Test Loss: 1.9906185865402222\n",
            "Test Loss: 2.179205894470215\n",
            "Test Loss: 2.3614087104797363\n",
            "Test Loss: 2.109495162963867\n",
            "Test Loss: 1.8102949857711792\n",
            "Test Loss: 2.062657117843628\n",
            "Test Loss: 2.416752576828003\n",
            "Test Loss: 1.9850140810012817\n",
            "Test Loss: 2.2074227333068848\n",
            "Test Loss: 1.9666134119033813\n",
            "Test Loss: 2.0919814109802246\n",
            "Test Loss: 2.274750232696533\n",
            "Test Loss: 2.178750514984131\n",
            "Test Loss: 1.992116928100586\n",
            "Test Loss: 1.8947830200195312\n",
            "Test Loss: 2.0928657054901123\n",
            "Test Loss: 2.049900770187378\n",
            "Test Loss: 1.9764940738677979\n",
            "Test Loss: 1.8210554122924805\n",
            "Test Loss: 1.959748387336731\n",
            "Test Loss: 2.052610158920288\n",
            "Test Loss: 1.966720461845398\n",
            "Test Loss: 2.0833981037139893\n",
            "Test Loss: 2.1522271633148193\n",
            "Test Loss: 1.893713116645813\n",
            "Test Loss: 2.050447702407837\n",
            "Test Loss: 2.0746331214904785\n",
            "Test Loss: 1.8072856664657593\n",
            "Test Loss: 2.075228214263916\n",
            "Test Loss: 2.098478317260742\n",
            "Test Loss: 1.8908218145370483\n",
            "Test Loss: 2.1751012802124023\n",
            "Test Loss: 2.2755160331726074\n",
            "Test Loss: 2.302032232284546\n",
            "Test Loss: 2.2267496585845947\n",
            "Test Loss: 2.0682456493377686\n",
            "Test Loss: 2.2464280128479004\n",
            "Test Loss: 2.3718101978302\n",
            "Test Loss: 2.145847797393799\n",
            "Test Loss: 2.215214252471924\n",
            "Test Loss: 2.0047452449798584\n",
            "Test Loss: 1.929782509803772\n",
            "Test Loss: 2.098708152770996\n",
            "Test Loss: 2.0666725635528564\n",
            "Test Loss: 2.1516611576080322\n",
            "Test Loss: 2.108020067214966\n",
            "Test Loss: 2.26590633392334\n",
            "Test Loss: 2.1929194927215576\n",
            "Test Loss: 2.310803174972534\n",
            "Test Loss: 2.188119411468506\n",
            "Test Loss: 2.0712385177612305\n",
            "Test Loss: 2.2710251808166504\n",
            "Test Loss: 2.2247819900512695\n",
            "Test Loss: 2.0653798580169678\n",
            "Test Loss: 2.1343719959259033\n",
            "Test Loss: 2.0636677742004395\n",
            "Test Loss: 1.964910864830017\n",
            "Test Loss: 2.1189279556274414\n",
            "Test Loss: 2.2482805252075195\n",
            "Test Loss: 2.3543670177459717\n",
            "Test Loss: 2.101280450820923\n",
            "Test Loss: 2.2495217323303223\n",
            "Test Loss: 2.2613532543182373\n",
            "Test Loss: 2.413912773132324\n",
            "Test Loss: 2.0824480056762695\n",
            "Test Loss: 2.066756248474121\n",
            "Test Loss: 2.011545181274414\n",
            "Test Loss: 1.883131742477417\n",
            "Test Loss: 2.6178088188171387\n"
          ]
        }
      ],
      "source": [
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        print(f\"Test Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofl363KBgiyq"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsbGPbE_hFPD",
        "outputId": "7bfbe4d9-52f2-43f0-918b-7c5346aae9cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"./transformer_state_dict_epoch_3\"\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 256\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "d_ff = 512\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 3\n",
        "\n",
        "transformer_loaded = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "transformer_loaded.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "x0NYt9BPMufm"
      },
      "outputs": [],
      "source": [
        "def translate(src):\n",
        "    src_tokens = tokenizer[SRC_LANGUAGE](src)\n",
        "    tgt_tokens = [\"<BOS>\"]\n",
        "\n",
        "    src_vectors = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](src_tokens) + [EOS_IDX] + [0] * (max_seq_len - len(src_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for i in range(max_seq_len):\n",
        "        tgt_vectors = torch.tensor((vocab[TGT_LANGUAGE](tgt_tokens) + [0] * (max_seq_len - len(tgt_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        output = transformer(src_vectors, tgt_vectors)\n",
        "        idx = torch.argmax(nn.functional.softmax(output, dim=2)[0][i]).item()\n",
        "        tgt_tokens.append(vocab[TGT_LANGUAGE].lookup_token(idx))\n",
        "\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return \" \".join(tgt_tokens).replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d9xThngkeK6W",
        "outputId": "c39a69d3-9ebe-4353-a7e9-0cf6b17195a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Por favor , estoy profesor .'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"Hello, I am a teacher.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lE7m-fQefb0V",
        "outputId": "c952c1ec-ce63-49f8-d03f-9228596adfd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mi nombre es John .'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"My name is John.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QyHIl2fhfp2O",
        "outputId": "247f0f94-5807-458f-92d3-979f922935f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Estoy aprendiendo .'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I am learning Spanish.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KahapCeCfwMr",
        "outputId": "69c4a3e7-c97d-4a88-9ad0-fad3ea4da9e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Me voy a manzanas .'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I eat apples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q_7aRX5Hf0A7",
        "outputId": "60b2db64-f72b-49b3-de2b-870496c632d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tengo tres libros y libros .'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I have three books and two pens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OQkoUnAmgMw1",
        "outputId": "8a91b56d-f187-4f57-bd5c-eb7a59d01724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'¿ Quieres trabajar en un oficina ?'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"Do you work in an office?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eQ3u-gnngu_l",
        "outputId": "108177f8-be87-4c0f-d2a3-5fa0c2353874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'¿ Cómo estás ?'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_p_dI_QOiWRJ",
        "outputId": "9820902f-7f01-48a6-aeb4-a9caebddb2c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If you really want to help, please come by 2:30.\n",
            "Si realmente quieres ayudar, por favor ven a las 2:30.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Si quieres quieres ayudar , por favor a las 2:30 .'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, spa = test_lines[0].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "T0VbizfWil7D",
        "outputId": "239fdf9c-a5eb-4466-d355-53d31285a9e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was stunned by what happened.\n",
            "Yo estaba anonadado por lo que había ocurrido.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Estaba de hacer lo que pasó .'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, spa = test_lines[500].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vp-71fggiuM5",
        "outputId": "354adf16-99c6-42b6-8ec8-a43f10536df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This one has a lot of advantages over that one.\n",
            "Esta tiene un montón de ventajas por sobre aquella.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Este es mucho de mucha <UNK> .'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, spa = train_lines[1000].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fGxzFl4Ui0lY",
        "outputId": "49683592-a50d-4930-b16a-c487372ab486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell me what Tom said.\n",
            "Cuéntame lo que dijo Tom.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Dime lo que dijo Tom .'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, spa = train_lines[10000].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HIFB166pJMq"
      },
      "source": [
        "# Export model and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NRQyHCffi7-s"
      },
      "outputs": [],
      "source": [
        "torch.save(vocab[SRC_LANGUAGE], \"./vocab-english\")\n",
        "torch.save(vocab[TGT_LANGUAGE], \"./vocab-spanish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "DFaLmjFWqB3e"
      },
      "outputs": [],
      "source": [
        "torch.save(tokenizer[SRC_LANGUAGE], \"./tokenizer-english\")\n",
        "torch.save(tokenizer[TGT_LANGUAGE], \"./tokenizer-spanish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "v5leNOqGqmT3"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer, \"./transformer_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ndAILIdq58F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPIvoBMMmuwyn/0MFLgs8k5",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
